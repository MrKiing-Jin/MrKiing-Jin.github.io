<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蜡笔小金QAQ的个人主页</title>
  
  <subtitle>Re0:从零开始的IT码农生活</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-09-07T14:13:04.220Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>蜡笔小金QAQ</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习-pytorch-CNN网络实践(GAP)</title>
    <link href="http://example.com/2022/09/07/%E4%B8%87%E8%81%AA%E8%80%81%E5%B8%88%E9%A1%B9%E7%9B%AEGAP%E9%AD%94%E6%94%B9/"/>
    <id>http://example.com/2022/09/07/%E4%B8%87%E8%81%AA%E8%80%81%E5%B8%88%E9%A1%B9%E7%9B%AEGAP%E9%AD%94%E6%94%B9/</id>
    <published>2022-09-07T14:01:46.656Z</published>
    <updated>2022-09-07T14:13:04.220Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CNN网络实践-GAP引入"><a href="#CNN网络实践-GAP引入" class="headerlink" title="CNN网络实践(GAP引入)"></a>CNN网络实践(GAP引入)</h1><p>加了一个卷积层，训练次数提到1000，总算过95%了<br><em><strong>开学力！！！</strong></em><br><em><strong>吼吼吼，新的学期也要加油哦！</strong></em></p><h3 id="引入包"><a href="#引入包" class="headerlink" title="引入包"></a>引入包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="读入数据集"><a href="#读入数据集" class="headerlink" title="读入数据集"></a>读入数据集</h3><p><a href="https://zhuanlan.zhihu.com/p/37471802">onehot编码</a></p><p><a href="https://blog.csdn.net/qq_39368111/article/details/110435536?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166089858416782184695575%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166089858416782184695575&biz_id=0&spm=1018.2226.3001.4187">iloc()函数</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/9ed2a7ecf1645053aadfbf3ebf1a8eb3.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">targetDict = &#123;</span><br><span class="line">    <span class="number">109</span>: torch.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">122</span>: torch.Tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">135</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">174</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">189</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">201</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">213</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">226</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">238</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">97</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 定义了一个字典[label:onehot 编码]</span></span><br><span class="line"></span><br><span class="line">output2lable = [torch.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(targetDict[<span class="number">109</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">bearDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, dim</span>):  <span class="comment"># dim为样本的维度</span></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.df = pd.read_csv(csv_file)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 取一行，一行的数目是[0,dim)</span></span><br><span class="line">        x = torch.tensor(</span><br><span class="line">            self.df.iloc[idx, <span class="number">0</span>:self.dim].values, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        x = x.view(<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">        y = self.df.iloc[idx, <span class="number">2000</span>]</span><br><span class="line">        y = targetDict[y]</span><br><span class="line">        <span class="keyword">return</span> [x, y]</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ds_demo = bearDataset(</span><br><span class="line">    <span class="string">&#x27;C:/Users/JINTIAN/Desktop/代码/PytorchCode/项目数据集/train.csv&#x27;</span>, <span class="number">400</span>)</span><br><span class="line">dl = torch.utils.data.DataLoader(</span><br><span class="line">    ds_demo, batch_size=<span class="number">1000</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)  <span class="comment"># 只训练10个</span></span><br><span class="line"><span class="comment"># dl是我们定义的一个数据加载函数，一次加载10个</span></span><br><span class="line">x1, y1 = ds_demo.__getitem__(<span class="number">11</span>)</span><br><span class="line"><span class="built_in">print</span>(x1)  <span class="comment"># 打印数据</span></span><br><span class="line"><span class="built_in">print</span>(x1.shape)</span><br><span class="line"><span class="built_in">print</span>(y1)  <span class="comment"># 标签</span></span><br><span class="line">idata = <span class="built_in">iter</span>(dl)</span><br><span class="line">x, y = <span class="built_in">next</span>(idata)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.1037,  0.0555, -0.0096, -0.0547, -0.0682, -0.0524, -0.0179,  0.0238,          0.0661,  0.0935,  0.0918,  0.0409, -0.0225, -0.0880, -0.1535, -0.1640,         -0.1462, -0.0985, -0.0415, -0.0083,  0.0273,  0.0409,  0.0382,  0.0054,         -0.0501, -0.0887, -0.1039, -0.0801, -0.0440, -0.0108,  0.0221,  0.0478,          0.0822,  0.1081,  0.0855,  0.0257, -0.0321, -0.0657, -0.0611, -0.0365,         -0.0177,  0.0048,  0.0332,  0.0684,  0.0951,  0.0903,  0.0463, -0.0211,         -0.0734, -0.0895, -0.0724, -0.0401, -0.0040,  0.0463,  0.0989,  0.1277,          0.1362,  0.1066,  0.0250, -0.0663, -0.1204, -0.1287, -0.0947, -0.0465,         -0.0038,  0.0509,  0.0989,  0.1500,  0.1663,  0.1122,  0.0467, -0.0198,         -0.0565, -0.0590, -0.0482, -0.0350, -0.0244,  0.0083,  0.0446,  0.0732,          0.0565,  0.0046, -0.0486, -0.0862, -0.0665, -0.0342,  0.0002,  0.0323,          0.0515,  0.0968,  0.1145,  0.0912,  0.0515, -0.0131, -0.0434, -0.0467,         -0.0334,  0.0104,  0.0365,  0.0676,  0.0953,  0.1364,  0.1775,  0.1512,          0.0962,  0.0169, -0.0463, -0.0680, -0.0793, -0.0597, -0.0271,  0.0136,          0.0643,  0.0928,  0.0962,  0.0607,  0.0006, -0.0359, -0.0415, -0.0238,         -0.0044,  0.0021,  0.0323,  0.0634,  0.0882,  0.1147,  0.0935,  0.0647,          0.0344,  0.0188,  0.0265,  0.0117,  0.0042, -0.0017,  0.0192,  0.0640,          0.0859,  0.0910,  0.0732,  0.0478,  0.0417,  0.0419,  0.0411,  0.0317,          0.0119,  0.0119,  0.0311,  0.0668,  0.0718,  0.0446,  0.0169, -0.0148,         -0.0154, -0.0169, -0.0165, -0.0125, -0.0008,  0.0509,  0.0933,  0.1318,          0.1400,  0.1026,  0.0630,  0.0342,  0.0323,  0.0213,  0.0044,  0.0021,          0.0225,  0.0584,  0.0857,  0.0966,  0.0695,  0.0321,  0.0094,  0.0119,          0.0250,  0.0204,  0.0179,  0.0390,  0.0774,  0.1187,  0.1381,  0.1145,          0.0638, -0.0031, -0.0365, -0.0307, -0.0238, -0.0052,  0.0136,  0.0613,          0.1254,  0.1519,  0.1490,  0.1099,  0.0522,  0.0271,  0.0213,  0.0029,         -0.0211, -0.0457, -0.0570, -0.0338,  0.0067,  0.0190, -0.0038, -0.0492,         -0.0878, -0.1104, -0.1279, -0.1406, -0.1510, -0.1406, -0.1168, -0.0559,         -0.0077, -0.0255, -0.0839, -0.1636, -0.1807, -0.1823, -0.1802, -0.1356,         -0.1137, -0.0780, -0.0478, -0.0192, -0.0021, -0.0570, -0.0980, -0.1204,         -0.1043, -0.0398, -0.0167, -0.0131, -0.0146, -0.0013,  0.0302,  0.0388,          0.0227, -0.0255, -0.0524, -0.0469, -0.0273, -0.0058, -0.0148, -0.0184,         -0.0104,  0.0244,  0.0695,  0.0862,  0.0899,  0.0613,  0.0492,  0.0726,          0.0682,  0.0584,  0.0403,  0.0355,  0.0749,  0.1233,  0.1594,  0.1212,          0.0584,  0.0407,  0.0494,  0.0784,  0.0993,  0.1118,  0.1064,  0.1043,          0.1172,  0.1181,  0.0991,  0.0688,  0.0524,  0.0513,  0.0668,  0.0776,          0.0505, -0.0002, -0.0273, -0.0150,  0.0255,  0.0611,  0.0609,  0.0355,          0.0240,  0.0365,  0.0401,  0.0338, -0.0163, -0.0757, -0.0816, -0.0513,          0.0058,  0.0409,  0.0478,  0.0330,  0.0202,  0.0267,  0.0046, -0.0426,         -0.0941, -0.1218, -0.1024, -0.0576, -0.0211, -0.0152, -0.0240, -0.0184,         -0.0038,  0.0050, -0.0259, -0.0953, -0.1602, -0.1844, -0.1462, -0.1020,         -0.0720, -0.0471, -0.0188,  0.0332,  0.0736,  0.0755,  0.0067, -0.0897,         -0.1396, -0.1375, -0.0895, -0.0421, -0.0071,  0.0060,  0.0081,  0.0302,          0.0252, -0.0063, -0.0653, -0.1141, -0.1089, -0.0741, -0.0073,  0.0369,          0.0515,  0.0640,  0.0722,  0.0859,  0.0471, -0.0319, -0.1164, -0.1748,         -0.1467, -0.0870, -0.0113,  0.0478,  0.0786,  0.1218,  0.1323,  0.1256,          0.0657, -0.0305, -0.0885, -0.1185, -0.0755, -0.0156,  0.0371,  0.0907,          0.1133,  0.1556,  0.1792,  0.1675,  0.1118,  0.0211, -0.0350, -0.0553,         -0.0229,  0.0181,  0.0294,  0.0486,  0.0826,  0.1285,  0.1287,  0.0832,          0.0363, -0.0113, -0.0096,  0.0357,  0.0962,  0.1425,  0.1508,  0.1658,          0.1727,  0.1621,  0.1273,  0.0713,  0.0288, -0.0054,  0.0021,  0.0309]])torch.Size([1, 400])tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])tensor([[[ 3.7718e-01,  3.8302e-01,  3.9679e-01,  ...,  3.1355e-01,           4.1535e-01,  5.1883e-01]],        [[ 8.2403e-02,  9.8884e-02,  7.1138e-02,  ..., -5.8204e-02,          -8.3238e-02, -7.0929e-02]],        [[-4.9025e-02,  5.2988e-02,  1.4353e-01,  ..., -5.8830e-02,           1.0744e-01,  2.4533e-01]],        ...,        [[ 4.4655e-01,  4.5823e-01,  4.5155e-01,  ..., -2.0867e-02,          -1.8488e-01, -3.0257e-01]],        [[ 3.8895e-01,  2.8295e-01,  1.4607e-01,  ..., -9.0061e-01,          -1.3956e+00, -1.6460e+00]],        [[ 6.4462e-02,  1.9193e-02, -6.2585e-04,  ..., -1.2121e-01,          -7.4476e-02, -2.1487e-02]]])torch.Size([1000, 1, 400])tensor([[0., 0., 0.,  ..., 0., 0., 0.],        [1., 0., 0.,  ..., 0., 0., 0.],        [0., 1., 0.,  ..., 0., 0., 0.],        ...,        [0., 0., 0.,  ..., 1., 0., 0.],        [0., 0., 0.,  ..., 0., 1., 0.],        [0., 0., 0.,  ..., 0., 0., 0.]])torch.Size([1000, 10])</code></pre><h3 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h3><p><a href="https://blog.csdn.net/liujh845633242/article/details/102668515">Conv1d()和Conv2d()区别</a></p><p><a href="https://blog.csdn.net/yingluo54/article/details/122168364">conv1d详细用法</a></p><p><a href="https://javajgs.com/archives/153549#:~:text=%E8%BE%93%E5%87%BA%E7%89%B9%E5%BE%81%E5%9B%BE%E5%B0%BA%E5%AF%B8%20%E6%9C%80%E5%90%8E%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E5%8D%B7%E7%A7%AF%E8%BE%93%E5%87%BA%E5%A4%A7%E5%B0%8F%3A%20N%20%3D%20%28W%20%E2%88%92%20F%20%2B,%E8%BE%93%E5%85%A5%E5%9B%BE%E7%89%87%E5%A4%A7%E5%B0%8F%20W%C3%97W%2C%20Filter%E5%A4%A7%E5%B0%8F%20F%C3%97F%2C%20%E6%AD%A5%E9%95%BF%20S%2C%20padding%E7%9A%84%E5%83%8F%E7%B4%A0%E6%95%B0%20P.">如何计算卷积网络的尺寸(公式)</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/35547f3a90d398bebc8e3aed0508a774.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pdb <span class="keyword">import</span> line_prefix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()  <span class="comment"># 一个样本维度为400 维度：[1000,1,400]</span></span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">11</span>,</span><br><span class="line">                               stride=<span class="number">4</span>)  <span class="comment"># 出来后此时：[1000,64,98]</span></span><br><span class="line">        <span class="comment"># 1dpooling 维度算法参考https://blog.csdn.net/yingluo54/article/details/122168364</span></span><br><span class="line">        self.pool1 = nn.MaxPool1d(<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># 出来后此时：[1000,64,48]</span></span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>)  <span class="comment"># 出来后此时：[1000,128,46]</span></span><br><span class="line">        self.pool2 = nn.MaxPool1d(<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># 出来后此时：[1000,128,22]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.fc1 = nn.Linear(22*128, 22*64)  # 从上边出来之后维度变成22</span></span><br><span class="line">        <span class="comment"># self.fc2 = nn.Linear(22*64, 500)</span></span><br><span class="line">        <span class="comment"># self.fc3 = nn.Linear(500, 10)</span></span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv1d(<span class="number">128</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>)  <span class="comment"># [1000,10,10]</span></span><br><span class="line">        self.gap = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        self.fla = nn.Flatten()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        x = self.pool2(F.relu(self.conv2(x)))</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        <span class="comment"># x = x.view(-1, 22*128)</span></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        <span class="comment"># x = F.relu(self.fc1(x))</span></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        <span class="comment"># x = F.relu(self.fc2(x))</span></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        <span class="comment"># x = self.fc3(x)</span></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        x = self.gap(x)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        x = self.fla(x)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Net(  (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(4,))  (pool1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,))  (pool2): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  (conv3): Conv1d(128, 10, kernel_size=(3,), stride=(1,))  (gap): AdaptiveAvgPool1d(output_size=1)  (fla): Flatten(start_dim=1, end_dim=-1))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = optim.Adam(net.parameters())  <span class="comment"># Adam训练器，用了发现比SGD好使</span></span><br><span class="line"><span class="comment"># optimizer = optim.RMSprop(net.parameters(), lr=0.001)  # RMSprop训练器,用了发现没有adam好使</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><p><a href="https://www.runoob.com/python/python-func-enumerate.html">enumerate()函数</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">loss_list = []</span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):  <span class="comment"># 多批次循环</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span>  <span class="comment"># 每次重置0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取输入</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度置0</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向传播，反向传播，优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        <span class="comment"># print(type(net(inputs)))</span></span><br><span class="line">        loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        res = loss(outputs, labels)  <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">        res.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印状态信息</span></span><br><span class="line">        running_loss += res.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:    <span class="comment"># 每2批次(1批次10个样本)打印一次</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.5f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i, running_loss / <span class="number">20</span>))</span><br><span class="line">            loss_list.append(running_loss / <span class="number">20</span>)</span><br><span class="line">            x_list.append(epoch*<span class="number">100</span>+i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[1,     0] loss: 0.11445torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[2,     0] loss: 0.11186torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[3,     0] loss: 0.10949torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[4,     0] loss: 0.10733torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[5,     0] loss: 0.10536torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[6,     0] loss: 0.10361torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[7,     0] loss: 0.10205torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[8,     0] loss: 0.10063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[9,     0] loss: 0.09928torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[10,     0] loss: 0.09789torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[11,     0] loss: 0.09642torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[12,     0] loss: 0.09484torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[13,     0] loss: 0.09315torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[14,     0] loss: 0.09139torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[15,     0] loss: 0.08957torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[16,     0] loss: 0.08771torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[17,     0] loss: 0.08582torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[18,     0] loss: 0.08393torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[19,     0] loss: 0.08204torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[20,     0] loss: 0.08016torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[21,     0] loss: 0.07828torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[22,     0] loss: 0.07643torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[23,     0] loss: 0.07462torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[24,     0] loss: 0.07287torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[25,     0] loss: 0.07117torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[26,     0] loss: 0.06954torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[27,     0] loss: 0.06797torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[28,     0] loss: 0.06646torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[29,     0] loss: 0.06501torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[30,     0] loss: 0.06361torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[31,     0] loss: 0.06227torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[32,     0] loss: 0.06098torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[33,     0] loss: 0.05974torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[34,     0] loss: 0.05856torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[35,     0] loss: 0.05742torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[36,     0] loss: 0.05634torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[37,     0] loss: 0.05530torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[38,     0] loss: 0.05430torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[39,     0] loss: 0.05334torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[40,     0] loss: 0.05243torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[41,     0] loss: 0.05155torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[42,     0] loss: 0.05071torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[43,     0] loss: 0.04991torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[44,     0] loss: 0.04914torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[45,     0] loss: 0.04839torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[46,     0] loss: 0.04768torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[47,     0] loss: 0.04698torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[48,     0] loss: 0.04631torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[49,     0] loss: 0.04566torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[50,     0] loss: 0.04503torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[51,     0] loss: 0.04441torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[52,     0] loss: 0.04381torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[53,     0] loss: 0.04322torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[54,     0] loss: 0.04264torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[55,     0] loss: 0.04208torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[56,     0] loss: 0.04153torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[57,     0] loss: 0.04098torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[58,     0] loss: 0.04045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[59,     0] loss: 0.03993torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[60,     0] loss: 0.03942torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[61,     0] loss: 0.03892torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[62,     0] loss: 0.03842torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[63,     0] loss: 0.03794torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[64,     0] loss: 0.03746torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[65,     0] loss: 0.03699torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[66,     0] loss: 0.03652torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[67,     0] loss: 0.03606torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[68,     0] loss: 0.03561torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[69,     0] loss: 0.03516torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[70,     0] loss: 0.03472torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[71,     0] loss: 0.03428torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[72,     0] loss: 0.03385torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[73,     0] loss: 0.03342torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[74,     0] loss: 0.03299torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[75,     0] loss: 0.03257torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[76,     0] loss: 0.03215torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[77,     0] loss: 0.03174torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[78,     0] loss: 0.03133torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[79,     0] loss: 0.03092torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[80,     0] loss: 0.03051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[81,     0] loss: 0.03011torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[82,     0] loss: 0.02971torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[83,     0] loss: 0.02932torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[84,     0] loss: 0.02893torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[85,     0] loss: 0.02854torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[86,     0] loss: 0.02815torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[87,     0] loss: 0.02777torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[88,     0] loss: 0.02740torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[89,     0] loss: 0.02702torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[90,     0] loss: 0.02665torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[91,     0] loss: 0.02629torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[92,     0] loss: 0.02593torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[93,     0] loss: 0.02557torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[94,     0] loss: 0.02522torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[95,     0] loss: 0.02487torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[96,     0] loss: 0.02453torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[97,     0] loss: 0.02419torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[98,     0] loss: 0.02386torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[99,     0] loss: 0.02353torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[100,     0] loss: 0.02321torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[101,     0] loss: 0.02289torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[102,     0] loss: 0.02257torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[103,     0] loss: 0.02226torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[104,     0] loss: 0.02196torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[105,     0] loss: 0.02166torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[106,     0] loss: 0.02136torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[107,     0] loss: 0.02107torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[108,     0] loss: 0.02078torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[109,     0] loss: 0.02050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[110,     0] loss: 0.02022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[111,     0] loss: 0.01995torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[112,     0] loss: 0.01968torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[113,     0] loss: 0.01942torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[114,     0] loss: 0.01916torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[115,     0] loss: 0.01890torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[116,     0] loss: 0.01865torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[117,     0] loss: 0.01840torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[118,     0] loss: 0.01815torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[119,     0] loss: 0.01791torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[120,     0] loss: 0.01767torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[121,     0] loss: 0.01744torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[122,     0] loss: 0.01721torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[123,     0] loss: 0.01698torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[124,     0] loss: 0.01676torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[125,     0] loss: 0.01654torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[126,     0] loss: 0.01633torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[127,     0] loss: 0.01611torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[128,     0] loss: 0.01590torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[129,     0] loss: 0.01570torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[130,     0] loss: 0.01549torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[131,     0] loss: 0.01529torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[132,     0] loss: 0.01509torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[133,     0] loss: 0.01490torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[134,     0] loss: 0.01471torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[135,     0] loss: 0.01452torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[136,     0] loss: 0.01433torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[137,     0] loss: 0.01415torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[138,     0] loss: 0.01397torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[139,     0] loss: 0.01379torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[140,     0] loss: 0.01361torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[141,     0] loss: 0.01344torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[142,     0] loss: 0.01327torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[143,     0] loss: 0.01310torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[144,     0] loss: 0.01294torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[145,     0] loss: 0.01278torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[146,     0] loss: 0.01262torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[147,     0] loss: 0.01246torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[148,     0] loss: 0.01231torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[149,     0] loss: 0.01215torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[150,     0] loss: 0.01200torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[151,     0] loss: 0.01186torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[152,     0] loss: 0.01171torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[153,     0] loss: 0.01157torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[154,     0] loss: 0.01143torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[155,     0] loss: 0.01129torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[156,     0] loss: 0.01115torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[157,     0] loss: 0.01102torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[158,     0] loss: 0.01089torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[159,     0] loss: 0.01076torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[160,     0] loss: 0.01063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[161,     0] loss: 0.01051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[162,     0] loss: 0.01039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[163,     0] loss: 0.01026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[164,     0] loss: 0.01015torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[165,     0] loss: 0.01003torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[166,     0] loss: 0.00992torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[167,     0] loss: 0.00980torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[168,     0] loss: 0.00969torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[169,     0] loss: 0.00958torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[170,     0] loss: 0.00948torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[171,     0] loss: 0.00937torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[172,     0] loss: 0.00927torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[173,     0] loss: 0.00917torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[174,     0] loss: 0.00907torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[175,     0] loss: 0.00897torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[176,     0] loss: 0.00887torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[177,     0] loss: 0.00878torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[178,     0] loss: 0.00869torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[179,     0] loss: 0.00859torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[180,     0] loss: 0.00850torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[181,     0] loss: 0.00842torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[182,     0] loss: 0.00833torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[183,     0] loss: 0.00824torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[184,     0] loss: 0.00816torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[185,     0] loss: 0.00808torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[186,     0] loss: 0.00800torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[187,     0] loss: 0.00792torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[188,     0] loss: 0.00784torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[189,     0] loss: 0.00776torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[190,     0] loss: 0.00769torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[191,     0] loss: 0.00761torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[192,     0] loss: 0.00754torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[193,     0] loss: 0.00747torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[194,     0] loss: 0.00740torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[195,     0] loss: 0.00733torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[196,     0] loss: 0.00726torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[197,     0] loss: 0.00719torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[198,     0] loss: 0.00713torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[199,     0] loss: 0.00706torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[200,     0] loss: 0.00700torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[201,     0] loss: 0.00694torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[202,     0] loss: 0.00688torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[203,     0] loss: 0.00682torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[204,     0] loss: 0.00676torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[205,     0] loss: 0.00670torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[206,     0] loss: 0.00664torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[207,     0] loss: 0.00658torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[208,     0] loss: 0.00653torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[209,     0] loss: 0.00647torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[210,     0] loss: 0.00642torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[211,     0] loss: 0.00637torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[212,     0] loss: 0.00631torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[213,     0] loss: 0.00626torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[214,     0] loss: 0.00621torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[215,     0] loss: 0.00616torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[216,     0] loss: 0.00611torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[217,     0] loss: 0.00606torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[218,     0] loss: 0.00602torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[219,     0] loss: 0.00597torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[220,     0] loss: 0.00592torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[221,     0] loss: 0.00588torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[222,     0] loss: 0.00583torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[223,     0] loss: 0.00579torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[224,     0] loss: 0.00574torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[225,     0] loss: 0.00570torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[226,     0] loss: 0.00566torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[227,     0] loss: 0.00562torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[228,     0] loss: 0.00557torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[229,     0] loss: 0.00553torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[230,     0] loss: 0.00549torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[231,     0] loss: 0.00545torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[232,     0] loss: 0.00541torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[233,     0] loss: 0.00537torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[234,     0] loss: 0.00534torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[235,     0] loss: 0.00530torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[236,     0] loss: 0.00526torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[237,     0] loss: 0.00522torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[238,     0] loss: 0.00519torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[239,     0] loss: 0.00515torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[240,     0] loss: 0.00512torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[241,     0] loss: 0.00508torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[242,     0] loss: 0.00505torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[243,     0] loss: 0.00501torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[244,     0] loss: 0.00498torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[245,     0] loss: 0.00495torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[246,     0] loss: 0.00491torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[247,     0] loss: 0.00488torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[248,     0] loss: 0.00485torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[249,     0] loss: 0.00481torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[250,     0] loss: 0.00478torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[251,     0] loss: 0.00475torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[252,     0] loss: 0.00472torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[253,     0] loss: 0.00469torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[254,     0] loss: 0.00466torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[255,     0] loss: 0.00463torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[256,     0] loss: 0.00460torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[257,     0] loss: 0.00457torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[258,     0] loss: 0.00454torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[259,     0] loss: 0.00451torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[260,     0] loss: 0.00449torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[261,     0] loss: 0.00446torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[262,     0] loss: 0.00443torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[263,     0] loss: 0.00440torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[264,     0] loss: 0.00438torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[265,     0] loss: 0.00435torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[266,     0] loss: 0.00432torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[267,     0] loss: 0.00430torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[268,     0] loss: 0.00427torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[269,     0] loss: 0.00424torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[270,     0] loss: 0.00422torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[271,     0] loss: 0.00419torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[272,     0] loss: 0.00417torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[273,     0] loss: 0.00414torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[274,     0] loss: 0.00412torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[275,     0] loss: 0.00409torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[276,     0] loss: 0.00407torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[277,     0] loss: 0.00405torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[278,     0] loss: 0.00402torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[279,     0] loss: 0.00400torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[280,     0] loss: 0.00398torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[281,     0] loss: 0.00395torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[282,     0] loss: 0.00393torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[283,     0] loss: 0.00391torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[284,     0] loss: 0.00389torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[285,     0] loss: 0.00386torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[286,     0] loss: 0.00384torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[287,     0] loss: 0.00382torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[288,     0] loss: 0.00380torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[289,     0] loss: 0.00378torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[290,     0] loss: 0.00376torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[291,     0] loss: 0.00373torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[292,     0] loss: 0.00371torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[293,     0] loss: 0.00369torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[294,     0] loss: 0.00367torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[295,     0] loss: 0.00365torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[296,     0] loss: 0.00363torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[297,     0] loss: 0.00361torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[298,     0] loss: 0.00359torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[299,     0] loss: 0.00357torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[300,     0] loss: 0.00355torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[301,     0] loss: 0.00353torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[302,     0] loss: 0.00351torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[303,     0] loss: 0.00349torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[304,     0] loss: 0.00348torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[305,     0] loss: 0.00346torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[306,     0] loss: 0.00344torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[307,     0] loss: 0.00342torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[308,     0] loss: 0.00340torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[309,     0] loss: 0.00338torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[310,     0] loss: 0.00337torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[311,     0] loss: 0.00335torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[312,     0] loss: 0.00333torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[313,     0] loss: 0.00331torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[314,     0] loss: 0.00330torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[315,     0] loss: 0.00328torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[316,     0] loss: 0.00326torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[317,     0] loss: 0.00325torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[318,     0] loss: 0.00323torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[319,     0] loss: 0.00321torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[320,     0] loss: 0.00319torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[321,     0] loss: 0.00318torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[322,     0] loss: 0.00316torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[323,     0] loss: 0.00314torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[324,     0] loss: 0.00313torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[325,     0] loss: 0.00311torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[326,     0] loss: 0.00310torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[327,     0] loss: 0.00308torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[328,     0] loss: 0.00306torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[329,     0] loss: 0.00305torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[330,     0] loss: 0.00303torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[331,     0] loss: 0.00302torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[332,     0] loss: 0.00300torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[333,     0] loss: 0.00299torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[334,     0] loss: 0.00297torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[335,     0] loss: 0.00295torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[336,     0] loss: 0.00294torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[337,     0] loss: 0.00293torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[338,     0] loss: 0.00291torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[339,     0] loss: 0.00290torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[340,     0] loss: 0.00288torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[341,     0] loss: 0.00287torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[342,     0] loss: 0.00285torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[343,     0] loss: 0.00284torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[344,     0] loss: 0.00282torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[345,     0] loss: 0.00281torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[346,     0] loss: 0.00280torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[347,     0] loss: 0.00278torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[348,     0] loss: 0.00277torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[349,     0] loss: 0.00276torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[350,     0] loss: 0.00274torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[351,     0] loss: 0.00273torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[352,     0] loss: 0.00271torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[353,     0] loss: 0.00270torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[354,     0] loss: 0.00269torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[355,     0] loss: 0.00268torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[356,     0] loss: 0.00266torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[357,     0] loss: 0.00265torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[358,     0] loss: 0.00264torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[359,     0] loss: 0.00262torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[360,     0] loss: 0.00261torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[361,     0] loss: 0.00260torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[362,     0] loss: 0.00259torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[363,     0] loss: 0.00257torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[364,     0] loss: 0.00256torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[365,     0] loss: 0.00255torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[366,     0] loss: 0.00254torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[367,     0] loss: 0.00253torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[368,     0] loss: 0.00251torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[369,     0] loss: 0.00250torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[370,     0] loss: 0.00249torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[371,     0] loss: 0.00248torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[372,     0] loss: 0.00247torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[373,     0] loss: 0.00246torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[374,     0] loss: 0.00244torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[375,     0] loss: 0.00243torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[376,     0] loss: 0.00242torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[377,     0] loss: 0.00241torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[378,     0] loss: 0.00240torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[379,     0] loss: 0.00239torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[380,     0] loss: 0.00238torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[381,     0] loss: 0.00236torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[382,     0] loss: 0.00235torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[383,     0] loss: 0.00234torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[384,     0] loss: 0.00233torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[385,     0] loss: 0.00232torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[386,     0] loss: 0.00231torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[387,     0] loss: 0.00230torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[388,     0] loss: 0.00229torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[389,     0] loss: 0.00228torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[390,     0] loss: 0.00227torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[391,     0] loss: 0.00226torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[392,     0] loss: 0.00225torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[393,     0] loss: 0.00224torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[394,     0] loss: 0.00223torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[395,     0] loss: 0.00222torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[396,     0] loss: 0.00221torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[397,     0] loss: 0.00220torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[398,     0] loss: 0.00219torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[399,     0] loss: 0.00218torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[400,     0] loss: 0.00217torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[401,     0] loss: 0.00216torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[402,     0] loss: 0.00215torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[403,     0] loss: 0.00214torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[404,     0] loss: 0.00213torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[405,     0] loss: 0.00212torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[406,     0] loss: 0.00211torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[407,     0] loss: 0.00210torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[408,     0] loss: 0.00209torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[409,     0] loss: 0.00208torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[410,     0] loss: 0.00207torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[411,     0] loss: 0.00206torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[412,     0] loss: 0.00206torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[413,     0] loss: 0.00205torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[414,     0] loss: 0.00204torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[415,     0] loss: 0.00203torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[416,     0] loss: 0.00202torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[417,     0] loss: 0.00201torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[418,     0] loss: 0.00200torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[419,     0] loss: 0.00199torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[420,     0] loss: 0.00198torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[421,     0] loss: 0.00197torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[422,     0] loss: 0.00197torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[423,     0] loss: 0.00196torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[424,     0] loss: 0.00195torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[425,     0] loss: 0.00194torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[426,     0] loss: 0.00193torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[427,     0] loss: 0.00192torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[428,     0] loss: 0.00191torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[429,     0] loss: 0.00190torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[430,     0] loss: 0.00190torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[431,     0] loss: 0.00189torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[432,     0] loss: 0.00188torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[433,     0] loss: 0.00187torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[434,     0] loss: 0.00186torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[435,     0] loss: 0.00185torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[436,     0] loss: 0.00185torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[437,     0] loss: 0.00184torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[438,     0] loss: 0.00183torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[439,     0] loss: 0.00182torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[440,     0] loss: 0.00181torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[441,     0] loss: 0.00180torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[442,     0] loss: 0.00180torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[443,     0] loss: 0.00179torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[444,     0] loss: 0.00178torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[445,     0] loss: 0.00177torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[446,     0] loss: 0.00176torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[447,     0] loss: 0.00176torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[448,     0] loss: 0.00175torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[449,     0] loss: 0.00174torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[450,     0] loss: 0.00173torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[451,     0] loss: 0.00172torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[452,     0] loss: 0.00172torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[453,     0] loss: 0.00171torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[454,     0] loss: 0.00170torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[455,     0] loss: 0.00169torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[456,     0] loss: 0.00169torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[457,     0] loss: 0.00168torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[458,     0] loss: 0.00167torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[459,     0] loss: 0.00166torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[460,     0] loss: 0.00165torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[461,     0] loss: 0.00165torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[462,     0] loss: 0.00164torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[463,     0] loss: 0.00163torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[464,     0] loss: 0.00163torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[465,     0] loss: 0.00162torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[466,     0] loss: 0.00161torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[467,     0] loss: 0.00161torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[468,     0] loss: 0.00161torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[469,     0] loss: 0.00161torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[470,     0] loss: 0.00160torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[471,     0] loss: 0.00159torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[472,     0] loss: 0.00157torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[473,     0] loss: 0.00156torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[474,     0] loss: 0.00156torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[475,     0] loss: 0.00156torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[476,     0] loss: 0.00155torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[477,     0] loss: 0.00154torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[478,     0] loss: 0.00153torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[479,     0] loss: 0.00152torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[480,     0] loss: 0.00152torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[481,     0] loss: 0.00151torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[482,     0] loss: 0.00150torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[483,     0] loss: 0.00149torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[484,     0] loss: 0.00149torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[485,     0] loss: 0.00148torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[486,     0] loss: 0.00148torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[487,     0] loss: 0.00147torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[488,     0] loss: 0.00146torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[489,     0] loss: 0.00146torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[490,     0] loss: 0.00145torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[491,     0] loss: 0.00144torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[492,     0] loss: 0.00144torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[493,     0] loss: 0.00143torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[494,     0] loss: 0.00142torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[495,     0] loss: 0.00142torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[496,     0] loss: 0.00141torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[497,     0] loss: 0.00140torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[498,     0] loss: 0.00140torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[499,     0] loss: 0.00139torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[500,     0] loss: 0.00139torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[501,     0] loss: 0.00138torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[502,     0] loss: 0.00137torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[503,     0] loss: 0.00137torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[504,     0] loss: 0.00136torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[505,     0] loss: 0.00136torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[506,     0] loss: 0.00135torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[507,     0] loss: 0.00134torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[508,     0] loss: 0.00134torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[509,     0] loss: 0.00133torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[510,     0] loss: 0.00133torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[511,     0] loss: 0.00132torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[512,     0] loss: 0.00131torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[513,     0] loss: 0.00131torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[514,     0] loss: 0.00130torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[515,     0] loss: 0.00130torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[516,     0] loss: 0.00129torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[517,     0] loss: 0.00129torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[518,     0] loss: 0.00128torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[519,     0] loss: 0.00127torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[520,     0] loss: 0.00127torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[521,     0] loss: 0.00126torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[522,     0] loss: 0.00126torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[523,     0] loss: 0.00125torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[524,     0] loss: 0.00125torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[525,     0] loss: 0.00124torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[526,     0] loss: 0.00124torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[527,     0] loss: 0.00123torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[528,     0] loss: 0.00123torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[529,     0] loss: 0.00122torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[530,     0] loss: 0.00121torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[531,     0] loss: 0.00121torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[532,     0] loss: 0.00120torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[533,     0] loss: 0.00120torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[534,     0] loss: 0.00119torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[535,     0] loss: 0.00119torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[536,     0] loss: 0.00118torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[537,     0] loss: 0.00118torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[538,     0] loss: 0.00117torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[539,     0] loss: 0.00117torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[540,     0] loss: 0.00116torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[541,     0] loss: 0.00116torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[542,     0] loss: 0.00115torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[543,     0] loss: 0.00115torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[544,     0] loss: 0.00114torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[545,     0] loss: 0.00114torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[546,     0] loss: 0.00113torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[547,     0] loss: 0.00113torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[548,     0] loss: 0.00112torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[549,     0] loss: 0.00112torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[550,     0] loss: 0.00111torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[551,     0] loss: 0.00111torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[552,     0] loss: 0.00110torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[553,     0] loss: 0.00110torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[554,     0] loss: 0.00109torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[555,     0] loss: 0.00109torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[556,     0] loss: 0.00108torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[557,     0] loss: 0.00108torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[558,     0] loss: 0.00107torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[559,     0] loss: 0.00107torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[560,     0] loss: 0.00106torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[561,     0] loss: 0.00106torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[562,     0] loss: 0.00105torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[563,     0] loss: 0.00105torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[564,     0] loss: 0.00105torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[565,     0] loss: 0.00104torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[566,     0] loss: 0.00104torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[567,     0] loss: 0.00103torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[568,     0] loss: 0.00103torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[569,     0] loss: 0.00102torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[570,     0] loss: 0.00102torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[571,     0] loss: 0.00101torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[572,     0] loss: 0.00101torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[573,     0] loss: 0.00100torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[574,     0] loss: 0.00100torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[575,     0] loss: 0.00100torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[576,     0] loss: 0.00099torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[577,     0] loss: 0.00099torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[578,     0] loss: 0.00098torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[579,     0] loss: 0.00098torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[580,     0] loss: 0.00097torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[581,     0] loss: 0.00097torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[582,     0] loss: 0.00097torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[583,     0] loss: 0.00096torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[584,     0] loss: 0.00096torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[585,     0] loss: 0.00095torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[586,     0] loss: 0.00095torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[587,     0] loss: 0.00094torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[588,     0] loss: 0.00094torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[589,     0] loss: 0.00094torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[590,     0] loss: 0.00093torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[591,     0] loss: 0.00093torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[592,     0] loss: 0.00092torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[593,     0] loss: 0.00092torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[594,     0] loss: 0.00092torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[595,     0] loss: 0.00091torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[596,     0] loss: 0.00091torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[597,     0] loss: 0.00091torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[598,     0] loss: 0.00090torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[599,     0] loss: 0.00090torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[600,     0] loss: 0.00089torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[601,     0] loss: 0.00089torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[602,     0] loss: 0.00089torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[603,     0] loss: 0.00088torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[604,     0] loss: 0.00088torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[605,     0] loss: 0.00087torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[606,     0] loss: 0.00087torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[607,     0] loss: 0.00087torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[608,     0] loss: 0.00086torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[609,     0] loss: 0.00086torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[610,     0] loss: 0.00086torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[611,     0] loss: 0.00085torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[612,     0] loss: 0.00085torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[613,     0] loss: 0.00085torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[614,     0] loss: 0.00084torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[615,     0] loss: 0.00084torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[616,     0] loss: 0.00083torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[617,     0] loss: 0.00083torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[618,     0] loss: 0.00083torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[619,     0] loss: 0.00082torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[620,     0] loss: 0.00082torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[621,     0] loss: 0.00082torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[622,     0] loss: 0.00081torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[623,     0] loss: 0.00081torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[624,     0] loss: 0.00081torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[625,     0] loss: 0.00080torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[626,     0] loss: 0.00080torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[627,     0] loss: 0.00080torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[628,     0] loss: 0.00079torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[629,     0] loss: 0.00079torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[630,     0] loss: 0.00079torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[631,     0] loss: 0.00078torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[632,     0] loss: 0.00078torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[633,     0] loss: 0.00078torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[634,     0] loss: 0.00077torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[635,     0] loss: 0.00077torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[636,     0] loss: 0.00077torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[637,     0] loss: 0.00076torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[638,     0] loss: 0.00076torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[639,     0] loss: 0.00076torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[640,     0] loss: 0.00075torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[641,     0] loss: 0.00075torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[642,     0] loss: 0.00075torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[643,     0] loss: 0.00074torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[644,     0] loss: 0.00074torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[645,     0] loss: 0.00074torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[646,     0] loss: 0.00073torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[647,     0] loss: 0.00073torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[648,     0] loss: 0.00073torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[649,     0] loss: 0.00073torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[650,     0] loss: 0.00072torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[651,     0] loss: 0.00072torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[652,     0] loss: 0.00072torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[653,     0] loss: 0.00071torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[654,     0] loss: 0.00071torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[655,     0] loss: 0.00071torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[656,     0] loss: 0.00070torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[657,     0] loss: 0.00070torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[658,     0] loss: 0.00070torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[659,     0] loss: 0.00070torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[660,     0] loss: 0.00069torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[661,     0] loss: 0.00069torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[662,     0] loss: 0.00069torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[663,     0] loss: 0.00068torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[664,     0] loss: 0.00068torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[665,     0] loss: 0.00068torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[666,     0] loss: 0.00068torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[667,     0] loss: 0.00067torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[668,     0] loss: 0.00067torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[669,     0] loss: 0.00067torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[670,     0] loss: 0.00066torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[671,     0] loss: 0.00066torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[672,     0] loss: 0.00066torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[673,     0] loss: 0.00066torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[674,     0] loss: 0.00065torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[675,     0] loss: 0.00065torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[676,     0] loss: 0.00065torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[677,     0] loss: 0.00065torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[678,     0] loss: 0.00064torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[679,     0] loss: 0.00064torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[680,     0] loss: 0.00064torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[681,     0] loss: 0.00064torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[682,     0] loss: 0.00063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[683,     0] loss: 0.00063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[684,     0] loss: 0.00063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[685,     0] loss: 0.00063torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[686,     0] loss: 0.00062torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[687,     0] loss: 0.00062torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[688,     0] loss: 0.00062torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[689,     0] loss: 0.00062torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[690,     0] loss: 0.00061torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[691,     0] loss: 0.00061torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[692,     0] loss: 0.00061torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[693,     0] loss: 0.00061torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[694,     0] loss: 0.00060torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[695,     0] loss: 0.00060torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[696,     0] loss: 0.00060torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[697,     0] loss: 0.00060torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[698,     0] loss: 0.00059torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[699,     0] loss: 0.00059torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[700,     0] loss: 0.00059torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[701,     0] loss: 0.00059torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[702,     0] loss: 0.00058torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[703,     0] loss: 0.00058torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[704,     0] loss: 0.00058torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[705,     0] loss: 0.00058torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[706,     0] loss: 0.00057torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[707,     0] loss: 0.00057torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[708,     0] loss: 0.00057torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[709,     0] loss: 0.00057torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[710,     0] loss: 0.00057torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[711,     0] loss: 0.00056torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[712,     0] loss: 0.00056torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[713,     0] loss: 0.00056torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[714,     0] loss: 0.00056torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[715,     0] loss: 0.00055torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[716,     0] loss: 0.00055torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[717,     0] loss: 0.00055torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[718,     0] loss: 0.00055torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[719,     0] loss: 0.00055torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[720,     0] loss: 0.00054torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[721,     0] loss: 0.00054torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[722,     0] loss: 0.00054torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[723,     0] loss: 0.00054torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[724,     0] loss: 0.00053torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[725,     0] loss: 0.00053torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[726,     0] loss: 0.00053torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[727,     0] loss: 0.00053torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[728,     0] loss: 0.00053torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[729,     0] loss: 0.00052torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[730,     0] loss: 0.00052torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[731,     0] loss: 0.00052torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[732,     0] loss: 0.00052torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[733,     0] loss: 0.00052torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[734,     0] loss: 0.00051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[735,     0] loss: 0.00051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[736,     0] loss: 0.00051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[737,     0] loss: 0.00051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[738,     0] loss: 0.00051torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[739,     0] loss: 0.00050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[740,     0] loss: 0.00050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[741,     0] loss: 0.00050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[742,     0] loss: 0.00050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[743,     0] loss: 0.00050torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[744,     0] loss: 0.00049torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[745,     0] loss: 0.00049torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[746,     0] loss: 0.00049torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[747,     0] loss: 0.00049torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[748,     0] loss: 0.00049torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[749,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[750,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[751,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[752,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[753,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[754,     0] loss: 0.00048torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[755,     0] loss: 0.00047torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[756,     0] loss: 0.00047torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[757,     0] loss: 0.00047torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[758,     0] loss: 0.00047torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[759,     0] loss: 0.00047torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[760,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[761,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[762,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[763,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[764,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[765,     0] loss: 0.00046torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[766,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[767,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[768,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[769,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[770,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[771,     0] loss: 0.00045torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[772,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[773,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[774,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[775,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[776,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[777,     0] loss: 0.00044torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[778,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[779,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[780,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[781,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[782,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[783,     0] loss: 0.00043torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[784,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[785,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[786,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[787,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[788,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[789,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[790,     0] loss: 0.00042torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[791,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[792,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[793,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[794,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[795,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[796,     0] loss: 0.00041torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[797,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[798,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[799,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[800,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[801,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[802,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[803,     0] loss: 0.00040torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[804,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[805,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[806,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[807,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[808,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[809,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[810,     0] loss: 0.00039torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[811,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[812,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[813,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[814,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[815,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[816,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[817,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[818,     0] loss: 0.00038torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[819,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[820,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[821,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[822,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[823,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[824,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[825,     0] loss: 0.00037torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[826,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[827,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[828,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[829,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[830,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[831,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[832,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[833,     0] loss: 0.00036torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[834,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[835,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[836,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[837,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[838,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[839,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[840,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[841,     0] loss: 0.00035torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[842,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[843,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[844,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[845,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[846,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[847,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[848,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[849,     0] loss: 0.00034torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[850,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[851,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[852,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[853,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[854,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[855,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[856,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[857,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[858,     0] loss: 0.00033torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[859,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[860,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[861,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[862,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[863,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[864,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[865,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[866,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[867,     0] loss: 0.00032torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[868,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[869,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[870,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[871,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[872,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[873,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[874,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[875,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[876,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[877,     0] loss: 0.00031torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[878,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[879,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[880,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[881,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[882,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[883,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[884,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[885,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[886,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[887,     0] loss: 0.00030torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[888,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[889,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[890,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[891,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[892,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[893,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[894,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[895,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[896,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[897,     0] loss: 0.00029torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[898,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[899,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[900,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[901,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[902,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[903,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[904,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[905,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[906,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[907,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[908,     0] loss: 0.00028torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[909,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[910,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[911,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[912,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[913,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[914,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[915,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[916,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[917,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[918,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[919,     0] loss: 0.00027torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[920,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[921,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[922,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[923,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[924,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[925,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[926,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[927,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[928,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[929,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[930,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[931,     0] loss: 0.00026torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[932,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[933,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[934,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[935,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[936,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[937,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[938,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[939,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[940,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[941,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[942,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[943,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[944,     0] loss: 0.00025torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[945,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[946,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[947,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[948,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[949,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[950,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[951,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[952,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[953,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[954,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[955,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[956,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[957,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[958,     0] loss: 0.00024torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[959,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[960,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[961,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[962,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[963,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[964,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[965,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[966,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[967,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[968,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[969,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[970,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[971,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[972,     0] loss: 0.00023torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[973,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[974,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[975,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[976,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[977,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[978,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[979,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[980,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[981,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[982,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[983,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[984,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[985,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[986,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[987,     0] loss: 0.00022torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[988,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[989,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[990,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[991,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[992,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[993,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[994,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[995,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[996,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[997,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[998,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[999,     0] loss: 0.00021torch.Size([1000, 1, 400])torch.Size([1000, 64, 48])torch.Size([1000, 128, 22])torch.Size([1000, 10, 20])torch.Size([1000, 10, 1])torch.Size([1000, 10])[1000,     0] loss: 0.00021Finished Training</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(x_list, loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="output_11_0.png" alt="png"></p><h3 id="开始测试"><a href="#开始测试" class="headerlink" title="开始测试"></a>开始测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ds_test = bearDataset(</span><br><span class="line">    <span class="string">&#x27;C:/Users/JINTIAN/Desktop/代码/PytorchCode/项目数据集/test.csv&#x27;</span>, <span class="number">400</span>)</span><br><span class="line">dl_test = torch.utils.data.DataLoader(</span><br><span class="line">    ds_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data, lable <span class="keyword">in</span> dl_test:</span><br><span class="line">        outputs = net(data)</span><br><span class="line">        <span class="comment"># print(outputs)</span></span><br><span class="line">        <span class="comment"># print(lable[0])</span></span><br><span class="line"></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(predicted.item())</span></span><br><span class="line">        p = output2lable[predicted.item()]</span><br><span class="line">        <span class="comment"># print(p)</span></span><br><span class="line"></span><br><span class="line">        total += lable.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(p.equal(lable[<span class="number">0</span>])):</span><br><span class="line">            correct = correct+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;total is &#x27;</span>, total)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])torch.Size([1, 1, 400])torch.Size([1, 64, 48])torch.Size([1, 128, 22])torch.Size([1, 10, 20])torch.Size([1, 10, 1])torch.Size([1, 10])total is  300Accuracy of the network: 95 %</code></pre>]]></content>
    
    
    <summary type="html">封面：Bing每日壁纸：The Needles sea stacks</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Matlab基本语法</title>
    <link href="http://example.com/2022/08/31/matlab%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/"/>
    <id>http://example.com/2022/08/31/matlab%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/</id>
    <published>2022-08-31T02:17:06.103Z</published>
    <updated>2022-08-31T02:53:52.826Z</updated>
    
    <content type="html"><![CDATA[<h3 id="鼠鼠哀嚎"><a href="#鼠鼠哀嚎" class="headerlink" title="鼠鼠哀嚎"></a>鼠鼠哀嚎</h3><p>写的代码和别人的参考代码，当作知识索引了<br><del>(暑假要结束了捏，🐀🐀还不想开学捏)</del></p><h3 id="鼠鼠自己的"><a href="#鼠鼠自己的" class="headerlink" title="鼠鼠自己的"></a>鼠鼠自己的</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cos</span>(((<span class="number">1</span>+<span class="number">2</span>+<span class="number">3</span>+<span class="number">4</span>)^<span class="number">3</span>/<span class="number">5</span>)^<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">sin</span>((<span class="built_in">pi</span>)^<span class="number">0.5</span>)*<span class="built_in">log</span>(<span class="built_in">tan</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">sin</span>(<span class="built_in">cos</span>(<span class="built_in">pi</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">% temp  ans</span></span><br><span class="line"><span class="built_in">cos</span>(<span class="built_in">pi</span>)</span><br><span class="line"><span class="built_in">sin</span>(<span class="built_in">ans</span>)</span><br><span class="line">whos</span><br><span class="line"></span><br><span class="line">clc</span><br><span class="line"><span class="comment">% Format</span></span><br><span class="line"><span class="built_in">pi</span> <span class="comment">% 只显示四位</span></span><br><span class="line"></span><br><span class="line">format long</span><br><span class="line"><span class="built_in">pi</span> <span class="comment">% 显示多位，精度更高</span></span><br><span class="line"></span><br><span class="line">format <span class="built_in">rat</span> <span class="comment">% 分数</span></span><br><span class="line"><span class="built_in">pi</span></span><br><span class="line"></span><br><span class="line">clc</span><br><span class="line">clear</span><br><span class="line">whos</span><br><span class="line"><span class="comment">% 全部清空</span></span><br><span class="line"><span class="comment">% 开始计算向量</span></span><br><span class="line">a=[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line">b=[<span class="number">1</span>;<span class="number">2</span>;<span class="number">3</span>;<span class="number">4</span>]</span><br><span class="line"><span class="comment">% *是外积</span></span><br><span class="line">a*b</span><br><span class="line">c=b*a</span><br><span class="line"><span class="comment">% 矩阵的下标</span></span><br><span class="line">a(<span class="number">3</span>) <span class="comment">% 访问第三个，下标从1开始</span></span><br><span class="line">c(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">% 第一行第二列</span></span><br><span class="line">c(<span class="number">2</span>) <span class="comment">% 第二个(按照列找的！！！竖着数，不是横着数)(https://s2.loli.net/2022/08/25/eApG6EZWadtNQy4.png)</span></span><br><span class="line">c([<span class="number">1</span> <span class="number">3</span> <span class="number">5</span>]) <span class="comment">% 调出下标为1,3,5的数字形成一个数组</span></span><br><span class="line">c([<span class="number">1</span> <span class="number">3</span>; <span class="number">1</span> <span class="number">3</span>]) <span class="comment">% 调出这几个数，形成一个矩阵</span></span><br><span class="line">c([<span class="number">1</span> <span class="number">3</span>],[<span class="number">1</span> <span class="number">3</span>]) <span class="comment">% 形成了一个矩阵(https://s2.loli.net/2022/08/25/7NBZQL6AEXnvObU.png)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A=[<span class="number">1</span>:<span class="number">100</span>] <span class="comment">% 首项为1，末项是100，公差默认为1的数列</span></span><br><span class="line">B=[<span class="number">1</span>:<span class="number">2</span>:<span class="number">99</span>] <span class="comment">% 首项为1，末项是99，公差为2的数列</span></span><br><span class="line">c(<span class="number">1</span>,:)<span class="comment">%访问c的第一行全部 </span></span><br><span class="line">c(<span class="number">4</span>,:)=[] <span class="comment">%c的第四行全部去掉</span></span><br><span class="line"></span><br><span class="line">f=[a ; c] <span class="comment">%将a和c放在一起，成为一个新的矩阵 (上下连接)</span></span><br><span class="line">c(<span class="number">4</span>,:)=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">g=[f ,c]  <span class="comment">%左右拼接在一起(3X4和4X4拼接之后变成7X4)</span></span><br><span class="line">clc</span><br><span class="line"></span><br><span class="line"><span class="comment">% .* 是点乘，*是矩乘</span></span><br><span class="line"><span class="comment">% ./和/同理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 特殊矩阵</span></span><br><span class="line"><span class="built_in">eye</span>(<span class="number">2</span>) <span class="comment">% 生成一个主对角线为1，其余位置为0的矩阵</span></span><br><span class="line"><span class="built_in">zeros</span>(<span class="number">4</span>,<span class="number">5</span>) <span class="comment">% 生成一个0矩阵</span></span><br><span class="line"><span class="built_in">ones</span>(<span class="number">5</span>,<span class="number">6</span>) <span class="comment">% 生成一个全1矩阵</span></span><br><span class="line"><span class="built_in">diag</span>([<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]) <span class="comment">%生成一个对角线为“2,3,4”的一个矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 常用函数，这些函数不会改变原来的矩阵</span></span><br><span class="line">sum(f,<span class="number">2</span>) <span class="comment">% 按照列求和</span></span><br><span class="line"><span class="built_in">max</span>(f) <span class="comment">%返回每一列的最大值</span></span><br><span class="line"><span class="built_in">max</span>(<span class="built_in">max</span>(f)) <span class="comment">% 整个矩阵最大值</span></span><br><span class="line"><span class="comment">% min,sum,mean同理</span></span><br><span class="line"><span class="built_in">sort</span>(f) <span class="comment">% 返回一个矩阵，将每一列按照从小到大排序</span></span><br><span class="line"><span class="built_in">sortrows</span>(f) <span class="comment">% 按照每行第一个数字排列，自上往下从小到大排序</span></span><br><span class="line"><span class="built_in">size</span>(f) <span class="comment">%访问这个矩阵的长宽</span></span><br><span class="line"><span class="built_in">find</span>(<span class="number">3</span>) <span class="comment">% 访问矩阵中等于3的位置</span></span><br><span class="line">prod(<span class="number">1</span>:<span class="number">100</span>)<span class="comment">% 算1到n这个数列的乘积</span></span><br><span class="line"></span><br><span class="line">h=[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>; ...</span><br><span class="line">    <span class="number">6</span> <span class="number">5</span> <span class="number">4</span> <span class="number">3</span> <span class="number">2</span> <span class="number">1</span>];</span><br><span class="line">h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">% 模块尝试</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> =<span class="number">1</span>:<span class="number">10</span></span><br><span class="line">    x=<span class="built_in">linspace</span>(<span class="number">0</span>, <span class="number">10</span>,<span class="number">101</span>);</span><br><span class="line">    <span class="built_in">plot</span>(x,<span class="built_in">sin</span>(x+<span class="built_in">i</span>));</span><br><span class="line">    print(gcf,<span class="string">&#x27;-deps&#x27;</span>,strcat(<span class="string">&#x27;plot&#x27;</span>,num2str(<span class="built_in">i</span>),<span class="string">&#x27;ps&#x27;</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> =<span class="number">1</span>:<span class="number">10</span></span><br><span class="line">    x=<span class="built_in">linspace</span>(<span class="number">0</span>, <span class="number">10</span>,<span class="number">101</span>);</span><br><span class="line">    <span class="built_in">plot</span>(x,<span class="built_in">sin</span>(x+<span class="built_in">i</span>));</span><br><span class="line">    print(gcf,<span class="string">&#x27;-deps&#x27;</span>,strcat(<span class="string">&#x27;plot&#x27;</span>,num2str(<span class="built_in">i</span>),<span class="string">&#x27;ps&#x27;</span>));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"></span><br><span class="line">[Acc Force]=acc(<span class="number">20</span>,<span class="number">15</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[a,F]</span>=<span class="title">acc</span><span class="params">(v2,v1,t2,t1,m)</span></span></span><br><span class="line">    a=(v2-v1)./(t2-t1);</span><br><span class="line">    F=m.*a;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="鼠鼠看别人的"><a href="#鼠鼠看别人的" class="headerlink" title="鼠鼠看别人的"></a>鼠鼠看别人的</h3><p>清风：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% Matlab基本的小常识</span></span><br><span class="line"><span class="comment">% (1)在每一行的语句后面加上分号(一定要是英文的哦;中文的长这个样子；)表示不显示运行结果</span></span><br><span class="line">a = <span class="number">3</span>;</span><br><span class="line">a = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% (2)多行注释:选中要注释的若干语句,快捷键Ctrl+R</span></span><br><span class="line"><span class="comment">% a = 3;</span></span><br><span class="line"><span class="comment">% a = 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% (3)取消注释:选中要取消注释的语句,快捷键Ctrl+T</span></span><br><span class="line"><span class="comment">% 我想要取消注释下面这行</span></span><br><span class="line"><span class="comment">% 还有这一行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% clear可以清楚工作区的所有变量</span></span><br><span class="line">clear</span><br><span class="line"></span><br><span class="line"><span class="comment">% clc可以清除命令行窗口中的所有文本,让屏幕变得干净</span></span><br><span class="line">clc</span><br><span class="line"></span><br><span class="line"><span class="comment">% 所以大家在很多代码开头，都会见到:</span></span><br><span class="line">clear;clc   <span class="comment">% 分号也用于区分行。</span></span><br><span class="line"><span class="comment">% 这两条一起使用，起到“初始化”的作用，防止之前的结果对新脚本文件（后缀名是 .m）产生干扰。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 输出和输入函数(disp 和 input)</span></span><br><span class="line"><span class="comment">% disp函数</span></span><br><span class="line"><span class="comment">% matlab中disp()就是屏幕输出函数，类似于c语言中的printf（）函数</span></span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]    <span class="comment">%同一行中间用逗号分隔，也可以不用逗号，直接用空格</span></span><br><span class="line">a = [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"><span class="built_in">disp</span>(a) </span><br><span class="line"><span class="comment">% 注意，disp函数比较特殊，这里可要分号，可不要分号哦</span></span><br><span class="line"><span class="built_in">disp</span>(a);</span><br><span class="line"><span class="comment">% matlab中两个字符串的合并有两种方法</span></span><br><span class="line"><span class="comment">% （1）strcat(str1,str2……,strn) </span></span><br><span class="line"> strcat(<span class="string">&#x27;字符串1&#x27;</span>,<span class="string">&#x27;字符串2&#x27;</span>) </span><br><span class="line"><span class="comment">% （2）[str 1,str 2，……, str n]或[str1  str2  ……  strn]</span></span><br><span class="line">[<span class="string">&#x27;字符串1&#x27;</span>  <span class="string">&#x27;字符串2&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;字符串1&#x27;</span>,<span class="string">&#x27;字符串2&#x27;</span>]</span><br><span class="line"><span class="comment">% 一个有用的字符串函数：num2str  将数字转换为字符串</span></span><br><span class="line">c = <span class="number">100</span></span><br><span class="line">num2str(c)</span><br><span class="line"><span class="built_in">disp</span>([<span class="string">&#x27;c的取值为&#x27;</span> num2str(c)])</span><br><span class="line"><span class="built_in">disp</span>(strcat(<span class="string">&#x27;c的取值为&#x27;</span>, num2str(c)))</span><br><span class="line"></span><br><span class="line"><span class="comment">% input函数</span></span><br><span class="line"><span class="comment">% 一般我们会将输入的数、向量、矩阵、字符串等赋给一个变量，这里我们赋给A</span></span><br><span class="line">A = input(<span class="string">&#x27;请输入A：&#x27;</span>);</span><br><span class="line">B = input(<span class="string">&#x27;请输入B：&#x27;</span>)</span><br><span class="line"><span class="comment">% 注意观察工作区，并体会input后面加分号和不加分号的区别</span></span><br><span class="line"><span class="comment">% 一个会输出结果，一个不会 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% sum函数</span></span><br><span class="line"><span class="comment">% （1）如果是向量（无论是行向量还是列向量），都是直接求和</span></span><br><span class="line">E = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">sum(E)</span><br><span class="line">E = [<span class="number">1</span>;<span class="number">2</span>;<span class="number">3</span>]</span><br><span class="line">sum(E)</span><br><span class="line"><span class="comment">% （2）如果是矩阵，则需要根据行和列的方向作区分</span></span><br><span class="line">clc</span><br><span class="line">E = [<span class="number">1</span>,<span class="number">2</span>;<span class="number">3</span>,<span class="number">4</span>;<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="comment">% a=sum(x); %按列求和(得到一个行向量）</span></span><br><span class="line">a = sum(E)</span><br><span class="line">a = sum(E,<span class="number">1</span>)</span><br><span class="line"><span class="comment">% a=sum(x,2); %按行求和(得到一个列向量）</span></span><br><span class="line">a = sum(E,<span class="number">2</span>)</span><br><span class="line"><span class="comment">% a=sum(x(:));%对整个矩阵求和</span></span><br><span class="line">a = sum(sum(E))</span><br><span class="line">a = sum(E(:))</span><br><span class="line"></span><br><span class="line"><span class="comment">%% 基础：matlab中如何提取矩阵中指定位置的元素？</span></span><br><span class="line"><span class="comment">% （1）取指定行和列的一个元素（输出的是一个值）</span></span><br><span class="line">clc;A=[<span class="number">1</span> <span class="number">1</span> <span class="number">4</span> <span class="number">1</span>/<span class="number">3</span> <span class="number">3</span>;<span class="number">1</span> <span class="number">1</span> <span class="number">4</span> <span class="number">1</span>/<span class="number">3</span> <span class="number">3</span>;<span class="number">1</span>/<span class="number">4</span> <span class="number">1</span>/<span class="number">4</span> <span class="number">1</span> <span class="number">1</span>/<span class="number">3</span> <span class="number">1</span>/<span class="number">2</span>;<span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">1</span> <span class="number">3</span>;<span class="number">1</span>/<span class="number">3</span> <span class="number">1</span>/<span class="number">3</span> <span class="number">2</span> <span class="number">1</span>/<span class="number">3</span> <span class="number">1</span>];</span><br><span class="line">A</span><br><span class="line">A(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">A(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">% （2）取指定的某一行的全部元素（输出的是一个行向量）</span></span><br><span class="line">clc;A</span><br><span class="line">A(<span class="number">2</span>,:)</span><br><span class="line">A(<span class="number">5</span>,:)</span><br><span class="line"><span class="comment">% （3）取指定的某一列的全部元素（输出的是一个列向量）</span></span><br><span class="line">clc;A</span><br><span class="line">A(:,<span class="number">1</span>)</span><br><span class="line">A(:,<span class="number">3</span>)</span><br><span class="line"><span class="comment">% （4）取指定的某些行的全部元素（输出的是一个矩阵）</span></span><br><span class="line">clc;A</span><br><span class="line">A([<span class="number">2</span>,<span class="number">5</span>],:)      <span class="comment">% 只取第二行和第五行（一共2行）</span></span><br><span class="line">A(<span class="number">2</span>:<span class="number">5</span>,:)        <span class="comment">% 取第二行到第五行（一共4行）</span></span><br><span class="line">A(<span class="number">2</span>:<span class="number">2</span>:<span class="number">5</span>,:)     <span class="comment">% 取第二行和第四行 （从2开始，每次递增2个单位，到5结束）</span></span><br><span class="line"><span class="number">1</span>:<span class="number">3</span>:<span class="number">10</span></span><br><span class="line"><span class="number">10</span>:<span class="number">-1</span>:<span class="number">1</span></span><br><span class="line">A(<span class="number">2</span>:<span class="keyword">end</span>,:)      <span class="comment">% 取第二行到最后一行</span></span><br><span class="line">A(<span class="number">2</span>:<span class="keyword">end</span><span class="number">-1</span>,:)    <span class="comment">% 取第二行到倒数第二行</span></span><br><span class="line"><span class="comment">% （5）取全部元素(按列拼接的，最终输出的是一个列向量)</span></span><br><span class="line">clc;A</span><br><span class="line">A(:)</span><br><span class="line"></span><br><span class="line"><span class="comment">%% size函数</span></span><br><span class="line">clc;</span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>;<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">B = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">size</span>(A)</span><br><span class="line"><span class="built_in">size</span>(B)</span><br><span class="line"><span class="comment">% size(A)函数是用来求矩阵A的大小的,它返回一个行向量，第一个元素是矩阵的行数，第二个元素是矩阵的列数</span></span><br><span class="line">[r,c] = <span class="built_in">size</span>(A)</span><br><span class="line"><span class="comment">% 将矩阵A的行数返回到第一个变量r，将矩阵的列数返回到第二个变量c</span></span><br><span class="line">r = <span class="built_in">size</span>(A,<span class="number">1</span>)  <span class="comment">%返回行数</span></span><br><span class="line">c = <span class="built_in">size</span>(A,<span class="number">2</span>) <span class="comment">%返回列数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% repmat函数</span></span><br><span class="line"><span class="comment">% B = repmat(A,m,n):将矩阵A复制m×n块，即把A作为B的元素，B由m×n个A平铺而成。</span></span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>;<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">B = <span class="built_in">repmat</span>(A,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">B = <span class="built_in">repmat</span>(A,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%% Matlab中矩阵的运算</span></span><br><span class="line"><span class="comment">% MATLAB在矩阵的运算中，“*”号和“/”号代表矩阵之间的乘法与除法(A/B = A*inv(B))</span></span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>;<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">B = [<span class="number">1</span>,<span class="number">0</span>;<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">A * B</span><br><span class="line">inv(B)  <span class="comment">% 求B的逆矩阵</span></span><br><span class="line">B * inv(B)</span><br><span class="line">A * inv(B)</span><br><span class="line">A / B</span><br><span class="line"></span><br><span class="line"><span class="comment">% 两个形状相同的矩阵对应元素之间的乘除法需要使用“.*”和“./”</span></span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>;<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">B = [<span class="number">1</span>,<span class="number">0</span>;<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">A .* B</span><br><span class="line">A ./ B</span><br><span class="line"></span><br><span class="line"><span class="comment">% 每个元素同时和常数相乘或相除操作都可以使用</span></span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>;<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">A * <span class="number">2</span></span><br><span class="line">A .* <span class="number">2</span></span><br><span class="line">A / <span class="number">2</span> </span><br><span class="line">A ./ <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 每个元素同时乘方时只能用 .^</span></span><br><span class="line">A = [<span class="number">1</span>,<span class="number">2</span>;<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">A .^ <span class="number">2</span></span><br><span class="line">A ^ <span class="number">2</span> </span><br><span class="line">A * A</span><br><span class="line"></span><br><span class="line"><span class="comment">%% Matlab中求特征值和特征向量</span></span><br><span class="line"><span class="comment">% 在Matlab中，计算矩阵A的特征值和特征向量的函数是eig(A),其中最常用的两个用法：</span></span><br><span class="line">A = [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> ;<span class="number">2</span> <span class="number">2</span> <span class="number">1</span>;<span class="number">2</span> <span class="number">0</span> <span class="number">3</span>]</span><br><span class="line"><span class="comment">% （1）E=eig(A)：求矩阵A的全部特征值，构成向量E。</span></span><br><span class="line">E=eig(A)</span><br><span class="line"><span class="comment">% （2）[V,D]=eig(A)：求矩阵A的全部特征值，构成对角阵D，并求A的特征向量构成V的列向量。（V的每一列都是D中与之相同列的特征值的特征向量）</span></span><br><span class="line">[V,D]=eig(A)</span><br><span class="line"></span><br><span class="line"><span class="comment">%% find函数的基本用法</span></span><br><span class="line"><span class="comment">% 下面例子来自博客：https://www.cnblogs.com/anzhiwu815/p/5907033.html 博客内有更加深入的探究</span></span><br><span class="line"><span class="comment">% find函数，它可以用来返回向量或者矩阵中不为0的元素的位置索引。</span></span><br><span class="line">clc;X = [<span class="number">1</span> <span class="number">0</span> <span class="number">4</span> <span class="number">-3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">8</span> <span class="number">6</span>]</span><br><span class="line">ind = <span class="built_in">find</span>(X)</span><br><span class="line"><span class="comment">% 其有多种用法，比如返回前2个不为0的元素的位置：</span></span><br><span class="line">ind = <span class="built_in">find</span>(X,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">%上面针对的是向量（一维），若X是一个矩阵（二维，有行和列），索引该如何返回呢？</span></span><br><span class="line">clc;X = [<span class="number">1</span> <span class="number">-3</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">0</span> <span class="number">8</span>;<span class="number">4</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line">ind = <span class="built_in">find</span>(X)</span><br><span class="line"><span class="comment">% 这是因为在Matlab在存储矩阵时，是一列一列存储的，我们可以做一下验证：</span></span><br><span class="line">X(<span class="number">4</span>)</span><br><span class="line"><span class="comment">% 假如你需要按照行列的信息输出该怎么办呢？</span></span><br><span class="line">[r,c] = <span class="built_in">find</span>(X)</span><br><span class="line">[r,c] = <span class="built_in">find</span>(X,<span class="number">1</span>) <span class="comment">%只找第一个非0元素</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 矩阵与常数的大小判断运算</span></span><br><span class="line"><span class="comment">% 共有三种运算符：大于&gt; ;小于&lt; ;等于 ==  （一个等号表示赋值；两个等号表示判断）</span></span><br><span class="line">clc</span><br><span class="line">X = [<span class="number">1</span> <span class="number">-3</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">0</span> <span class="number">8</span>;<span class="number">4</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line">X &gt; <span class="number">0</span></span><br><span class="line">X == <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 判断语句</span></span><br><span class="line"><span class="comment">% Matlab的判断语句，if所在的行不需要冒号，语句的最后一定要以end结尾 ；中间的语句要注意缩进。</span></span><br><span class="line">a = input(<span class="string">&#x27;请输入考试分数:&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> a &gt;= <span class="number">85</span>  </span><br><span class="line">    <span class="built_in">disp</span>(<span class="string">&#x27;成绩优秀&#x27;</span>)</span><br><span class="line"><span class="keyword">elseif</span> a &gt;= <span class="number">60</span> </span><br><span class="line">    <span class="built_in">disp</span>(<span class="string">&#x27;成绩合格&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">disp</span>(<span class="string">&#x27;成绩挂科&#x27;</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">封面：史蒂文·斯皮尔伯格，好莱坞导演。
代表作：《拯救大兵瑞恩》，《夺宝奇兵》，《侏罗纪公园》，《辛德勒的名单》等</summary>
    
    
    
    <category term="数学建模" scheme="http://example.com/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-经典CNN模型-NIN和GoogLeNet</title>
    <link href="http://example.com/2022/08/22/CNN_NIN/"/>
    <id>http://example.com/2022/08/22/CNN_NIN/</id>
    <published>2022-08-22T11:26:38.684Z</published>
    <updated>2022-08-22T11:32:23.299Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NIN网络"><a href="#NIN网络" class="headerlink" title="NIN网络"></a>NIN网络</h1><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/cb21496d62d74728cd9a6aa367b58381.png"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　分类总数为10</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential output shape:     torch.Size([1, 96, 54, 54])MaxPool2d output shape:     torch.Size([1, 96, 26, 26])Sequential output shape:     torch.Size([1, 256, 26, 26])MaxPool2d output shape:     torch.Size([1, 256, 12, 12])Sequential output shape:     torch.Size([1, 384, 12, 12])MaxPool2d output shape:     torch.Size([1, 384, 5, 5])Dropout output shape:     torch.Size([1, 384, 5, 5])Sequential output shape:     torch.Size([1, 10, 5, 5])AdaptiveAvgPool2d output shape:     torch.Size([1, 10, 1, 1])Flatten output shape:     torch.Size([1, 10])</code></pre><h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><h3 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h3><p>冷知识：名字中的L大写是为了致敬LeNet，网络结构和思想与LeNet无关<br><img src="https://pic.rmb.bdstatic.com/bjh/8bdee662348edbdaeee71bed691ec4da.png"><br><img src="https://pic.rmb.bdstatic.com/bjh/5e842212b3dbccf9c70b453526a4e2dc.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h5 id="首先实现inception块的定义"><a href="#首先实现inception块的定义" class="headerlink" title="首先实现inception块的定义"></a>首先实现inception块的定义</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="实例化五个inception块"><a href="#实例化五个inception块" class="headerlink" title="实例化五个inception块"></a>实例化五个inception块</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="封装成网络"><a href="#封装成网络" class="headerlink" title="封装成网络"></a>封装成网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># “net”必须是一个将被传递给“d2l.train_ch6（）”的函数。</span></span><br><span class="line"><span class="comment"># 为了利用我们现有的CPU/GPU设备，这样模型构建/编译需要在“strategy.scope()”</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个网络</span></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="查看各个层"><a href="#查看各个层" class="headerlink" title="查看各个层"></a>查看各个层</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential output shape:     torch.Size([1, 64, 24, 24])Sequential output shape:     torch.Size([1, 192, 12, 12])Sequential output shape:     torch.Size([1, 480, 6, 6])Sequential output shape:     torch.Size([1, 832, 3, 3])Sequential output shape:     torch.Size([1, 1024])Linear output shape:     torch.Size([1, 10])</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NIN网络&quot;&gt;&lt;a href=&quot;#NIN网络&quot; class=&quot;headerlink&quot; title=&quot;NIN网络&quot;&gt;&lt;/a&gt;NIN网络&lt;/h1&gt;&lt;h3 id=&quot;结构&quot;&gt;&lt;a href=&quot;#结构&quot; class=&quot;headerlink&quot; title=&quot;结构&quot;&gt;&lt;/a&gt;结</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-经典CNN模型-ResNet</title>
    <link href="http://example.com/2022/08/22/CNN_ResNet/"/>
    <id>http://example.com/2022/08/22/CNN_ResNet/</id>
    <published>2022-08-22T11:26:33.713Z</published>
    <updated>2022-08-22T11:47:43.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/5c21f2f2a6e1558713898d96974072bc.png"><br>吴恩达老师讲的ResNet理论，通俗易懂(吴恩达老师yyds)<br><a href="https://www.bilibili.com/video/BV1FT4y1E74V?p=121&vd_source=4b97cfdf07bf7a7f8cea18c60a0c5280">传送门</a></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment"># @save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 定义两层卷积</span></span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:  <span class="comment"># 如果要使用1x1卷积网络</span></span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="comment"># 在第二个层激活之前，加上第一层的初始数据，实现残差</span></span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://pic.rmb.bdstatic.com/bjh/a0e1de3a7319e9e7f7fef539dccfeb37.png"><br>ResNet实现<br><img src="https://pic.rmb.bdstatic.com/bjh/86ce0c424e706489773ef33e8d9eebf8.png"></p><p>ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模块</span></span><br><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>), nn.BatchNorm2d(</span><br><span class="line">    <span class="number">64</span>), nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 残差块</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:  <span class="comment"># 不是第一个</span></span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                       use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>封装一个整的网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d(</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>)), nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>打印一下每一层的信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><pre><code>Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 128, 28, 28])Sequential output shape:     torch.Size([1, 256, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])AdaptiveAvgPool2d output shape:     torch.Size([1, 512, 1, 1])Flatten output shape:     torch.Size([1, 512])Linear output shape:     torch.Size([1, 10])</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ResNet&quot;&gt;&lt;a href=&quot;#ResNet&quot; class=&quot;headerlink&quot; title=&quot;ResNet&quot;&gt;&lt;/a&gt;ResNet&lt;/h1&gt;&lt;h3 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-批量归一化</title>
    <link href="http://example.com/2022/08/22/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://example.com/2022/08/22/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/</id>
    <published>2022-08-22T11:26:26.888Z</published>
    <updated>2022-08-22T11:29:38.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h1><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>加速收敛，加快训练速度，一般不改变精度<br>(不与dropout混用)<br><img src="https://pic.rmb.bdstatic.com/bjh/75cfb31f3620da1934fbe9232f43bc98.png"></p><h3 id="pytorch框架实现"><a href="#pytorch框架实现" class="headerlink" title="pytorch框架实现"></a>pytorch框架实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># BatchNorm即批量归一化</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">120</span>), nn.BatchNorm1d(<span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.BatchNorm1d(<span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;批量归一化&quot;&gt;&lt;a href=&quot;#批量归一化&quot; class=&quot;headerlink&quot; title=&quot;批量归一化&quot;&gt;&lt;/a&gt;批量归一化&lt;/h1&gt;&lt;h3 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-CNN网络实践</title>
    <link href="http://example.com/2022/08/20/%E4%B8%87%E8%81%AA%E8%80%81%E5%B8%88%E9%A1%B9%E7%9B%AE/"/>
    <id>http://example.com/2022/08/20/%E4%B8%87%E8%81%AA%E8%80%81%E5%B8%88%E9%A1%B9%E7%9B%AE/</id>
    <published>2022-08-20T09:23:24.635Z</published>
    <updated>2022-08-22T12:25:19.646Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CNN网络实践"><a href="#CNN网络实践" class="headerlink" title="CNN网络实践"></a>CNN网络实践</h1><h3 id="引入包"><a href="#引入包" class="headerlink" title="引入包"></a>引入包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="读入数据集"><a href="#读入数据集" class="headerlink" title="读入数据集"></a>读入数据集</h3><p><a href="https://zhuanlan.zhihu.com/p/37471802">onehot编码</a></p><p><a href="https://blog.csdn.net/qq_39368111/article/details/110435536?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166089858416782184695575%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166089858416782184695575&biz_id=0&spm=1018.2226.3001.4187">iloc()函数</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/9ed2a7ecf1645053aadfbf3ebf1a8eb3.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">targetDict = &#123;</span><br><span class="line">    <span class="number">109</span>: torch.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">122</span>: torch.Tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">135</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">174</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">189</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">201</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">213</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">226</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">238</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]),</span><br><span class="line">    <span class="number">97</span>: torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 定义了一个字典[label:onehot 编码]</span></span><br><span class="line"></span><br><span class="line">output2lable = [torch.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]),</span><br><span class="line">                torch.Tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(targetDict[<span class="number">109</span>])</span><br><span class="line"><span class="comment"># 定义一个数据集</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">bearDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, dim</span>):  <span class="comment"># dim为样本的维度</span></span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.df = pd.read_csv(csv_file)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># 取一行，一行的数目是[0,dim)</span></span><br><span class="line">        x = torch.tensor(</span><br><span class="line">            self.df.iloc[idx, <span class="number">0</span>:self.dim].values, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        x = x.view(<span class="number">1</span>, <span class="number">400</span>)</span><br><span class="line">        y = self.df.iloc[idx, <span class="number">2000</span>]</span><br><span class="line">        y = targetDict[y]</span><br><span class="line">        <span class="keyword">return</span> [x, y]</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ds_demo = bearDataset(</span><br><span class="line">    <span class="string">&#x27;C:/Users/JINTIAN/Desktop/代码/PytorchCode/项目数据集/train.csv&#x27;</span>, <span class="number">400</span>)</span><br><span class="line">dl = torch.utils.data.DataLoader(</span><br><span class="line">    ds_demo, batch_size=<span class="number">1000</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)  <span class="comment"># 只训练10个</span></span><br><span class="line"><span class="comment"># dl是我们定义的一个数据加载函数，一次加载10个</span></span><br><span class="line">x1, y1 = ds_demo.__getitem__(<span class="number">11</span>)</span><br><span class="line"><span class="built_in">print</span>(x1)  <span class="comment"># 打印数据</span></span><br><span class="line"><span class="built_in">print</span>(x1.shape)</span><br><span class="line"><span class="built_in">print</span>(y1)  <span class="comment"># 标签</span></span><br><span class="line">idata = <span class="built_in">iter</span>(dl)</span><br><span class="line">x, y = <span class="built_in">next</span>(idata)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.1037,  0.0555, -0.0096, -0.0547, -0.0682, -0.0524, -0.0179,  0.0238,          0.0661,  0.0935,  0.0918,  0.0409, -0.0225, -0.0880, -0.1535, -0.1640,         -0.1462, -0.0985, -0.0415, -0.0083,  0.0273,  0.0409,  0.0382,  0.0054,         -0.0501, -0.0887, -0.1039, -0.0801, -0.0440, -0.0108,  0.0221,  0.0478,          0.0822,  0.1081,  0.0855,  0.0257, -0.0321, -0.0657, -0.0611, -0.0365,         -0.0177,  0.0048,  0.0332,  0.0684,  0.0951,  0.0903,  0.0463, -0.0211,         -0.0734, -0.0895, -0.0724, -0.0401, -0.0040,  0.0463,  0.0989,  0.1277,          0.1362,  0.1066,  0.0250, -0.0663, -0.1204, -0.1287, -0.0947, -0.0465,         -0.0038,  0.0509,  0.0989,  0.1500,  0.1663,  0.1122,  0.0467, -0.0198,         -0.0565, -0.0590, -0.0482, -0.0350, -0.0244,  0.0083,  0.0446,  0.0732,          0.0565,  0.0046, -0.0486, -0.0862, -0.0665, -0.0342,  0.0002,  0.0323,          0.0515,  0.0968,  0.1145,  0.0912,  0.0515, -0.0131, -0.0434, -0.0467,         -0.0334,  0.0104,  0.0365,  0.0676,  0.0953,  0.1364,  0.1775,  0.1512,          0.0962,  0.0169, -0.0463, -0.0680, -0.0793, -0.0597, -0.0271,  0.0136,          0.0643,  0.0928,  0.0962,  0.0607,  0.0006, -0.0359, -0.0415, -0.0238,         -0.0044,  0.0021,  0.0323,  0.0634,  0.0882,  0.1147,  0.0935,  0.0647,          0.0344,  0.0188,  0.0265,  0.0117,  0.0042, -0.0017,  0.0192,  0.0640,          0.0859,  0.0910,  0.0732,  0.0478,  0.0417,  0.0419,  0.0411,  0.0317,          0.0119,  0.0119,  0.0311,  0.0668,  0.0718,  0.0446,  0.0169, -0.0148,         -0.0154, -0.0169, -0.0165, -0.0125, -0.0008,  0.0509,  0.0933,  0.1318,          0.1400,  0.1026,  0.0630,  0.0342,  0.0323,  0.0213,  0.0044,  0.0021,          0.0225,  0.0584,  0.0857,  0.0966,  0.0695,  0.0321,  0.0094,  0.0119,          0.0250,  0.0204,  0.0179,  0.0390,  0.0774,  0.1187,  0.1381,  0.1145,          0.0638, -0.0031, -0.0365, -0.0307, -0.0238, -0.0052,  0.0136,  0.0613,          0.1254,  0.1519,  0.1490,  0.1099,  0.0522,  0.0271,  0.0213,  0.0029,         -0.0211, -0.0457, -0.0570, -0.0338,  0.0067,  0.0190, -0.0038, -0.0492,         -0.0878, -0.1104, -0.1279, -0.1406, -0.1510, -0.1406, -0.1168, -0.0559,         -0.0077, -0.0255, -0.0839, -0.1636, -0.1807, -0.1823, -0.1802, -0.1356,         -0.1137, -0.0780, -0.0478, -0.0192, -0.0021, -0.0570, -0.0980, -0.1204,         -0.1043, -0.0398, -0.0167, -0.0131, -0.0146, -0.0013,  0.0302,  0.0388,          0.0227, -0.0255, -0.0524, -0.0469, -0.0273, -0.0058, -0.0148, -0.0184,         -0.0104,  0.0244,  0.0695,  0.0862,  0.0899,  0.0613,  0.0492,  0.0726,          0.0682,  0.0584,  0.0403,  0.0355,  0.0749,  0.1233,  0.1594,  0.1212,          0.0584,  0.0407,  0.0494,  0.0784,  0.0993,  0.1118,  0.1064,  0.1043,          0.1172,  0.1181,  0.0991,  0.0688,  0.0524,  0.0513,  0.0668,  0.0776,          0.0505, -0.0002, -0.0273, -0.0150,  0.0255,  0.0611,  0.0609,  0.0355,          0.0240,  0.0365,  0.0401,  0.0338, -0.0163, -0.0757, -0.0816, -0.0513,          0.0058,  0.0409,  0.0478,  0.0330,  0.0202,  0.0267,  0.0046, -0.0426,         -0.0941, -0.1218, -0.1024, -0.0576, -0.0211, -0.0152, -0.0240, -0.0184,         -0.0038,  0.0050, -0.0259, -0.0953, -0.1602, -0.1844, -0.1462, -0.1020,         -0.0720, -0.0471, -0.0188,  0.0332,  0.0736,  0.0755,  0.0067, -0.0897,         -0.1396, -0.1375, -0.0895, -0.0421, -0.0071,  0.0060,  0.0081,  0.0302,          0.0252, -0.0063, -0.0653, -0.1141, -0.1089, -0.0741, -0.0073,  0.0369,          0.0515,  0.0640,  0.0722,  0.0859,  0.0471, -0.0319, -0.1164, -0.1748,         -0.1467, -0.0870, -0.0113,  0.0478,  0.0786,  0.1218,  0.1323,  0.1256,          0.0657, -0.0305, -0.0885, -0.1185, -0.0755, -0.0156,  0.0371,  0.0907,          0.1133,  0.1556,  0.1792,  0.1675,  0.1118,  0.0211, -0.0350, -0.0553,         -0.0229,  0.0181,  0.0294,  0.0486,  0.0826,  0.1285,  0.1287,  0.0832,          0.0363, -0.0113, -0.0096,  0.0357,  0.0962,  0.1425,  0.1508,  0.1658,          0.1727,  0.1621,  0.1273,  0.0713,  0.0288, -0.0054,  0.0021,  0.0309]])torch.Size([1, 400])tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])tensor([[[-0.3764, -0.6076, -0.8242,  ...,  0.2153,  0.3919,  0.5388]],        [[ 0.2809,  0.4340,  0.5613,  ..., -0.3155, -0.4678, -0.5692]],        [[-1.0104, -0.8004, -0.4507,  ...,  0.5763,  0.4357,  0.2191]],        ...,        [[-0.0013, -0.0332, -0.0559,  ..., -0.1903, -0.0924,  0.0196]],        [[ 0.0785,  0.0317,  0.0797,  ..., -0.1131, -0.2045, -0.1916]],        [[-0.0096, -0.0338, -0.0355,  ..., -0.1273, -0.1494, -0.1273]]])torch.Size([1000, 1, 400])tensor([[1., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 1., 0., 0.],        [0., 0., 1.,  ..., 0., 0., 0.],        ...,        [0., 1., 0.,  ..., 0., 0., 0.],        [1., 0., 0.,  ..., 0., 0., 0.],        [0., 0., 0.,  ..., 1., 0., 0.]])torch.Size([1000, 10])</code></pre><h3 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h3><p><a href="https://blog.csdn.net/liujh845633242/article/details/102668515">Conv1d()和Conv2d()区别</a></p><p><a href="https://blog.csdn.net/yingluo54/article/details/122168364">conv1d详细用法</a></p><p><a href="https://javajgs.com/archives/153549#:~:text=%E8%BE%93%E5%87%BA%E7%89%B9%E5%BE%81%E5%9B%BE%E5%B0%BA%E5%AF%B8%20%E6%9C%80%E5%90%8E%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E5%8D%B7%E7%A7%AF%E8%BE%93%E5%87%BA%E5%A4%A7%E5%B0%8F%3A%20N%20%3D%20%28W%20%E2%88%92%20F%20%2B,%E8%BE%93%E5%85%A5%E5%9B%BE%E7%89%87%E5%A4%A7%E5%B0%8F%20W%C3%97W%2C%20Filter%E5%A4%A7%E5%B0%8F%20F%C3%97F%2C%20%E6%AD%A5%E9%95%BF%20S%2C%20padding%E7%9A%84%E5%83%8F%E7%B4%A0%E6%95%B0%20P.">如何计算卷积网络的尺寸(公式)</a></p><p><img src="https://pic.rmb.bdstatic.com/bjh/35547f3a90d398bebc8e3aed0508a774.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()  <span class="comment"># 一个样本维度为400 维度：[10,1,1,400]</span></span><br><span class="line">        self.conv1 = nn.Conv1d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">11</span>,</span><br><span class="line">                               stride=<span class="number">4</span>)  <span class="comment"># 出来后此时：[10,64,1,98]</span></span><br><span class="line">        <span class="comment"># 1dpooling 维度算法参考https://blog.csdn.net/yingluo54/article/details/122168364</span></span><br><span class="line">        self.pool1 = nn.MaxPool1d(<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># 出来后此时：[10,64,1,48]</span></span><br><span class="line">        self.conv2 = nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>)  <span class="comment"># 出来后此时：[10,128,1,46]</span></span><br><span class="line">        self.pool2 = nn.MaxPool1d(<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># 出来后此时：[10,128,1,22]</span></span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">22</span>*<span class="number">128</span>, <span class="number">22</span>*<span class="number">64</span>)  <span class="comment"># 从上边出来之后维度变成22</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">22</span>*<span class="number">64</span>, <span class="number">500</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">500</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.pool2(F.relu(self.conv2(x)))</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">22</span>*<span class="number">128</span>)</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Net(  (conv1): Conv1d(1, 64, kernel_size=(11,), stride=(4,))  (pool1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,))  (pool2): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  (fc1): Linear(in_features=2816, out_features=1408, bias=True)  (fc2): Linear(in_features=1408, out_features=500, bias=True)  (fc3): Linear(in_features=500, out_features=10, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = optim.Adam(net.parameters())  <span class="comment"># Adam训练器，用了发现比SGD好使</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><p><a href="https://www.runoob.com/python/python-func-enumerate.html">enumerate()函数</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">loss_list = []</span><br><span class="line">x_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 多批次循环</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span>  <span class="comment"># 每次重置0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取输入</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度置0</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向传播，反向传播，优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = nn.CrossEntropyLoss()</span><br><span class="line">        res = loss(outputs, labels)  <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">        res.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印状态信息</span></span><br><span class="line">        running_loss += res.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:    <span class="comment"># 每2批次(1批次10个样本)打印一次</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.5f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i, running_loss / <span class="number">20</span>))</span><br><span class="line">            loss_list.append(running_loss / <span class="number">20</span>)</span><br><span class="line">            x_list.append(epoch*<span class="number">100</span>+i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>[1,     0] loss: 0.11560[2,     0] loss: 0.10816[3,     0] loss: 0.10406[4,     0] loss: 0.10030[5,     0] loss: 0.09376[6,     0] loss: 0.08903[7,     0] loss: 0.08112[8,     0] loss: 0.07515[9,     0] loss: 0.06976[10,     0] loss: 0.06235[11,     0] loss: 0.06183[12,     0] loss: 0.05548[13,     0] loss: 0.05526[14,     0] loss: 0.04942[15,     0] loss: 0.04974[16,     0] loss: 0.04542[17,     0] loss: 0.04663[18,     0] loss: 0.04146[19,     0] loss: 0.04268[20,     0] loss: 0.04244[21,     0] loss: 0.03886[22,     0] loss: 0.04085[23,     0] loss: 0.03681[24,     0] loss: 0.03655[25,     0] loss: 0.03605[26,     0] loss: 0.03463[27,     0] loss: 0.03309[28,     0] loss: 0.03342[29,     0] loss: 0.03143[30,     0] loss: 0.03075[31,     0] loss: 0.02923[32,     0] loss: 0.02879[33,     0] loss: 0.02753[34,     0] loss: 0.02645[35,     0] loss: 0.02521[36,     0] loss: 0.02453[37,     0] loss: 0.02356[38,     0] loss: 0.02244[39,     0] loss: 0.02177[40,     0] loss: 0.02063[41,     0] loss: 0.01998[42,     0] loss: 0.01928[43,     0] loss: 0.01841[44,     0] loss: 0.01762[45,     0] loss: 0.01702[46,     0] loss: 0.01625[47,     0] loss: 0.01556[48,     0] loss: 0.01509[49,     0] loss: 0.01438[50,     0] loss: 0.01373[51,     0] loss: 0.01312[52,     0] loss: 0.01256[53,     0] loss: 0.01212[54,     0] loss: 0.01175[55,     0] loss: 0.01171[56,     0] loss: 0.01280[57,     0] loss: 0.01157[58,     0] loss: 0.01030[59,     0] loss: 0.00939[60,     0] loss: 0.01008[61,     0] loss: 0.01057[62,     0] loss: 0.00840[63,     0] loss: 0.00928[64,     0] loss: 0.01041[65,     0] loss: 0.00745[66,     0] loss: 0.01025[67,     0] loss: 0.01061[68,     0] loss: 0.00787[69,     0] loss: 0.01133[70,     0] loss: 0.00717[71,     0] loss: 0.00910[72,     0] loss: 0.00646[73,     0] loss: 0.00830[74,     0] loss: 0.00706[75,     0] loss: 0.00806[76,     0] loss: 0.00628[77,     0] loss: 0.00786[78,     0] loss: 0.00644[79,     0] loss: 0.01000[80,     0] loss: 0.00878[81,     0] loss: 0.01111[82,     0] loss: 0.00684[83,     0] loss: 0.01090[84,     0] loss: 0.00600[85,     0] loss: 0.00747[86,     0] loss: 0.00538[87,     0] loss: 0.00774[88,     0] loss: 0.00504[89,     0] loss: 0.00619[90,     0] loss: 0.00443[91,     0] loss: 0.00461[92,     0] loss: 0.00471[93,     0] loss: 0.00392[94,     0] loss: 0.00499[95,     0] loss: 0.00378[96,     0] loss: 0.00412[97,     0] loss: 0.00401[98,     0] loss: 0.00354[99,     0] loss: 0.00373[100,     0] loss: 0.00302Finished Training</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(x_list, loss_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://pic.rmb.bdstatic.com/bjh/d39e8b44779baeff785736f39a410c96.png" alt="png"></p><h3 id="开始测试"><a href="#开始测试" class="headerlink" title="开始测试"></a>开始测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ds_test = bearDataset(</span><br><span class="line">    <span class="string">&#x27;C:/Users/JINTIAN/Desktop/代码/PytorchCode/项目数据集/test.csv&#x27;</span>, <span class="number">400</span>)</span><br><span class="line">dl_test = torch.utils.data.DataLoader(</span><br><span class="line">    ds_test, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data, lable <span class="keyword">in</span> dl_test:</span><br><span class="line">        outputs = net(data)</span><br><span class="line">        <span class="comment"># print(outputs)</span></span><br><span class="line">        <span class="comment"># print(lable[0])</span></span><br><span class="line"></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># print(predicted.item())</span></span><br><span class="line">        p = output2lable[predicted.item()]</span><br><span class="line">        <span class="comment"># print(p)</span></span><br><span class="line"></span><br><span class="line">        total += lable.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(p.equal(lable[<span class="number">0</span>])):</span><br><span class="line">            correct = correct+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;total is &#x27;</span>, total)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>total is  300Accuracy of the network: 93 %</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CNN网络实践&quot;&gt;&lt;a href=&quot;#CNN网络实践&quot; class=&quot;headerlink&quot; title=&quot;CNN网络实践&quot;&gt;&lt;/a&gt;CNN网络实践&lt;/h1&gt;&lt;h3 id=&quot;引入包&quot;&gt;&lt;a href=&quot;#引入包&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-经典CNN模型-VGG网络</title>
    <link href="http://example.com/2022/08/18/%E7%BB%8F%E5%85%B8CNN-VGG/"/>
    <id>http://example.com/2022/08/18/%E7%BB%8F%E5%85%B8CNN-VGG/</id>
    <published>2022-08-18T09:46:30.625Z</published>
    <updated>2022-08-18T09:48:58.701Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VGG网络模型"><a href="#VGG网络模型" class="headerlink" title="VGG网络模型"></a>VGG网络模型</h1><h3 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/f0cab66ca59de0925cf9b44eaefd5c41.png"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"><span class="comment"># 等效之前的Sequential装网络</span></span><br><span class="line"><span class="comment"># 这是一个单独的块</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 分成5块，高宽减半，通道数翻倍</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []  <span class="comment"># 建立一个列表</span></span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span>(num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs=num_convs,</span><br><span class="line">                         in_channels=in_channels, out_channels=out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="comment"># 建立多个块</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        nn.Linear(out_channels*<span class="number">7</span>*<span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch=conv_arch)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>观察一下每一层的形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential output shape:     torch.Size([1, 64, 112, 112])Sequential output shape:     torch.Size([1, 128, 56, 56])Sequential output shape:     torch.Size([1, 256, 28, 28])Sequential output shape:     torch.Size([1, 512, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])Flatten output shape:     torch.Size([1, 25088])Linear output shape:     torch.Size([1, 4096])ReLU output shape:     torch.Size([1, 4096])Dropout output shape:     torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:     torch.Size([1, 4096])Dropout output shape:     torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>]//ratio)<span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://pic.rmb.bdstatic.com/bjh/19b9f7d52be0b70cc19416ea9a9c81e2.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VGG网络模型&quot;&gt;&lt;a href=&quot;#VGG网络模型&quot; class=&quot;headerlink&quot; title=&quot;VGG网络模型&quot;&gt;&lt;/a&gt;VGG网络模型&lt;/h1&gt;&lt;h3 id=&quot;结构图&quot;&gt;&lt;a href=&quot;#结构图&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-经典CNN模型-AlexNet</title>
    <link href="http://example.com/2022/08/16/%E7%BB%8F%E5%85%B8CNN%E6%A8%A1%E5%9E%8B-AlexNet/"/>
    <id>http://example.com/2022/08/16/%E7%BB%8F%E5%85%B8CNN%E6%A8%A1%E5%9E%8B-AlexNet/</id>
    <published>2022-08-16T13:16:59.207Z</published>
    <updated>2022-08-18T03:31:01.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AlexNet模型"><a href="#AlexNet模型" class="headerlink" title="AlexNet模型"></a>AlexNet模型</h1><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/92b7e57362bcd0cced6a833a55f8c4d1.png"><br><img src="https://pic.rmb.bdstatic.com/bjh/840fceff6de46d9d6ef96112f4138057.png"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&quot;Output shape:\t&quot;</span>, X.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Conv2d Output shape:     torch.Size([1, 96, 54, 54])ReLU Output shape:     torch.Size([1, 96, 54, 54])MaxPool2d Output shape:     torch.Size([1, 96, 26, 26])Conv2d Output shape:     torch.Size([1, 256, 26, 26])ReLU Output shape:     torch.Size([1, 256, 26, 26])MaxPool2d Output shape:     torch.Size([1, 256, 12, 12])Conv2d Output shape:     torch.Size([1, 384, 12, 12])ReLU Output shape:     torch.Size([1, 384, 12, 12])Conv2d Output shape:     torch.Size([1, 384, 12, 12])ReLU Output shape:     torch.Size([1, 384, 12, 12])Conv2d Output shape:     torch.Size([1, 256, 12, 12])ReLU Output shape:     torch.Size([1, 256, 12, 12])MaxPool2d Output shape:     torch.Size([1, 256, 5, 5])Flatten Output shape:     torch.Size([1, 6400])Linear Output shape:     torch.Size([1, 4096])ReLU Output shape:     torch.Size([1, 4096])Dropout Output shape:     torch.Size([1, 4096])Linear Output shape:     torch.Size([1, 4096])ReLU Output shape:     torch.Size([1, 4096])Dropout Output shape:     torch.Size([1, 4096])Linear Output shape:     torch.Size([1, 10])</code></pre><p>拿Fashion-MINST的数据集跑一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(</span><br><span class="line">    batch_size=batch_size, resize=<span class="number">224</span>)</span><br><span class="line"><span class="comment"># 因为Fashion-MNIST图像的分辨率，低于ImageNet图像，我们将它们强行拉长到224×224</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.05</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter=train_iter, test_iter=test_iter, num_epochs=num_epochs, lr=lr, device=d2l.try_gpu())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>由于手头上的电脑没有独立显卡，这是沐神的截图</p><p><img src="https://pic.rmb.bdstatic.com/bjh/0ae32699f034861f2eb65ba2a09f796f.png"></p><p><del>(鼠鼠暂时没有能跑cuda的机器，只能拿李沐老师的运行效果图充数了QAQ)</del><br><del>今晚就去给拯救者配置cuda环境(逃)</del></p><h3 id="2022-8-18日更新"><a href="#2022-8-18日更新" class="headerlink" title="2022/8/18日更新"></a>2022/8/18日更新</h3><p><del>我回来辣</del><br>成功在游戏本上安装pytorch运行环境，并且可以使用cuda<br>电脑所支持的cuda版本<br><img src="https://pic.rmb.bdstatic.com/bjh/ecd7fdae19bd353d77f8a0f9a864edba.png"><br>电脑所装的的cuda版本<br><img src="https://pic.rmb.bdstatic.com/bjh/9e78cfe9c9e7dca1565719e323f6f046.png"><br>从pytorch官网装的是pytorch11.6版本(向下兼容)</p><p>我们来看一下运行效果<del>(装了一下午加一晚上人整麻了)</del><br><img src="https://pic.rmb.bdstatic.com/bjh/27af23b4d9d767ca4e396cbf65f1ea40.png"><br>运行时间<br><img src="https://pic.rmb.bdstatic.com/bjh/8c1882ec56c6ef9f3bc63c749617896f.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AlexNet模型&quot;&gt;&lt;a href=&quot;#AlexNet模型&quot; class=&quot;headerlink&quot; title=&quot;AlexNet模型&quot;&gt;&lt;/a&gt;AlexNet模型&lt;/h1&gt;&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-经典CNN模型-LeNet</title>
    <link href="http://example.com/2022/08/16/%E7%BB%8F%E5%85%B8CNN%E6%A8%A1%E5%9E%8B-LeNet/"/>
    <id>http://example.com/2022/08/16/%E7%BB%8F%E5%85%B8CNN%E6%A8%A1%E5%9E%8B-LeNet/</id>
    <published>2022-08-16T07:51:43.351Z</published>
    <updated>2022-08-16T08:49:23.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LeNet模型"><a href="#LeNet模型" class="headerlink" title="LeNet模型"></a>LeNet模型</h1><h3 id="原理框架图"><a href="#原理框架图" class="headerlink" title="原理框架图"></a>原理框架图</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/1d11764b466cd12abf3937679b390416.png"><br><img src="https://pic.rmb.bdstatic.com/bjh/21ba07bf44df0966f0d9d2e590f153f6.png"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>首先定义我们的LeNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(),  <span class="comment"># 对应 阶段1</span></span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),  <span class="comment"># 对应 阶段2</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># 对应 阶段3</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),  <span class="comment"># 对应 阶段4</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),  <span class="comment"># 对应 阶段5</span></span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),  <span class="comment"># 对应 阶段6</span></span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),  <span class="comment"># 对应 阶段7</span></span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)  <span class="comment"># 对应 阶段8</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#x27;zeros&#x27;, device=None, dtype=None)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>检验模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Reshape output shape:     torch.Size([1, 1, 28, 28])Conv2d output shape:     torch.Size([1, 6, 28, 28])Sigmoid output shape:     torch.Size([1, 6, 28, 28])AvgPool2d output shape:     torch.Size([1, 6, 14, 14])Conv2d output shape:     torch.Size([1, 16, 10, 10])Sigmoid output shape:     torch.Size([1, 16, 10, 10])AvgPool2d output shape:     torch.Size([1, 16, 5, 5])Flatten output shape:     torch.Size([1, 400])Linear output shape:     torch.Size([1, 120])Sigmoid output shape:     torch.Size([1, 120])Linear output shape:     torch.Size([1, 84])Sigmoid output shape:     torch.Size([1, 84])Linear output shape:     torch.Size([1, 10])</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;LeNet模型&quot;&gt;&lt;a href=&quot;#LeNet模型&quot; class=&quot;headerlink&quot; title=&quot;LeNet模型&quot;&gt;&lt;/a&gt;LeNet模型&lt;/h1&gt;&lt;h3 id=&quot;原理框架图&quot;&gt;&lt;a href=&quot;#原理框架图&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-卷积层</title>
    <link href="http://example.com/2022/08/15/%E5%8D%B7%E7%A7%AF%E5%B1%82/"/>
    <id>http://example.com/2022/08/15/%E5%8D%B7%E7%A7%AF%E5%B1%82/</id>
    <published>2022-08-15T12:41:12.777Z</published>
    <updated>2022-08-15T12:42:27.321Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><h3 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现矩阵与核矩阵的运算,K是核矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 计算二维互相关运算</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>, X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i+h, j:j+w]*K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>试验一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[19., 25.],        [37., 43.]])</code></pre><p>实现二维卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight)+self.bias</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>应用：检测图像中不同颜色的边缘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br><span class="line"><span class="comment"># 我们的输入</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.],        [1., 1., 0., 0., 0., 0., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　核矩阵</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure><p>我们的目的是通过卷积将边界部分识别出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</code></pre><p>可以看到交界处已经被识别出来了</p><p>但是我们这个核矩阵只能检验列向的分界线，如果想检测行向的分界线，必须要换一个核矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给X取转置</span></span><br><span class="line">corr2d(X.t(), K)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.],        [0., 0., 0., 0., 0.]])</code></pre><p>可见无法识别</p><p>现在给定X，Y，我们要通过X和Y来学习K(核矩阵)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 黑白图片通道为1，RGB图片通道为3</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#x27;zeros&#x27;, device=None, dtype=None)</span></span><br><span class="line"></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))  <span class="comment"># 批量数，通道数，行，列</span></span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat-Y)**<span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    conv2d.weight.data[:] -= <span class="number">3e-2</span>*conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span>(i+<span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>,loss<span class="subst">&#123;l.<span class="built_in">sum</span>():3f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>batch2,loss4.527621batch4,loss1.422135batch6,loss0.509966batch8,loss0.196711batch10,loss0.078531</code></pre><p>看一下我们学的卷积核的张量权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.9644, -1.0217]])</code></pre><p>可以看到和[[1,-1]]很接近了</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/745d8a7a81c602329de12d479377019b.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>)+X.shape)  <span class="comment"># (1,1)是批量大小和通道数</span></span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># 卷积核为3，填充数为1</span></span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="comment"># 输出的矩阵大小和原来的一样</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([8, 8])</code></pre><p>并不是那么对称的情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([8, 8])</code></pre><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p><img src="https://pic.rmb.bdstatic.com/bjh/9c7ad155b6402709461567ab0dcfb64b.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>) <span class="comment"># stride为步幅</span></span><br><span class="line">comp_conv2d(conv2d,X).shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 4])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 稍微复杂一点的例子</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([6, 4])</code></pre><h3 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h3><p>彩色图片RGB格式是三个通道</p><p>灰度图片是单通道</p><p>下面开始从零开始实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line"><span class="comment"># zip会对输入通道的维度做遍历，实现对所有通道的求和</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">                  [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line"><span class="comment"># X 是一个三维的矩阵</span></span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line"><span class="comment"># K 是也是一个三维的矩阵</span></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 56.,  72.],        [104., 120.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算多个通道输出的互相关函数</span></span><br><span class="line"><span class="keyword">from</span> inspect <span class="keyword">import</span> stack</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>torch.Size([3, 2, 2, 2])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr2d_multi_in_out(X, K)</span><br></pre></td></tr></table></figure><pre><code>tensor([[[ 56.,  72.],         [104., 120.]],        [[ 76., 100.],         [148., 172.]],        [[ 96., 128.],         [192., 224.]]])</code></pre><p>1×1的卷积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape((c_i, h*w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(c_o, h, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1-Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 没有多通道的池化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>]-p_h+<span class="number">1</span>, X.shape[<span class="number">1</span>]-p_w+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i+p_h, j:j+p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i+p_h, j:j+p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>验证值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>),mode=<span class="string">&#x27;max&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[4., 5.],        [7., 8.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), mode=<span class="string">&#x27;avg&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[2., 3.],        [5., 6.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 1., 2.],        [3., 4., 5.],        [6., 7., 8.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[[0., 1., 2.],          [3., 4., 5.],          [6., 7., 8.]]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]]]])</code></pre><p>使用框架</p><p>深度学习框架的步幅和池化窗口的大小相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入矩阵必须是3维4维的</span></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)  <span class="comment"># 创建一个1d的3×3的一个窗口</span></span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[[10.]]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以设置一个任意大小的池化窗口</span></span><br><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[[ 1.,  3.],          [ 9., 11.],          [13., 15.]]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　池化在每个通道上的单独运算</span></span><br><span class="line">X = torch.cat((X, X+<span class="number">1</span>), <span class="number">1</span>)  <span class="comment"># 复制</span></span><br><span class="line">X</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[ 1.,  2.,  3.,  4.],          [ 5.,  6.,  7.,  8.],          [ 9., 10., 11., 12.],          [13., 14., 15., 16.]]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d == nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([[[[ 1.,  3.],          [ 9., 11.],          [13., 15.]],         [[ 2.,  4.],          [10., 12.],          [14., 16.]]]])</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;卷积层&quot;&gt;&lt;a href=&quot;#卷积层&quot; class=&quot;headerlink&quot; title=&quot;卷积层&quot;&gt;&lt;/a&gt;卷积层&lt;/h1&gt;&lt;h3 id=&quot;图像卷积&quot;&gt;&lt;a href=&quot;#图像卷积&quot; class=&quot;headerlink&quot; title=&quot;图像卷积&quot;&gt;&lt;/a&gt;图像卷</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-神经网络基础</title>
    <link href="http://example.com/2022/08/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2022/08/14/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</id>
    <published>2022-08-14T03:08:27.684Z</published>
    <updated>2022-08-14T03:10:40.537Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h3 id="层的定义"><a href="#层的定义" class="headerlink" title="层的定义"></a>层的定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>,<span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.0226,  0.1110,  0.1338,  0.0594,  0.0579,  0.0372,  0.2026, -0.2140,          0.0259,  0.0358],        [-0.1102,  0.0507,  0.0410,  0.1030,  0.1872,  0.0963,  0.1452, -0.1649,         -0.0152,  0.1379]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><p>nn.Sequential定义了一个特殊的Moudule，用法看下面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> forward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 调用父类的初始化</span></span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用法和上面是一样的</span></span><br><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.0278,  0.0588,  0.1944, -0.1413,  0.0069, -0.0176,  0.0903,  0.1522,          0.1853, -0.0288],        [-0.0594, -0.0154,  0.1833, -0.1290,  0.0523, -0.0802,  0.1188, -0.0268,          0.1307, -0.0880]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><p>下面这个类将实现和Sequential几乎一样的功能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):  <span class="comment"># *args传进来一个列表</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            self._modules[block] = block  <span class="comment"># 按顺序放入一个特殊的容器，里面存层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.2277, -0.0270, -0.0845,  0.1933,  0.1448, -0.0288,  0.1228,  0.2044,          0.1765, -0.0015],        [ 0.2065,  0.0460, -0.1169,  0.1954,  0.0135, -0.1282,  0.1200,  0.3168,          0.1600, -0.0043]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.Linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.Linear(X)</span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight)+<span class="number">1</span>)  <span class="comment"># y=Xw+b</span></span><br><span class="line">        <span class="comment"># mm就是矩阵相乘</span></span><br><span class="line">        X = self.Linear(X)</span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = FixedHiddenMLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor(0.1070, grad_fn=&lt;SumBackward0&gt;)</code></pre><p>可以灵活的嵌套使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更加复杂嵌套的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure><pre><code>tensor(-0.2101, grad_fn=&lt;SumBackward0&gt;)</code></pre><h3 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个线性层</span></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.1461],        [-0.0553]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="comment"># Sequential 相当于一个容器(列表)，里面的层可以通过下标来访问</span></span><br><span class="line"><span class="comment"># net[2] 就是第三个输出层</span></span><br><span class="line"><span class="comment"># state_dict()可以访问它的状态，即它的权重和偏差 (w,b)</span></span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&#39;weight&#39;, tensor([[-0.0300,  0.1777, -0.0958, -0.2357, -0.3395,  0.2508,  0.2884, -0.1535]])), (&#39;bias&#39;, tensor([-0.2819]))])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)  <span class="comment"># 访问bias的信息(包括数据)</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)  <span class="comment"># 访问bias的数据(只有数据)</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;Parameter containing:tensor([-0.2819], requires_grad=True)tensor([-0.2819])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.grad)  <span class="comment"># 还未做梯度计算，所以现在是none</span></span><br></pre></td></tr></table></figure><pre><code>None</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unicodedata <span class="keyword">import</span> name</span><br><span class="line"><span class="comment"># 打印所有参数信息</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(param, param.shape)<span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>(&#39;weight&#39;, torch.Size([8, 4])) (&#39;bias&#39;, torch.Size([8]))(Parameter containing:tensor([[-0.4920, -0.1636,  0.0745,  0.2927],        [-0.2327,  0.0116,  0.1465, -0.1842],        [ 0.2754, -0.2012, -0.2815,  0.3537],        [-0.4651,  0.2001, -0.2492, -0.2959],        [ 0.2676, -0.1646, -0.3039,  0.3171],        [-0.1426, -0.2941, -0.4617, -0.2725],        [ 0.1695,  0.2115,  0.4533, -0.0576],        [-0.4780, -0.0624, -0.0300, -0.1215]], requires_grad=True), torch.Size([8, 4])) (Parameter containing:tensor([-0.0066, -0.1765,  0.1123, -0.0507, -0.1850, -0.3662,  0.1150,  0.3825],       requires_grad=True), torch.Size([8])) (Parameter containing:tensor([[-0.0300,  0.1777, -0.0958, -0.2357, -0.3395,  0.2508,  0.2884, -0.1535]],       requires_grad=True), torch.Size([1, 8])) (Parameter containing:tensor([-0.2819], requires_grad=True), torch.Size([1]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 插入四个block1</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block<span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())  <span class="comment"># 这样写可以传进去一个字符串名字进去</span></span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[0.1414],        [0.1414]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看一下嵌套的网络(用block2构建的，实质用block1嵌套的网络)</span></span><br><span class="line"><span class="built_in">print</span>(rgnet)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential(  (0): Sequential(    (block0): Sequential(      (0): Linear(in_features=4, out_features=8, bias=True)      (1): ReLU()      (2): Linear(in_features=8, out_features=4, bias=True)      (3): ReLU()    )    (block1): Sequential(      (0): Linear(in_features=4, out_features=8, bias=True)      (1): ReLU()      (2): Linear(in_features=8, out_features=4, bias=True)      (3): ReLU()    )    (block2): Sequential(      (0): Linear(in_features=4, out_features=8, bias=True)      (1): ReLU()      (2): Linear(in_features=8, out_features=4, bias=True)      (3): ReLU()    )    (block3): Sequential(      (0): Linear(in_features=4, out_features=8, bias=True)      (1): ReLU()      (2): Linear(in_features=8, out_features=4, bias=True)      (3): ReLU()    )  )  (1): Linear(in_features=4, out_features=1, bias=True))</code></pre><p>随机初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化层参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):  <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)  <span class="comment"># 正态分布初始化</span></span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_normal)  <span class="comment"># apply的作用就是将这个函数层层调用，可以理解里面有一个forloop在层层贯彻</span></span><br><span class="line">net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias.data</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>(tensor([[ 3.6479e-03, -1.1540e-02, -7.7268e-04, -5.1345e-03],         [ 7.4597e-03,  1.1359e-02, -6.7565e-03,  8.4972e-03],         [-2.4252e-03,  3.5522e-03, -4.1856e-03,  7.3790e-03],         [-8.8164e-03, -2.4064e-03,  2.4951e-02, -1.1745e-02],         [ 1.0388e-02, -1.9615e-03,  9.7956e-05, -9.9438e-03],         [-1.6299e-03, -5.7079e-03,  2.8373e-04,  9.8193e-03],         [-4.1368e-03,  6.9908e-03, -3.5671e-02,  3.6108e-03],         [ 3.5765e-03, -1.2091e-02, -2.0029e-04, -4.3693e-03]]), tensor([0., 0., 0., 0., 0., 0., 0., 0.]))</code></pre><p>介绍一个比较特殊的初始化，常数初始化，但是神经网络里面不能用常数初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)  <span class="comment"># 将矩阵赋值成同一个数</span></span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data, net[<span class="number">0</span>].bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># api提供了常数初始化的方式，但是训练的时候不能这么初始化！</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.]]), Parameter containing: tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))</code></pre><p>apply不仅可以应用一个整的网络，也可以用来对各个层进行操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义两种初始化方式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xvaier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)  <span class="comment"># 42：宇宙的答案————沐神</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(xvaier)  <span class="comment"># 单独对第一层的应用</span></span><br><span class="line">net[<span class="number">2</span>].apply(init_42)  <span class="comment"># 单独对第三层的应用</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[-0.5463, -0.7031,  0.2726, -0.3747],        [ 0.0067, -0.2665, -0.0956,  0.1011],        [ 0.1168,  0.6005,  0.5389,  0.3871],        [ 0.0057,  0.3475,  0.2790,  0.4906],        [-0.3758, -0.0016, -0.4157,  0.3072],        [ 0.4774, -0.4041,  0.1078,  0.4043],        [ 0.3532,  0.3053,  0.2614, -0.5882],        [ 0.4585,  0.2418,  0.3941,  0.1144]])tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])</code></pre><p><img src="https://img-blog.csdnimg.cn/c87204e59b17424a8ee0b5fa07a1d733.png" alt="在这里插入图片描述"><br><a href="https://pytorch.org/docs/stable/nn.init.html?highlight=xavier_uniform_#torch.nn.init.xavier_uniform_">xavier_uniform_()</a>的用法，说实话看不太懂。。。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更加奇怪的初始化的方式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Init&quot;</span>, *[(name, param.shape)</span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line">    <span class="comment"># *是点乘。这个意思是保留所有对应绝对值大于5的权重，其他全部赋值为0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Init weight torch.Size([8, 4])Init weight torch.Size([1, 8])Parameter containing:tensor([[ 8.8280, -5.6789, -6.5591,  0.0000],        [ 6.1000, -7.2595,  7.4734,  0.0000],        [ 0.0000,  7.6965,  0.0000,  0.0000],        [-7.0142, -0.0000, -0.0000,  7.0553],        [ 9.3206,  0.0000,  7.1112,  0.0000],        [-8.2768, -0.0000,  8.3142,  9.4527],        [-0.0000, -6.7602,  0.0000,  7.3447],        [-9.7821, -6.0867,  9.6227,  0.0000]], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 或者更暴力的方式</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([42.0000, -4.6789, -5.5591,  1.0000])</code></pre><p>参数绑定，可以将几个层的参数进行绑定，做到一对多同时修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), shared,</span><br><span class="line">                    nn.ReLU(), shared, nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 这样的话，net里面的两个shared的全连接层的参数是一样的，任意修改一个，另外一个也会修改</span></span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 现在修改其中一个shared的值，康康另外一个有没有被影响</span></span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 通过输出我们能看到修改了其中一个，另外一个也被修改了</span></span><br><span class="line"><span class="comment"># 两个层的参数实际上是被绑定了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([True, True, True, True, True, True, True, True])tensor([True, True, True, True, True, True, True, True])</code></pre><h3 id="自定义一个层"><a href="#自定义一个层" class="headerlink" title="自定义一个层"></a>自定义一个层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X-X.mean()</span><br><span class="line"><span class="comment"># 一个很简单的层</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure><pre><code>tensor([-2., -1.,  0.,  1.,  2.])</code></pre><p>将我们的层合并作为组件加到更复杂的模型当中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line"></span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1642e-09, grad_fn=&lt;MeanBackward0&gt;)</code></pre><p>想让自己的层自带参数，需要调用parameter类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="comment"># in_units是输入维度，units是输出维度</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Linear = torch.matmul(X, self.weight.data)+self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(Linear)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dense = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">dense.weight</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Parameter containing:tensor([[-0.3888,  0.7929,  0.0935],        [ 1.3672, -2.3833,  2.3415],        [-0.5297,  0.5334,  0.0868],        [ 1.3455, -0.5648,  0.5850],        [-0.0486,  1.0883, -0.6080]], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dense(torch.rand(<span class="number">2</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([[1.1443, 0.0000, 1.3325],        [0.0000, 0.0000, 0.2987]])</code></pre><p>用自己定义的层来构建网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([[5.9957],        [0.0000]])</code></pre><h3 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)  <span class="comment"># 长为四的向量</span></span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([0, 1, 2, 3])</code></pre><p>存一个张量列表，将他们读回内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;x-files&#x27;</span>)  <span class="comment"># 存了一个列表</span></span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</code></pre><p>存了一个字典，以字符串作为索引，和字典内部的东西一一映射</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;x&#39;: tensor([0, 1, 2, 3]), &#39;y&#39;: tensor([0., 0., 0., 0.])&#125;</code></pre><p>加载和保存参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> clone</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"><span class="comment"># 构建了一个神经网络的层</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br><span class="line"></span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br><span class="line"><span class="comment"># mlp的参数全部存到一个字典里</span></span><br></pre></td></tr></table></figure><p>实例化了一个MLP的备份，直接读取其中的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 现在把它加载回来</span></span><br><span class="line"><span class="comment"># clone是一个关键字</span></span><br><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&quot;mlp.params&quot;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><pre><code>MLP(  (hidden): Linear(in_features=20, out_features=256, bias=True)  (output): Linear(in_features=256, out_features=10, bias=True))</code></pre><p>我们来验证一下新克隆的层和之前的层是否一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_clone=clone(X)</span><br><span class="line">Y_clone==Y</span><br></pre></td></tr></table></figure><pre><code>tensor([[True, True, True, True, True, True, True, True, True, True],        [True, True, True, True, True, True, True, True, True, True]])</code></pre><p>可以看出来是一摸一样的，我们等于用克隆出来的层去加载了原来的参数，从而使参数相同，输出相同</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;神经网络基础&quot;&gt;&lt;a href=&quot;#神经网络基础&quot; class=&quot;headerlink&quot; title=&quot;神经网络基础&quot;&gt;&lt;/a&gt;神经网络基础&lt;/h1&gt;&lt;h3 id=&quot;层的定义&quot;&gt;&lt;a href=&quot;#层的定义&quot; class=&quot;headerlink&quot; title=&quot;层</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>暑假集训week4</title>
    <link href="http://example.com/2022/08/12/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%AD-week4/"/>
    <id>http://example.com/2022/08/12/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%AD-week4/</id>
    <published>2022-08-12T09:35:46.594Z</published>
    <updated>2022-08-12T10:08:59.888Z</updated>
    
    <content type="html"><![CDATA[<h1 id="暑假集训-week4题解-暑假结构进阶"><a href="#暑假集训-week4题解-暑假结构进阶" class="headerlink" title="暑假集训-week4题解-暑假结构进阶"></a>暑假集训-week4题解-暑假结构进阶</h1><h3 id="A-ST-表"><a href="#A-ST-表" class="headerlink" title="A - ST 表"></a>A - ST 表</h3><p><img src="https://img-blog.csdnimg.cn/55afdc8740db459da5cdbc688563f3ff.png" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/a4216fbcab104331a87d54ea78bac850.png" alt="在这里插入图片描述"><br>ST</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3865</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">1e6</span> + <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> a[maxn], f[maxn][<span class="number">100</span>]; </span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">int</span> <span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">0</span>, f = <span class="number">1</span>;</span><br><span class="line">    <span class="type">char</span> ch = <span class="built_in">getchar</span>();</span><br><span class="line">    <span class="keyword">while</span> (ch &lt; <span class="string">&#x27;0&#x27;</span> || ch &gt; <span class="string">&#x27;9&#x27;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (ch == <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            f = <span class="number">-1</span>;</span><br><span class="line">        ch = <span class="built_in">getchar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (ch &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; ch &lt;= <span class="string">&#x27;9&#x27;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        x = x * <span class="number">10</span> + ch - <span class="number">48</span>;</span><br><span class="line">        ch = <span class="built_in">getchar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> x * f;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">ST_prework</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        f[i][<span class="number">0</span>] = a[i];</span><br><span class="line">    <span class="type">int</span> t = <span class="built_in">log</span>(n) / <span class="built_in">log</span>(<span class="number">2</span>) + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; t; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n - (<span class="number">1</span> &lt;&lt; j) + <span class="number">1</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            f[i][j] = <span class="built_in">max</span>(f[i][j - <span class="number">1</span>], f[i + (<span class="number">1</span> &lt;&lt; (j - <span class="number">1</span>))][j - <span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ST_query</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> k = <span class="built_in">log</span>(r - l + <span class="number">1</span>) / <span class="built_in">log</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(f[l][k], f[r - (<span class="number">1</span> &lt;&lt; k) + <span class="number">1</span>][k]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    n = <span class="built_in">read</span>(), m = <span class="built_in">read</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        a[i] = <span class="built_in">read</span>();</span><br><span class="line">    <span class="built_in">ST_prework</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> l, r;</span><br><span class="line">        l = <span class="built_in">read</span>(), r = <span class="built_in">read</span>();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%ld\n&quot;</span>,<span class="built_in">ST_query</span>(l,r));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="B-并查集"><a href="#B-并查集" class="headerlink" title="B - 并查集"></a>B - 并查集</h3><p><img src="https://img-blog.csdnimg.cn/e2e108f037ec441f974b906fe6243b06.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3367</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2e5</span>;</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> fa[maxn];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == fa[x])</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> fa[x] = <span class="built_in">find</span>(fa[x]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">connect</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fa[<span class="built_in">find</span>(x)] = fa[<span class="built_in">find</span>(y)];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        fa[i] = i;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> flag, x, y;</span><br><span class="line">        cin &gt;&gt; flag &gt;&gt; x &gt;&gt; y;</span><br><span class="line">        <span class="keyword">if</span> (flag == <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">connect</span>(x, y);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (flag == <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">find</span>(x) == <span class="built_in">find</span>(y))</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;Y&quot;</span> &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;N&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="C-字符串哈希"><a href="#C-字符串哈希" class="headerlink" title="C - 字符串哈希"></a>C - 字符串哈希</h3><p><img src="https://img-blog.csdnimg.cn/94bc954d32f34cd5ae38b86251f3a828.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3370</span></span><br><span class="line">map&lt;string, <span class="type">bool</span>&gt; visit;</span><br><span class="line"><span class="type">int</span> num;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        string remp;</span><br><span class="line">        cin &gt;&gt; remp;</span><br><span class="line">        <span class="keyword">if</span> (!visit[remp])</span><br><span class="line">            visit[remp] = <span class="literal">true</span>, num++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; num;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="D-统计难题"><a href="#D-统计难题" class="headerlink" title="D - 统计难题"></a>D - 统计难题</h3><p><img src="https://img-blog.csdnimg.cn/78199c0cde634be386c20ea2059f8c06.png" alt="在这里插入图片描述"></p><p>trie模板题，trie挺有用的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 传送门：https://vjudge.csgrandeur.cn/contest/509181#problem/D</span></span><br><span class="line"><span class="comment">// Trie字典树</span></span><br><span class="line"><span class="type">int</span> trie[<span class="number">1000100</span>][<span class="number">26</span>], cnt[<span class="number">1000100</span>], tot;</span><br><span class="line">string remp;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(string tar)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> len = tar.<span class="built_in">length</span>(), node = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; len; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> ch = tar[<span class="number">0</span>] - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        <span class="keyword">if</span> (trie[node][ch] == <span class="number">0</span>)</span><br><span class="line">            trie[node][ch] = ++tot;</span><br><span class="line">        node = trie[node][ch];</span><br><span class="line">    &#125;</span><br><span class="line">    cnt[node]++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">query</span><span class="params">(string tar)</span> <span class="comment">//查询以此字串开头的串有多少个</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> node = <span class="number">0</span>, ans = <span class="number">0</span>, len = tar.<span class="built_in">length</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> ch = tar[i] - <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">        node = trie[node][ch];</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        ans += cnt[node];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">while</span> (cin &gt;&gt; remp)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">insert</span>(remp);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (cin &gt;&gt; remp)</span><br><span class="line">    &#123;</span><br><span class="line">        cout &lt;&lt; <span class="built_in">query</span>(remp) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="E-树状数组-1"><a href="#E-树状数组-1" class="headerlink" title="E - 树状数组 1"></a>E - 树状数组 1</h3><p><img src="https://img-blog.csdnimg.cn/6bef81b252b44bfd9102701affdd8134.png" alt="在这里插入图片描述"><br>树状数组模板题<br>单点修改区间询问<br>假如要单点询问的话用树状数组再维护一个差分数组就行了</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3374</span></span><br><span class="line"><span class="comment">// 树状数组</span></span><br><span class="line"><span class="comment">// 单点修改，区间询问</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">1e6</span>;</span><br><span class="line"><span class="type">int</span> tree_array[maxn];</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (; x &lt;= n; x += x &amp; -x)</span><br><span class="line">        tree_array[x] += y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ask</span><span class="params">(<span class="type">int</span> r)</span> <span class="comment">//求1到r之间的前缀和</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (; r; r -= r &amp; -r)</span><br><span class="line">    &#123;</span><br><span class="line">        ans += tree_array[r];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">query</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">ask</span>(r) - <span class="built_in">ask</span>(l - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> remp;</span><br><span class="line">        cin &gt;&gt; remp;</span><br><span class="line">        <span class="built_in">add</span>(i, remp);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> remp, x, y;</span><br><span class="line">        cin &gt;&gt; remp &gt;&gt; x &gt;&gt; y;</span><br><span class="line">        <span class="keyword">if</span> (remp == <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">add</span>(x, y);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            cout &lt;&lt; <span class="built_in">query</span>(x, y) &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="F-线段树-1"><a href="#F-线段树-1" class="headerlink" title="F - 线段树 1"></a>F - 线段树 1</h3><p><img src="https://img-blog.csdnimg.cn/e794fac9c5e742659caaf450e30d8982.png" alt="在这里插入图片描述"><br>模板题</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 线段树</span></span><br><span class="line"><span class="comment">// 区间询问，区间修改(RMQ)</span></span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3372</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> maxn 100010</span></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> a[maxn];</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> tree[<span class="number">4</span> * maxn];</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> lazy[<span class="number">4</span> * maxn];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">pushdown</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> left, <span class="type">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (lazy[x] == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="type">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    lazy[x &lt;&lt; <span class="number">1</span>] += lazy[x], tree[x &lt;&lt; <span class="number">1</span>] += lazy[x] * (mid - left + <span class="number">1</span>);</span><br><span class="line">    lazy[x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>] += lazy[x], tree[x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>] += lazy[x] * (right - mid);</span><br><span class="line">    lazy[x] = <span class="number">0</span>; <span class="comment">//该节点已结算</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">build</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> left, <span class="type">int</span> right)</span> <span class="comment">// x为堆数组编号，left与right代表其维护的区间</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left == right)</span><br><span class="line">    &#123;</span><br><span class="line">        tree[x] = a[left];</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">build</span>(x &lt;&lt; <span class="number">1</span>, left, mid);</span><br><span class="line">    <span class="built_in">build</span>(x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>, mid + <span class="number">1</span>, right);</span><br><span class="line">    tree[x] = tree[x &lt;&lt; <span class="number">1</span>] + tree[x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">query</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> left, <span class="type">int</span> right, <span class="type">int</span> tar_left, <span class="type">int</span> tar_right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left == tar_left &amp;&amp; right == tar_right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> tree[x];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">pushdown</span>(x, left, right);</span><br><span class="line">    <span class="type">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &lt;= mid &amp;&amp; tar_right &gt; mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> leftans = <span class="built_in">query</span>(x &lt;&lt; <span class="number">1</span>, left, mid, tar_left, mid);</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> rightans = <span class="built_in">query</span>(x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>, mid + <span class="number">1</span>, right, mid + <span class="number">1</span>, tar_right);</span><br><span class="line">        <span class="keyword">return</span> leftans + rightans;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &lt;= mid &amp;&amp; tar_right &lt;= mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">query</span>(x &lt;&lt; <span class="number">1</span>, left, mid, tar_left, tar_right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &gt; mid &amp;&amp; tar_right &gt; mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">query</span>(x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>, mid + <span class="number">1</span>, right, tar_left, tar_right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">modify</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> left, <span class="type">int</span> right, <span class="type">int</span> tar_left, <span class="type">int</span> tar_right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left == tar_left &amp;&amp; right == tar_right)</span><br><span class="line">    &#123;</span><br><span class="line">        tree[x] += (right - left + <span class="number">1</span>) * y;</span><br><span class="line">        lazy[x] += y; <span class="comment">//懒数组标记！</span></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">pushdown</span>(x, left, right);</span><br><span class="line">    <span class="type">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &lt;= mid &amp;&amp; tar_right &gt; mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">modify</span>(x &lt;&lt; <span class="number">1</span>, y, left, mid, tar_left, mid);</span><br><span class="line">        <span class="built_in">modify</span>(x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>, y, mid + <span class="number">1</span>, right, mid + <span class="number">1</span>, tar_right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &lt;= mid &amp;&amp; tar_right &lt;= mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">modify</span>(x &lt;&lt; <span class="number">1</span>, y, left, mid, tar_left, tar_right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (tar_left &gt; mid &amp;&amp; tar_right &gt; mid)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">modify</span>(x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>, y, mid + <span class="number">1</span>, right, tar_left, tar_right);</span><br><span class="line">    &#125;</span><br><span class="line">    tree[x] = tree[x &lt;&lt; <span class="number">1</span>] + tree[x &lt;&lt; <span class="number">1</span> | <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">    <span class="built_in">build</span>(<span class="number">1</span>, <span class="number">1</span>, n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> remp;</span><br><span class="line">        cin &gt;&gt; remp;</span><br><span class="line">        <span class="keyword">if</span> (remp == <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x, y, k;</span><br><span class="line">            cin &gt;&gt; x &gt;&gt; y &gt;&gt; k;</span><br><span class="line">            <span class="built_in">modify</span>(<span class="number">1</span>, k, <span class="number">1</span>, n, x, y);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (remp == <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x, y;</span><br><span class="line">            cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">            cout &lt;&lt; <span class="built_in">query</span>(<span class="number">1</span>, <span class="number">1</span>, n, x, y) &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="G-Palindrome"><a href="#G-Palindrome" class="headerlink" title="G - Palindrome"></a>G - Palindrome</h3><p><img src="https://img-blog.csdnimg.cn/20804883b4384f14bda7bd82d0d35afb.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/dd68e6c15b6b480fa08f5e80c7b003cf.png" alt="在这里插入图片描述"><br>hash加二分搜索答案，思路挺巧妙的<br>注意下反向hash是怎么存的 (知识增加)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 传送门：https://vjudge.csgrandeur.cn/contest/509181#problem/G</span></span><br><span class="line"><span class="comment">// 字符串哈希《蓝书》P67</span></span><br><span class="line"><span class="comment">// 题解：https://www.acwing.com/solution/content/125631/</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2000010</span>, P = <span class="number">131</span>;</span><br><span class="line"><span class="type">char</span> s[maxn];</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> h1[maxn], h2[maxn], p[maxn];</span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">get</span><span class="params">(<span class="type">long</span> <span class="type">long</span> h[], <span class="type">long</span> <span class="type">long</span> l, <span class="type">long</span> <span class="type">long</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> h[r] - h[l - <span class="number">1</span>] * p[r - l + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, s + <span class="number">1</span>) &amp;&amp; <span class="built_in">strcmp</span>(s + <span class="number">1</span>, <span class="string">&quot;END&quot;</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n = <span class="built_in">strlen</span>(s + <span class="number">1</span>) * <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n; i; i -= <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            s[i] = s[i / <span class="number">2</span>];</span><br><span class="line">            s[i - <span class="number">1</span>] = <span class="string">&#x27;z&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        p[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>, j = n; i &lt;= n; i++, j--)</span><br><span class="line">        &#123;</span><br><span class="line">            h1[i] = h1[i - <span class="number">1</span>] * P + s[i] - <span class="string">&#x27;a&#x27;</span> + <span class="number">1</span>;</span><br><span class="line">            h2[i] = h2[i - <span class="number">1</span>] * P + s[j] - <span class="string">&#x27;a&#x27;</span> + <span class="number">1</span>; <span class="comment">//倒着存进去了</span></span><br><span class="line">            p[i] = p[i - <span class="number">1</span>] * P;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">long</span> <span class="type">long</span> x = <span class="number">-1</span>, l = <span class="number">0</span>, r = <span class="built_in">min</span>(i - <span class="number">1</span>, n - i);</span><br><span class="line">            <span class="keyword">while</span> (l &lt;= r)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">long</span> <span class="type">long</span> mid = (l + r + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">get</span>(h1, i - mid, i - <span class="number">1</span>) != <span class="built_in">get</span>(h2, n - (i + mid) + <span class="number">1</span>, n - i))</span><br><span class="line">                &#123;</span><br><span class="line">                    r = mid - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    x = mid;</span><br><span class="line">                    l = mid + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (s[i - x] &lt;= <span class="string">&#x27;z&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                ans = <span class="built_in">max</span>(ans, x + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                ans = <span class="built_in">max</span>(ans, x);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case %d: %d\n&quot;</span>, ++cnt, ans);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="H-The-XOR-Largest-Pair"><a href="#H-The-XOR-Largest-Pair" class="headerlink" title="H - The XOR Largest Pair"></a>H - The XOR Largest Pair</h3><p><img src="https://img-blog.csdnimg.cn/fa3b7173bd7a43feac4b4c6247636104.png" alt="在这里插入图片描述"><br>trie的题<br>思路是将输入的数字想象成一个01的字符串，我们考虑贪心策略，组数字的时候尽可能让这个数字的二进制对应位置都不一样</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// tire树(01 trie树)</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">1e6</span>;</span><br><span class="line"><span class="type">int</span> n, a[maxn], tot;</span><br><span class="line"><span class="type">int</span> trie[<span class="number">1000000</span>][<span class="number">2</span>]; <span class="comment">//每一位要么是0要么是1</span></span><br><span class="line"><span class="comment">// trie[x][y]的值是下一个节点的编号，x是当前编号，y是当前位置的值</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> p = <span class="number">0</span>;                    <span class="comment">// root节点,用于移动的指针</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">30</span>; i &gt;= <span class="number">0</span>; i--) <span class="comment">//二进制从第0位开始</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> remp = x &gt;&gt; i &amp; <span class="number">1</span>; <span class="comment">//取出第i位 (从高位开始传)</span></span><br><span class="line">        <span class="keyword">if</span> (trie[p][remp] == <span class="number">0</span>)</span><br><span class="line">            trie[p][remp] = ++tot; <span class="comment">//指针只想这里</span></span><br><span class="line">        p = trie[p][remp];         <span class="comment">//将当前指针对准这里</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">search</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> p = <span class="number">0</span>;        <span class="comment">//用于移动的指针</span></span><br><span class="line">    <span class="type">int</span> temp_ans = <span class="number">0</span>; <span class="comment">//本次搜索到最大值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">30</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 取贪心策略，我们尽量走相反位置</span></span><br><span class="line">        <span class="type">int</span> remp = x &gt;&gt; i &amp; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (trie[p][remp ^ <span class="number">1</span>]) <span class="comment">//如果相反节点存在，直接跳过去</span></span><br><span class="line">        &#123;</span><br><span class="line">            p = trie[p][remp ^ <span class="number">1</span>];</span><br><span class="line">            temp_ans += (<span class="number">1</span> &lt;&lt; i); <span class="comment">//此位置确定是1，可以上去了！</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            p = trie[p][remp]; <span class="comment">//没有的话就原节点继续</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> temp_ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x;</span><br><span class="line">        cin &gt;&gt; x;</span><br><span class="line">        <span class="built_in">insert</span>(x);</span><br><span class="line">        ans = <span class="built_in">max</span>(ans, <span class="built_in">search</span>(x));</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="I-食物链"><a href="#I-食物链" class="headerlink" title="I - 食物链"></a>I - 食物链</h3><p><img src="https://img-blog.csdnimg.cn/25c4c55334f5406d9cf4351ef2161c3b.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/a136ceff6969486996eaca1d86fe9fe5.png" alt="在这里插入图片描述"><br>并查集的拓展域做法，很巧妙的解法！</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">3e6</span>;</span><br><span class="line"><span class="type">int</span> N, K, fa[<span class="number">200000</span>];</span><br><span class="line"><span class="comment">// 拓展域做法</span></span><br><span class="line"><span class="comment">// fa[x]代表本类，fa[x+N]代表捕食类，fa[x+2n]代表天敌</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (fa[x] != x)</span><br><span class="line">        <span class="keyword">return</span> fa[x] = <span class="built_in">find</span>(fa[x]);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">connect</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fa[<span class="built_in">find</span>(x)] = fa[<span class="built_in">find</span>(y)];</span><br><span class="line">    <span class="comment">// fa[find(x)] = find(y);</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; N &gt;&gt; K;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= N * <span class="number">3</span>; i++)</span><br><span class="line">        fa[i] = i;</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= K; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> opt, x, y;</span><br><span class="line">        cin &gt;&gt; opt &gt;&gt; x &gt;&gt; y;</span><br><span class="line">        <span class="keyword">if</span> (x &gt; N || y &gt; N)</span><br><span class="line">        &#123;</span><br><span class="line">            cnt++;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (opt == <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">find</span>(x + N) == <span class="built_in">find</span>(y) || <span class="built_in">find</span>(y + N) == <span class="built_in">find</span>(x)) <span class="comment">// x吃y，或者y吃x</span></span><br><span class="line">            &#123;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">connect</span>(x, y);</span><br><span class="line">                <span class="built_in">connect</span>(x + N, y + N);</span><br><span class="line">                <span class="built_in">connect</span>(x + N + N, y + N + N);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (opt == <span class="number">2</span>) <span class="comment">// X 吃 Y</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">find</span>(y + N) == <span class="built_in">find</span>(x) || <span class="built_in">find</span>(x) == <span class="built_in">find</span>(y)) <span class="comment">//如果y的捕食域有x，或者同类</span></span><br><span class="line">            &#123;</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">connect</span>(x + N, y);         <span class="comment">// x的捕食域放入y</span></span><br><span class="line">                <span class="built_in">connect</span>(y + N + N, x);     <span class="comment">// y的天敌域放入x</span></span><br><span class="line">                <span class="built_in">connect</span>(y + N, x + N + N); <span class="comment">// y的捕食是x天敌</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     </span><br><span class="line">    cout &lt;&lt; cnt &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;暑假集训-week4题解-暑假结构进阶&quot;&gt;&lt;a href=&quot;#暑假集训-week4题解-暑假结构进阶&quot; class=&quot;headerlink&quot; title=&quot;暑假集训-week4题解-暑假结构进阶&quot;&gt;&lt;/a&gt;暑假集训-week4题解-暑假结构进阶&lt;/h1&gt;&lt;h3 i</summary>
      
    
    
    
    <category term="集训" scheme="http://example.com/categories/%E9%9B%86%E8%AE%AD/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-MLP提高泛化</title>
    <link href="http://example.com/2022/08/12/MLP/"/>
    <id>http://example.com/2022/08/12/MLP/</id>
    <published>2022-08-12T03:51:55.801Z</published>
    <updated>2022-08-12T03:55:18.337Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MLP提高泛化的方法"><a href="#MLP提高泛化的方法" class="headerlink" title="MLP提高泛化的方法"></a>MLP提高泛化的方法</h1><p><img src="https://img-blog.csdnimg.cn/668a4be6d97745cdb1ca8b13c4411686.png" alt="在这里插入图片描述"></p><h3 id="权重衰减-pytorch框架实现"><a href="#权重衰减-pytorch框架实现" class="headerlink" title="权重衰减-pytorch框架实现"></a>权重衰减-pytorch框架实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>造数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line"><span class="comment"># 训练数，测试数，输入数，步长</span></span><br><span class="line">true_w, true_b = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)  <span class="comment"># 生成训练集</span></span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)  <span class="comment"># 生成验证集</span></span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>开整！</p><p>由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):  <span class="comment"># 手动输入L2范数惩罚的参数</span></span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))  <span class="comment"># 网络中的参数</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)  <span class="comment"># 损失函数</span></span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span>  <span class="comment"># 训练100个</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].weight, <span class="string">&#x27;weight_decay&#x27;</span>:wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].bias&#125;], lr=lr)  <span class="comment"># 定义训练器</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])  <span class="comment"># 定义动画生成器</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span>(epoch+<span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当系数为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>w的L2范数： 12.364677429199219</code></pre><p><img src="https://s2.loli.net/2022/08/12/3rMiB6lzENy1JKF.png" alt="svg"></p><p>很显然，上图过拟合了</p><p>我们看看加入L2范数权重衰减会怎么样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>w的L2范数： 0.3969815671443939</code></pre><p><img src="https://s2.loli.net/2022/08/12/ApR2HOxaPL8M3bI.png" alt="svg"></p><h3 id="暂退法-Dropout-pytorch实现"><a href="#暂退法-Dropout-pytorch实现" class="headerlink" title="暂退法(Dropout)-pytorch实现"></a>暂退法(Dropout)-pytorch实现</h3><p>这个比较常用</p><p><img src="https://img-blog.csdnimg.cn/02ca6085fbb948efa5031c453e34a24d.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=<span class="number">4</span>),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=<span class="number">4</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">dropout1 = <span class="number">0.2</span></span><br><span class="line">dropout2 = <span class="number">0.5</span></span><br><span class="line"><span class="comment"># w矩阵赋值为0的比例(概率)</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),  <span class="comment"># 第一个全连接层</span></span><br><span class="line">                    <span class="comment"># 在这个地方设置一个dropout层</span></span><br><span class="line">                    nn.Dropout(dropout1),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),   <span class="comment"># 第二个全连接层</span></span><br><span class="line">                    <span class="comment"># 再设置一个dropout层</span></span><br><span class="line">                    nn.Dropout(dropout2),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)  <span class="comment"># 给矩阵赋初值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential(  (0): Flatten(start_dim=1, end_dim=-1)  (1): Linear(in_features=784, out_features=256, bias=True)  (2): ReLU()  (3): Dropout(p=0.2, inplace=False)  (4): Linear(in_features=256, out_features=256, bias=True)  (5): ReLU()  (6): Dropout(p=0.5, inplace=False)  (7): Linear(in_features=256, out_features=10, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/12/RNnf8OZDvScLE6x.png" alt="svg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MLP提高泛化的方法&quot;&gt;&lt;a href=&quot;#MLP提高泛化的方法&quot; class=&quot;headerlink&quot; title=&quot;MLP提高泛化的方法&quot;&gt;&lt;/a&gt;MLP提高泛化的方法&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-多层感知机(MLP)</title>
    <link href="http://example.com/2022/08/10/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>http://example.com/2022/08/10/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</id>
    <published>2022-08-10T13:59:08.295Z</published>
    <updated>2022-08-11T02:42:24.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多层感知机MLP"><a href="#多层感知机MLP" class="headerlink" title="多层感知机MLP"></a>多层感知机MLP</h1><h3 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h3><p>之前的回归模型已经能解决一些线性模型，但是对于非线性问题，用之前的去模拟是不合适的，我们需要在神经网络中加入隐藏层，形成非线性模型，实现对模型预测的普适性</p><h3 id="从零开始实现MLP"><a href="#从零开始实现MLP" class="headerlink" title="从零开始实现MLP"></a>从零开始实现MLP</h3><p>一些准备工作：引入包，加载数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"><span class="comment"># 重写下载数据集</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment"># 创建训练集和测试集</span></span><br></pre></td></tr></table></figure><p>将每个图像视为具有784个输入特征 和10个类的简单分类数据集。 首先，我们将实现一个具有单隐藏层的多层感知机， 它包含256个隐藏单元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"><span class="comment"># y= X*W+b</span></span><br><span class="line"><span class="comment"># X是 1*784</span></span><br><span class="line"><span class="comment"># 创建第一层 第一层  W:784 * 256  b: 1*256</span></span><br><span class="line">W1 = nn.Parameter(torch.randn(</span><br><span class="line">    num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 创建第二层 第二层  W：256 * 10  b:1*10</span></span><br><span class="line">W2 = nn.Parameter(torch.randn(</span><br><span class="line">    num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>)*<span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>增加非线性，隐藏层使用激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用ReLU函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a=torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X,a)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X@W1+b1)  <span class="comment"># 从输入层到隐藏层</span></span><br><span class="line">    <span class="keyword">return</span> (H@W2+b2)  <span class="comment"># 从隐藏层到输出层</span></span><br><span class="line"><span class="comment"># @代表矩阵乘法</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用封装好的交叉熵函数</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 训练的样本个数是 10</span></span><br><span class="line"><span class="comment"># 学习率是0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/11/4P8X5FRdpu2fQba.png" alt="svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.predict_ch3(net,test_iter,n=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/11/RI8BPbDmnruaENd.png" alt="svg"></p><h3 id="pytorch框架实现MLP"><a href="#pytorch框架实现MLP" class="headerlink" title="pytorch框架实现MLP"></a>pytorch框架实现MLP</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>我们添加了2个全连接层，第一层是隐藏层，它包含256个隐藏单元，并使用了ReLU激活函数。 第二层是输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),  <span class="comment"># 降成一维</span></span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)  <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)  <span class="comment"># 初始化权重</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential(  (0): Flatten(start_dim=1, end_dim=-1)  (1): Linear(in_features=784, out_features=256, bias=True)  (2): ReLU()  (3): Linear(in_features=256, out_features=10, bias=True))</code></pre><p>训练！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)  <span class="comment"># 损失函数</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)  <span class="comment"># 优化器</span></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line"><span class="comment"># 创建训练集和测试集</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/11/7fjN93BQcYPwCWl.png" alt="svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.predict_ch3(net, test_iter)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/11/BC8EWGJDyM6hoOf.png" alt="svg"></p><p>学了MLP感觉思路清晰了很多，之前看封装好的代码框架如同看天书，现在总算找到一点规律了，总算有点进展了<del>热泪盈眶</del></p><p>今晚得以安眠</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;多层感知机MLP&quot;&gt;&lt;a href=&quot;#多层感知机MLP&quot; class=&quot;headerlink&quot; title=&quot;多层感知机MLP&quot;&gt;&lt;/a&gt;多层感知机MLP&lt;/h1&gt;&lt;h3 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-图片集与Softmax回归</title>
    <link href="http://example.com/2022/08/10/Softmax%E5%9B%9E%E5%BD%92/"/>
    <id>http://example.com/2022/08/10/Softmax%E5%9B%9E%E5%BD%92/</id>
    <published>2022-08-10T12:06:57.332Z</published>
    <updated>2022-08-11T02:44:55.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><h3 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h3><p>回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。</p><p>事实上，我们也对分类问题感兴趣：不是问“多少”，而是问“哪一个”！<br><img src="https://img-blog.csdnimg.cn/4fcda33fe0274418b2862f91e997d2c8.png" alt="在这里插入图片描述"></p><h3 id="图像分类数据集"><a href="#图像分类数据集" class="headerlink" title="图像分类数据集"></a>图像分类数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br></pre></td></tr></table></figure><h5 id="下载并读取数据集"><a href="#下载并读取数据集" class="headerlink" title="下载并读取数据集"></a>下载并读取数据集</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test)</span><br></pre></td></tr></table></figure><pre><code>(60000, 10000)</code></pre><h5 id="制定标签"><a href="#制定标签" class="headerlink" title="制定标签"></a>制定标签</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    <span class="comment"># 返回get_fashion_mnist_labels数据集标签</span></span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure><p>可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure><pre><code>array([&lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;ankle boot&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;t-shirt&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;t-shirt&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;dress&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;t-shirt&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;pullover&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sneaker&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;pullover&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sandal&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sandal&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;t-shirt&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;ankle boot&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sandal&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sandal&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;sneaker&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;ankle boot&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;trouser&#39;&#125;&gt;,       &lt;AxesSubplot:title=&#123;&#39;center&#39;:&#39;t-shirt&#39;&#125;&gt;], dtype=object)</code></pre><p><img src="https://s2.loli.net/2022/08/11/Yxf6egOPKHhGECj.png" alt="svg"></p><h5 id="小批量读取"><a href="#小批量读取" class="headerlink" title="小批量读取"></a>小批量读取</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br></pre></td></tr></table></figure><h5 id="读取所需要的时间"><a href="#读取所需要的时间" class="headerlink" title="读取所需要的时间"></a>读取所需要的时间</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure><pre><code>&#39;3.66 sec&#39;</code></pre><h5 id="整合组件"><a href="#整合组件" class="headerlink" title="整合组件"></a>整合组件</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="调整图像大小"><a href="#调整图像大小" class="headerlink" title="调整图像大小"></a>调整图像大小</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64</code></pre><h3 id="softmax——从零开始实现"><a href="#softmax——从零开始实现" class="headerlink" title="softmax——从零开始实现"></a>softmax——从零开始实现</h3><p><a href="https://www.zhihu.com/question/23765351/answer/2144591565">softmax及其回归机制，讲的很好</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size =<span class="number">256</span></span><br><span class="line">train_iter,test_iter=load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="comment"># 原图片是28*28的图片，通道数为1</span></span><br><span class="line"><span class="comment"># 将图片拉长，拉成28*28=784长度的向量</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp/partition   <span class="comment"># softmax的实现</span></span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/BlairGrowing/p/15062915.html#:~:text=softmax%20%E5%9B%9E%E5%BD%92%E5%90%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%80%E6%A0%B7%EF%BC%8C%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%82%20%E7%94%B1%E4%BA%8E%E6%AF%8F%E4%B8%AA%E8%BE%93%E5%87%BA%20o%201%2C%20o%202%2C%20o,1%2C%20x%202%2C%20x%203%2C%20x%204%2C%20softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E8%BE%93%E5%87%BA%E5%B1%82%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E3%80%82">softmax回归理解</a><br><img src="https://img-blog.csdnimg.cn/36a875034aeb48a589fc9edb12c9e9cd.png" alt="在这里插入图片描述"></p><p><a href="https://blog.csdn.net/qq_41565359/article/details/112977221">多元分类器交叉熵公式</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W)+b)</span><br></pre></td></tr></table></figure><p>测试我们的softmax：</p><p>对于任何随机输入，我们将每个元素变成一个非负数。 此外，依据概率原理，每行总和为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">X_prob = softmax(X)</span><br><span class="line">X_prob, X_prob.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>(tensor([[0.2772, 0.0864, 0.1937, 0.0683, 0.3745],         [0.0335, 0.2456, 0.1310, 0.2208, 0.3691]]), tensor([1.0000, 1.0000]))</code></pre><p><img src="https://img-blog.csdnimg.cn/6a82767ab87d45d58f1b96323ef9345e.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]</span><br></pre></td></tr></table></figure><pre><code>tensor([0.1000, 0.5000])</code></pre><p>损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line">cross_entropy(y_hat, y)</span><br></pre></td></tr></table></figure><pre><code>tensor([2.3026, 0.6931])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 还是在检测上面的数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:<span class="comment"># 如果是一个二维矩阵</span></span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>) <span class="comment">#找出横向最大值的那个下标，作为预测结果</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">accuracy(y_hat,y)/<span class="built_in">len</span>(y)</span><br></pre></td></tr></table></figure><pre><code>0.5</code></pre><p>评估任意模型net的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment"># @save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel()) <span class="comment"># 将正确的预测次数进行累加</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>] <span class="comment"># 预测正确/总数</span></span><br></pre></td></tr></table></figure><p>后面的实现不会了。。。<del>前方的区域以后再来探索吧</del></p><h3 id="softmax——pytorch框架实现"><a href="#softmax——pytorch框架实现" class="headerlink" title="softmax——pytorch框架实现"></a>softmax——pytorch框架实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size) <span class="comment"># 批量大小是256</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以前知识的回顾</span></span><br><span class="line">X = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]]) </span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>), X.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 0是竖着求和，1是横着求和</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[5., 7., 9.]]), tensor([[ 6.],         [15.]]))</code></pre><p><img src="https://img-blog.csdnimg.cn/e5aa7d1c285446039ba7189c568a2c70.png" alt="在这里插入图片描述"></p><p><a href="https://blog.csdn.net/lz_peter/article/details/84574716">softmax</a></p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential">Sequential</a></p><p><a href="https://pytorch.org/docs/stable/nn.init.html?highlight=normal_#torch.nn.init.normal_">normal_</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 输入大小是28*28=784，因为我们的数据集有10个类别，所以网络输出维度为10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weight</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按每一层跑一遍</span></span><br><span class="line">net.apply(init_weight)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Sequential(  (0): Flatten(start_dim=1, end_dim=-1)  (1): Linear(in_features=784, out_features=10, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer =torch.optim.SGD(net.parameters(),lr=<span class="number">0.1</span>) <span class="comment"># 学习率为0.1小批量随机梯度下降的优化算法</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs=<span class="number">10</span></span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/08/11/fVqQ9CahdtRpHj8.png" alt="svg"></p><p><strong>参考李沐老师的《动手学深度学习》</strong></p><p><del>softmax好难，学了好久才大概懂一点点，我是菜鸟</del></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Softmax回归&quot;&gt;&lt;a href=&quot;#Softmax回归&quot; class=&quot;headerlink&quot; title=&quot;Softmax回归&quot;&gt;&lt;/a&gt;Softmax回归&lt;/h1&gt;&lt;h3 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-自动求导与线性回归</title>
    <link href="http://example.com/2022/08/08/%E6%B1%82%E5%AF%BC%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://example.com/2022/08/08/%E6%B1%82%E5%AF%BC%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2022-08-08T04:07:18.536Z</published>
    <updated>2022-08-08T07:19:46.729Z</updated>
    
    <content type="html"><![CDATA[<h3 id="自动求导尝试"><a href="#自动求导尝试" class="headerlink" title="自动求导尝试"></a>自动求导尝试</h3><p>torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性.</p><p>​ 要阻止一个张量被跟踪历史，可以调用 .detach() 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。</p><p>​ 为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。</p><p>​ 还有一个类对于autograd的实现非常重要：Function。</p><p>Tensor 和 Function 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个张量都有一个 .grad_fn 属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的 grad_fn 是 None )。</p><p>​<strong>如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个 gradient 参数，该参数是形状匹配的张量。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">y = x+<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 1.],        [1., 1.],        [1., 1.]], requires_grad=True)tensor([[3., 3.],        [3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p><img src="https://img-blog.csdnimg.cn/5199c3f14c384bde80002df88702b5c7.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn((<span class="number">2</span>, <span class="number">3</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = x+<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.grad.zero_()</span></span><br><span class="line">y = y.<span class="built_in">sum</span>()  <span class="comment"># 必须将y最终结果化为标量才能进行求导操作</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y*<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[-1.5443, -0.0354, -0.8403],        [ 1.5566,  0.7182,  1.5884]], requires_grad=True)tensor([[0.4557, 1.9646, 1.1597],        [3.5566, 2.7182, 3.5884]], grad_fn=&lt;AddBackward0&gt;)tensor([[1., 1., 1.],        [1., 1., 1.]])tensor(1720.7292, grad_fn=&lt;MulBackward0&gt;)</code></pre><h3 id="线性回归理解"><a href="#线性回归理解" class="headerlink" title="线性回归理解"></a>线性回归理解</h3><p>给定一个数据集，我们的目标是寻找模型的权重和偏置， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。</p><p><img src="https://img-blog.csdnimg.cn/500d56e0602c4faa83e78a05c5dd2000.png" alt="在这里插入图片描述"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img src="https://img-blog.csdnimg.cn/a48ba0d06cdb4cf2bd111c534e5f21a0.png" alt="在这里插入图片描述"></p><h3 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h3><p>其中d2l包需要自己导入离线安装，附上链接：<a href="https://blog.csdn.net/NEUQ_snowy/article/details/126118438?spm=1001.2014.3001.5502">感谢SWY大神</a></p><p>顺便补一个d2l_zh的包：<a href="https://blog.csdn.net/NEUQ_snowy/article/details/123969408?spm=1001.2014.3001.5502">再次感谢SWY大神</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l  <span class="comment"># 李沐大神自己写的包，需要自己导入</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">%matplotlib</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Using matplotlib backend: QtAgg</code></pre><p><img src="https://img-blog.csdnimg.cn/2e130b7b77094757bd3096c4bf6ab54b.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="comment"># &quot;生成 y=Xw+b+e&quot; e是噪音</span></span><br><span class="line">    <span class="comment"># 均值为0，标准差为1的随机数，行数等于样本数，列数等于w的长度的随机数</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    <span class="comment"># y=Xw+b</span></span><br><span class="line">    y = torch.matmul(X, w)+b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)  <span class="comment"># 加了点噪音</span></span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 转化成只有一列</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features&#x27;</span>, features[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;labels&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>features tensor([-0.3295, -0.7524])labels tensor([6.0904])</code></pre><p>读取小批量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):  <span class="comment"># batch_size 批量大小</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="comment"># 随机读取样本</span></span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 随机打乱样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):  <span class="comment"># 从0开始，到样本总数数量结束,每次跳batch_size大小</span></span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i:<span class="built_in">min</span>(i+batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。 每个批量的特征维度显示批量大小和输入特征数。 同样的，批量的标签形状与batch_size相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.7996,  0.4114],        [ 0.0760,  1.0155],        [ 0.6860,  0.2270],        [-2.0028,  0.9113],        [ 0.4199, -0.6192],        [-0.0252,  2.0244],        [ 1.7162, -0.4195],        [ 0.6579, -0.3955],        [-1.5817, -0.1407],        [-1.0975,  0.1836]])  tensor([[ 4.4066],        [ 0.8990],        [ 4.8128],        [-2.8907],        [ 7.1436],        [-2.7258],        [ 9.0706],        [ 6.8511],        [ 1.5068],        [ 1.3921]])</code></pre><p>初始化模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w=torch.normal(<span class="number">0</span>,<span class="number">0.01</span>,size=(<span class="number">2</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=torch.zeros(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>定义模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w)+b</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>定义损失函数<br><img src="https://img-blog.csdnimg.cn/e11ed845eec041faad31b003213b6b08.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="comment"># 均方损失</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat-y.reshape(y_hat.shape))**<span class="number">2</span>/<span class="number">2</span></span><br></pre></td></tr></table></figure><p>定义优化算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="comment"># 小批量的随机梯度下降</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># para参数列表，lr是学习率，batch_size</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad/batch_size</span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure><p>开始训练</p><p>另外附上python特有的print(f’’)用法：<a href="https://blog.csdn.net/Joey9898/article/details/121940891?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165992664916781432975725%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165992664916781432975725&biz_id=0&spm=1018.2226.3001.4187">传送门</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span>  <span class="comment"># 学习率是0.03(太小效率太低，太大容易超出范围，造成摆动)</span></span><br><span class="line">num_epochs = <span class="number">3</span>  <span class="comment"># 训练次数是3次</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  <span class="comment"># x和y的小批量损失</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度进行更新</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.038161epoch 2, loss 0.000139epoch 3, loss 0.000048</code></pre><p>输出通过学习修正过的参数值，评估训练成功程度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure><pre><code>w的估计误差: tensor([ 0.0002, -0.0004])b的估计误差: tensor([-0.0006])tensor([[ 1.9998, -3.3996]])tensor([4.2006])</code></pre><h3 id="线性回归深度学习框架的简洁实现"><a href="#线性回归深度学习框架的简洁实现" class="headerlink" title="线性回归深度学习框架的简洁实现"></a>线性回归深度学习框架的简洁实现</h3><p>使用pytorch的nn来实现加载数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>创建初始w，b。<br>并且生成标签数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">true_w =torch.tensor([<span class="number">2</span>,-<span class="number">3.4</span>])</span><br><span class="line">true_b =<span class="number">4.2</span></span><br><span class="line">features,labels=d2l.synthetic_data(true_w,true_b,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>构造pytorch迭代器</p><p><a href="https://blog.csdn.net/qq_40211493/article/details/107529148?ops_request_misc=&request_id=&biz_id=102&spm=1018.2226.3001.4187">TensorDataset</a></p><p><a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">DataLoader</a></p><p><a href="https://pytorch.org/docs/stable/fx.html?highlight=next#torch.fx.Node.next">next</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrrays, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># 构建迭代器</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br><span class="line"><span class="comment"># 这里我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>[tensor([[ 0.1652,  0.8560],         [ 0.0147,  1.5673],         [ 0.9652, -0.2222],         [ 0.4242,  0.1483],         [ 0.8722, -1.3510],         [ 0.0506,  0.8717],         [-1.2767, -0.0087],         [-0.4736,  0.8434],         [-0.1852, -0.9320],         [-1.6329,  0.1184]]), tensor([[ 1.6253],         [-1.0980],         [ 6.8757],         [ 4.5338],         [10.5444],         [ 1.3342],         [ 1.6711],         [ 0.3882],         [ 7.0090],         [ 0.5239]])]</code></pre><p>使用深度学习框架定好的层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn是神经网络的缩写</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>)) <span class="comment"># 2是输入维度，1是输出维度</span></span><br></pre></td></tr></table></figure><p>初始化模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>,<span class="number">0.01</span>) <span class="comment">#使用正太分布替换它的值，均值0，方差0.01</span></span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>) <span class="comment">#偏差</span></span><br></pre></td></tr></table></figure><p>计算均方误差(平方范数)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss=nn.MSELoss()</span><br></pre></td></tr></table></figure><p>实例化SGD实例(优化算法)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="comment">#第一个参数将w，b传入</span></span><br><span class="line"><span class="comment">#第二个参数设置的学习率</span></span><br></pre></td></tr></table></figure><p>开始训练</p><p>回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（train_data）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:</p><ul><li><p>通过调用net(X)生成预测并计算损失l（前向传播）。</p></li><li><p>通过进行反向传播来计算梯度。</p></li><li><p>通过调用优化器来更新模型参数。</p></li></ul><p>为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span>  <span class="comment"># 训练三次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward() <span class="comment">#已经做过了sum</span></span><br><span class="line">        trainer.step() <span class="comment">#模型更新</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.000246epoch 2, loss 0.000098epoch 3, loss 0.000098</code></pre><p>输出并评估训练结果</p><p>下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w的估计误差：&#x27;</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b的估计误差：&#x27;</span>, true_b - b)</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure><pre><code>w的估计误差： tensor([ 0.0002, -0.0004])b的估计误差： tensor([-0.0006])tensor([[ 1.9998, -3.3996]])tensor([4.2006])</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;自动求导尝试&quot;&gt;&lt;a href=&quot;#自动求导尝试&quot; class=&quot;headerlink&quot; title=&quot;自动求导尝试&quot;&gt;&lt;/a&gt;自动求导尝试&lt;/h3&gt;&lt;p&gt;torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习-pytorch-张量tensor语法</title>
    <link href="http://example.com/2022/08/05/pytorch%E8%AF%AD%E6%B3%95/"/>
    <id>http://example.com/2022/08/05/pytorch%E8%AF%AD%E6%B3%95/</id>
    <published>2022-08-05T12:34:46.544Z</published>
    <updated>2022-08-05T12:44:21.292Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=torch.arange(<span class="number">12</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x.shape</span><br><span class="line">x.numel <span class="comment">#元素数量</span></span><br><span class="line">x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">y=torch.zeros(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line">torch.cat((x, y), dim=<span class="number">0</span>), torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># dim=0按行合并  dim=1按列合并</span></span><br><span class="line">x==y</span><br><span class="line"><span class="comment"># 返回一个矩阵，对应位置返回bool</span></span><br><span class="line">x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><pre><code>tensor([[1, 2, 3],        [3, 4, 5],        [4, 3, 2]])tensor([2., 2., 2., 2.])tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.]])tensor([[2., 1., 5., 3.],        [3., 2., 2., 3.],        [3., 2., 3., 3.]])tensor(66.)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广播(很容易错)</span></span><br><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line">a+b</span><br></pre></td></tr></table></figure><pre><code>tensor([[0, 1],        [1, 2],        [2, 3]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x[-<span class="number">1</span>], x[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"><span class="comment"># x可以选择最后一个元素</span></span><br><span class="line"><span class="comment"># 可以[1:3]选择第二个和第三个元素[2,3)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.]])(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=torch.zeros(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x[<span class="number">1</span>,<span class="number">2</span>]=<span class="number">9</span></span><br><span class="line"><span class="built_in">print</span>(x) </span><br><span class="line">x[<span class="number">0</span>:<span class="number">2</span>,:]=<span class="number">12</span> <span class="comment">#列的全部</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">before = <span class="built_in">id</span>(y)</span><br><span class="line">y+=x</span><br><span class="line"><span class="built_in">id</span>(y)==before</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0., 0.],        [0., 0., 0., 0.],        [0., 0., 0., 0.]])tensor([[0., 0., 0., 0.],        [0., 0., 9., 0.],        [0., 0., 0., 0.]])tensor([[12., 12., 12., 12.],        [12., 12., 12., 12.],        [ 0.,  0.,  0.,  0.]])True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before=<span class="built_in">id</span>(x)</span><br><span class="line">x+=y</span><br><span class="line"><span class="built_in">id</span>(x)==before</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;./&#x27;</span>, <span class="string">&quot;data&quot;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;./&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRoom,Alley,Price\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;Na,Pave,127500\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;2,Na,199999\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;5,Jager,199999\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;3,Bruse,199999\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">data</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>NumRoom</th>      <th>Alley</th>      <th>Price</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Na</td>      <td>Pave</td>      <td>127500</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>Na</td>      <td>199999</td>    </tr>    <tr>      <th>2</th>      <td>5</td>      <td>Jager</td>      <td>199999</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>Bruse</td>      <td>199999</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>)</span><br><span class="line">b = a.reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">b[:] = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># 尽量不要乱改</span></span><br></pre></td></tr></table></figure><pre><code>tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])tensor([[2, 2, 2, 2],        [2, 2, 2, 2],        [2, 2, 2, 2]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">3.0</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3.9</span>])</span><br><span class="line">x+y</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x))</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.T)  <span class="comment"># 转置</span></span><br><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)  <span class="comment"># 重要</span></span><br><span class="line">b = a.clone()</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a*b)</span><br><span class="line">a = <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>4torch.Size([4])tensor([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11],        [12, 13, 14, 15],        [16, 17, 18, 19]])tensor([[ 0,  4,  8, 12, 16],        [ 1,  5,  9, 13, 17],        [ 2,  6, 10, 14, 18],        [ 3,  7, 11, 15, 19]])tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.],        [12., 13., 14., 15.],        [16., 17., 18., 19.]])tensor([[  0.,   1.,   4.,   9.],        [ 16.,  25.,  36.,  49.],        [ 64.,  81., 100., 121.],        [144., 169., 196., 225.],        [256., 289., 324., 361.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">sum_1 = a.<span class="built_in">sum</span>(axis=<span class="number">0</span>)  <span class="comment"># 竖着求和</span></span><br><span class="line"><span class="built_in">print</span>(sum_1)</span><br><span class="line">sum_2 = a.<span class="built_in">sum</span>(axis=<span class="number">1</span>)  <span class="comment"># 横着求和</span></span><br><span class="line"><span class="built_in">print</span>(sum_2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11]])tensor([12, 15, 18, 21])tensor([ 6, 22, 38])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.mean())  <span class="comment"># 求平均值</span></span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>()/a.numel())  <span class="comment"># 求平均值</span></span><br></pre></td></tr></table></figure><pre><code>tensor(5.5000, dtype=torch.float64)tensor(5.5000, dtype=torch.float64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保持维度求和</span></span><br><span class="line">a = torch.arange(<span class="number">12</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">sum_a = a.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(sum_a)</span><br><span class="line">sum1=a.cumsum(axis=<span class="number">0</span>) <span class="comment">#累加，结果显示于最后一行</span></span><br><span class="line"><span class="built_in">print</span>(sum1)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 6.],        [22.],        [38.]], dtype=torch.float64)tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  6.,  8., 10.],        [12., 15., 18., 21.]], dtype=torch.float64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">12</span>, dtype=<span class="built_in">float</span>)</span><br><span class="line">b = torch.ones(<span class="number">12</span>, dtype=<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.dot(a, b))</span><br><span class="line"></span><br><span class="line">a = torch.arange(<span class="number">12</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b = torch.ones((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">torch.<span class="built_in">sum</span>(a*b)  <span class="comment"># 点乘</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor(66., dtype=torch.float64)tensor(66., dtype=torch.float64)</code></pre><p><img src="https://img-blog.csdnimg.cn/c4a21b9dea474320bf13b7373126bf90.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><pre><code>tensor(7.0711)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones((<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>(axis=<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>(axis=<span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">2</span>]).shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor([[[1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.]],        [[1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.],         [1., 1., 1., 1.]]])torch.Size([2, 4])torch.Size([5, 4])torch.Size([5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">4</span>, dtype=<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"></span><br><span class="line">b = <span class="number">2</span>*torch.dot(a, a)</span><br><span class="line">b.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"></span><br><span class="line">a.grad.zero_() <span class="comment">#下划线 重写内容</span></span><br><span class="line">y=a.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"></span><br><span class="line">a.grad.zero_()</span><br><span class="line">y=a*a</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">a.grad</span><br><span class="line"></span><br><span class="line">a.grad.zero_()</span><br><span class="line">y=a*a</span><br><span class="line">u=y.detach()</span><br><span class="line">z=u*a</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">a.grad==u</span><br></pre></td></tr></table></figure><pre><code>Nonetensor([ 0.,  4.,  8., 12.], dtype=torch.float64)tensor([1., 1., 1., 1.], dtype=torch.float64)tensor([True, True, True, True])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">4</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.arange(<span class="number">4</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">result = torch.empty(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">torch.add(a, b, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">a.add_(b)  <span class="comment"># 一定要有下划线！！！下划线等于c++的&quot;+=&quot;</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 2.],        [0., 0.]])tensor([[0., 2.],        [4., 6.]])tensor([[0., 1.],        [2., 3.]], dtype=torch.float64)tensor([[0., 1.],        [2., 3.]], dtype=torch.float64)tensor([[0., 2.],        [4., 6.]], dtype=torch.float64)</code></pre><p><img src="https://img-blog.csdnimg.cn/1a0d8c70da294523b0a3426a2747971c.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>, dtype=<span class="built_in">float</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">1</span>:<span class="number">3</span>, :])  <span class="comment"># 左闭右开</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.]], dtype=torch.float64)tensor([ 2.,  6., 10.], dtype=torch.float64)tensor([[ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.]], dtype=torch.float64)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>2018-HBCPC题解</title>
    <link href="http://example.com/2022/08/05/2018HBCPC/"/>
    <id>http://example.com/2022/08/05/2018HBCPC/</id>
    <published>2022-08-05T09:59:30.371Z</published>
    <updated>2022-08-05T10:05:17.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2018HBCPC部分题解"><a href="#2018HBCPC部分题解" class="headerlink" title="2018HBCPC部分题解"></a>2018HBCPC部分题解</h1><h3 id="Mex-Query"><a href="#Mex-Query" class="headerlink" title="Mex Query"></a>Mex Query</h3><p><img src="https://img-blog.csdnimg.cn/78b1e15fab854d69829b1673d32b1880.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 传送门：http://newoj.acmclub.cn/problems/2011</span></span><br><span class="line"><span class="type">int</span> T;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span> (T--)</span><br><span class="line">    &#123;</span><br><span class="line">        set&lt;<span class="type">int</span>&gt; s;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> remp;</span><br><span class="line">            cin &gt;&gt; remp;</span><br><span class="line">            s.<span class="built_in">insert</span>(remp);</span><br><span class="line">        &#125;</span><br><span class="line">        set&lt;<span class="type">int</span>&gt;::iterator it;</span><br><span class="line">        <span class="type">int</span> cont = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (it = s.<span class="built_in">begin</span>(); it != s.<span class="built_in">end</span>(); it++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (*it != cont)</span><br><span class="line">            &#123;</span><br><span class="line">                cout &lt;&lt; cont &lt;&lt; endl;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            cont++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure><h3 id="icebound的商店"><a href="#icebound的商店" class="headerlink" title="icebound的商店"></a>icebound的商店</h3><p><img src="https://img-blog.csdnimg.cn/7b56213d43d54c969311edff868073fb.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/78e710f6d20e4351a1c343a2f8cf6bde.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// http://newoj.acmclub.cn/problems/2012</span></span><br><span class="line"><span class="comment">// 完全背包</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> mod 1000000009</span></span><br><span class="line"><span class="type">int</span> bag[<span class="number">15</span>];</span><br><span class="line"><span class="type">int</span> ans[<span class="number">3010</span>];</span><br><span class="line"><span class="type">int</span> T;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    bag[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    bag[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">3</span>; i &lt;= <span class="number">15</span>; i++)</span><br><span class="line">        bag[i] = bag[i - <span class="number">1</span>] + bag[i - <span class="number">2</span>];</span><br><span class="line">    ans[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">15</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = bag[i]; j &lt;= <span class="number">3010</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            ans[j] += ans[j - bag[i]];</span><br><span class="line">            ans[j] %= mod;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= T; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> tar;</span><br><span class="line">        cin &gt;&gt; tar;</span><br><span class="line">        cout &lt;&lt; ans[tar] &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Nim-Game"><a href="#Nim-Game" class="headerlink" title="Nim Game"></a>Nim Game</h3><p><img src="https://img-blog.csdnimg.cn/b401dfad9760498aa908a1fa20d70529.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/7deb10becb0544e5ad5d09ed4541855f.png" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/8bc6b80757c94db599f51d739f034a42.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 博弈论：Nim</span></span><br><span class="line"><span class="comment">// http://newoj.acmclub.cn/problems/2013</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MOD = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="type">int</span> T;</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> a[<span class="number">1000100</span>];</span><br><span class="line"><span class="type">int</span> sum[<span class="number">1000100</span>];</span><br><span class="line"><span class="type">int</span> f[<span class="number">1000100</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span> (T--)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">        <span class="built_in">memset</span>(sum, <span class="number">0</span>, <span class="built_in">sizeof</span>(sum));</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; a[i];</span><br><span class="line">            sum[i] = sum[i - <span class="number">1</span>] ^ a[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> l, r;</span><br><span class="line">            cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">            <span class="type">int</span> result = sum[r] ^ sum[l - <span class="number">1</span>]; <span class="comment">//第i次结果</span></span><br><span class="line">            <span class="keyword">if</span> (result)</span><br><span class="line">                f[i] = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                f[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            ans = ans &lt;&lt; <span class="number">1</span>;</span><br><span class="line">            ans += f[i];</span><br><span class="line">            ans = ans % MOD;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="神殿"><a href="#神殿" class="headerlink" title="神殿"></a>神殿</h3><p><img src="https://img-blog.csdnimg.cn/b7f12475c56944aba0e0cd157b61787f.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20e3f8f258204d43b9a02a4f611c4222.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// http://newoj.acmclub.cn/problems/2016</span></span><br><span class="line"><span class="type">long</span> <span class="type">long</span> l, r;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">    <span class="keyword">while</span> (l &lt;= r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> ((l | l + <span class="number">1</span>) &gt; r) <span class="comment">//(l | l + 1)为的是将l的最低一位0尝试改成1</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        l = (l | l + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; l &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="跑图"><a href="#跑图" class="headerlink" title="跑图"></a>跑图</h3><p><img src="https://img-blog.csdnimg.cn/fd6e64fb237e4100bac2f894c80f2302.png" alt="在这里插入图片描述"><br>TLE解法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// TLE解法。。。</span></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> graph[<span class="number">510</span>][<span class="number">510</span>];</span><br><span class="line"><span class="type">int</span> ans[<span class="number">510</span>][<span class="number">510</span>];</span><br><span class="line"><span class="type">int</span> cnt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">&#125; point[<span class="number">250010</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">memset</span>(ans, <span class="number">0x3f</span>, <span class="built_in">sizeof</span>(ans));</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; graph[i][j];</span><br><span class="line">            <span class="keyword">if</span> (graph[i][j])</span><br><span class="line">            &#123;</span><br><span class="line">                ++cnt;</span><br><span class="line">                point[cnt].x = i;</span><br><span class="line">                point[cnt].y = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">1</span>; k &lt;= cnt; k++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (graph[i][j])</span><br><span class="line">                    ans[i][j] = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    ans[i][j] = <span class="built_in">min</span>(ans[i][j], <span class="built_in">abs</span>(point[k].x - i) + <span class="built_in">abs</span>(point[k].y - j));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; ans[i][j];</span><br><span class="line">            <span class="keyword">if</span> (j != m)</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>正解</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// http://newoj.acmclub.cn/problems/2018</span></span><br><span class="line"><span class="comment">// 正解：BFS</span></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> graph[<span class="number">510</span>][<span class="number">510</span>];</span><br><span class="line"><span class="type">bool</span> visit[<span class="number">510</span>][<span class="number">510</span>];</span><br><span class="line"><span class="type">int</span> ans[<span class="number">510</span>][<span class="number">510</span>];</span><br><span class="line"><span class="type">int</span> dirx[] = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">1</span>&#125;;</span><br><span class="line"><span class="type">int</span> diry[] = &#123;<span class="number">1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">&#125;;</span><br><span class="line">queue&lt;node&gt; q;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; graph[i][j];</span><br><span class="line">            <span class="keyword">if</span> (graph[i][j])</span><br><span class="line">            &#123;</span><br><span class="line">                node remp;</span><br><span class="line">                remp.x = i, remp.y = j;</span><br><span class="line">                visit[i][j] = <span class="literal">true</span>;</span><br><span class="line">                q.<span class="built_in">push</span>(remp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (q.<span class="built_in">empty</span>() == <span class="literal">false</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        node remp = q.<span class="built_in">front</span>();</span><br><span class="line">        <span class="type">int</span> rempx = remp.x, rempy = remp.y;</span><br><span class="line">        q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> nextx = rempx + dirx[i];</span><br><span class="line">            <span class="type">int</span> nexty = rempy + diry[i];</span><br><span class="line">            <span class="keyword">if</span> (visit[nextx][nexty] == <span class="literal">false</span> &amp;&amp; (nextx &lt;= n &amp;&amp; nextx &gt;= <span class="number">1</span> &amp;&amp; nexty &lt;= m &amp;&amp; nexty &gt;= <span class="number">1</span>))</span><br><span class="line">            &#123;</span><br><span class="line">                ans[nextx][nexty] = ans[rempx][rempy] + <span class="number">1</span>;</span><br><span class="line">                visit[nextx][nexty] = <span class="literal">true</span>; <span class="comment">//易错点！！一定要第一次更新完就马上标记！因为此时不标记，后面可能被二次标记，这时候就不是最近的了</span></span><br><span class="line">                node next;</span><br><span class="line">                next.x = nextx, next.y = nexty;</span><br><span class="line">                q.<span class="built_in">push</span>(next);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; ans[i][j];</span><br><span class="line">            <span class="keyword">if</span> (j != m)</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="520"><a href="#520" class="headerlink" title="520"></a>520</h3><p><img src="https://img-blog.csdnimg.cn/72619b70c994481eb4e4285fe0e87309.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 快速幂</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MOD 20180520</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">fast_power</span><span class="params">(<span class="type">long</span> <span class="type">long</span> a, <span class="type">long</span> <span class="type">long</span> b, <span class="type">long</span> <span class="type">long</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> ans = <span class="number">1</span>;</span><br><span class="line">    a = a % c;</span><br><span class="line">    <span class="keyword">while</span> (b)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (b % <span class="number">2</span>)</span><br><span class="line">            ans = (ans * a) % c;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">        a = (a * a) % c;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">fast_power</span>(<span class="number">2</span>, n, MOD);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="icebound的账单"><a href="#icebound的账单" class="headerlink" title="icebound的账单"></a>icebound的账单</h3><p><img src="https://img-blog.csdnimg.cn/fb0b3a0683f04b37918bd724c7fed67f.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;2018HBCPC部分题解&quot;&gt;&lt;a href=&quot;#2018HBCPC部分题解&quot; class=&quot;headerlink&quot; title=&quot;2018HBCPC部分题解&quot;&gt;&lt;/a&gt;2018HBCPC部分题解&lt;/h1&gt;&lt;h3 id=&quot;Mex-Query&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="HBCPC" scheme="http://example.com/categories/HBCPC/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>暑假集训week3</title>
    <link href="http://example.com/2022/08/05/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%ADweek3/"/>
    <id>http://example.com/2022/08/05/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%ADweek3/</id>
    <published>2022-08-05T07:42:19.627Z</published>
    <updated>2022-08-05T07:56:28.099Z</updated>
    
    <content type="html"><![CDATA[<h1 id="暑假集训-week3-动态规划"><a href="#暑假集训-week3-动态规划" class="headerlink" title="暑假集训-week3-动态规划"></a>暑假集训-week3-动态规划</h1><h3 id="A-最大子段和"><a href="#A-最大子段和" class="headerlink" title="A - 最大子段和"></a>A - 最大子段和</h3><p><img src="https://img-blog.csdnimg.cn/238efd51413848d7a72c7baff0d327be.png" alt="在这里插入图片描述"><br>经典的最大子段和</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 最大子段和(dp模板题)</span></span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P1115</span></span><br><span class="line"><span class="type">int</span> a[<span class="number">200001</span>];</span><br><span class="line"><span class="type">int</span> dp[<span class="number">200001</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">-9999999</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        dp[i] = <span class="built_in">max</span>(dp[i - <span class="number">1</span>] + a[i], a[i]);</span><br><span class="line">        ans = <span class="built_in">max</span>(dp[i], ans);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="B-Max-Sum-Plus-Plus"><a href="#B-Max-Sum-Plus-Plus" class="headerlink" title="B - Max Sum Plus Plus"></a>B - Max Sum Plus Plus</h3><p><img src="https://img-blog.csdnimg.cn/8adfbd38bf444e779fa45f4e2d7220e9.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://vjudge.csgrandeur.cn/contest/507882#problem/B</span></span><br><span class="line"><span class="comment">// 参考题解：https://blog.csdn.net/weixin_44035017/article/details/103318078?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165934103416782388023006%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=165934103416782388023006&amp;biz_id=0&amp;spm=1018.2226.3001.4187</span></span><br><span class="line"><span class="comment">// 二维dp+滚动数组优化</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">1e6</span> + <span class="number">9</span>;</span><br><span class="line"><span class="type">int</span> n, m, a[maxn], dp[maxn], lastmax[maxn];</span><br><span class="line"><span class="type">int</span> remp_sum, sum;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;m, &amp;n) != EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;a[i]);</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="built_in">sizeof</span>(dp));</span><br><span class="line">        <span class="built_in">memset</span>(lastmax, <span class="number">0</span>, <span class="built_in">sizeof</span>(lastmax));</span><br><span class="line">        remp_sum = <span class="number">0</span>, sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            sum = <span class="number">-0x3f3f3f3f</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i; j &lt;= n; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j - <span class="number">1</span>] + a[j], lastmax[j - <span class="number">1</span>] + a[j]);</span><br><span class="line">                lastmax[j - <span class="number">1</span>] = sum;</span><br><span class="line">                sum = <span class="built_in">max</span>(sum, dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; sum &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="C-Longest-Ordered-Subsequence"><a href="#C-Longest-Ordered-Subsequence" class="headerlink" title="C - Longest Ordered Subsequence"></a>C - Longest Ordered Subsequence</h3><p><img src="https://img-blog.csdnimg.cn/a3ca8fddece64e1b96a7d34b3e4e1612.png" alt="在这里插入图片描述"><br>最大上升子序列模板</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 模板：最大上升子序列</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">100010</span>, INF = <span class="number">0x7f7f7f7f</span>;</span><br><span class="line"><span class="type">int</span> a[maxn], dp[maxn]; <span class="comment">// dp[i]代表以a[i]结尾的子序列最大长度</span></span><br><span class="line"><span class="type">int</span> n, ans = -INF;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;a[i]);</span><br><span class="line">        dp[i] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; i; j++)</span><br><span class="line">            <span class="keyword">if</span> (a[j] &lt; a[i])</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        ans = <span class="built_in">max</span>(ans, dp[i]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, ans);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="D-采药"><a href="#D-采药" class="headerlink" title="D - 采药"></a>D - 采药</h3><p><img src="https://img-blog.csdnimg.cn/9a5693f4094949d88547866bc7cb25ea.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 01背包模板题</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">10000</span>;</span><br><span class="line"><span class="type">int</span> t, m;</span><br><span class="line"><span class="type">int</span> cost[maxn], value[maxn], bag[maxn];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; t &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; cost[i] &gt;&gt; value[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = t; j &gt;= cost[i]; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            bag[j] = <span class="built_in">max</span>(bag[j - cost[i]] + value[i], bag[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; bag[t];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="E-Piggy-Bank"><a href="#E-Piggy-Bank" class="headerlink" title="E - Piggy-Bank"></a>E - Piggy-Bank</h3><p><img src="https://img-blog.csdnimg.cn/15f3afc44c1c49d3a3d3b71718d8fa33.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/11008687b1524b1ea0d8eff1aa6775ce.png" alt="在这里插入图片描述"><br>完全背包分别求最大和求最小<br>求最小时先全统一赋值为无穷大，然后再将初始赋值为0<br>顺便快读的板子也在这里了</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 传送门：https://vjudge.csgrandeur.cn/contest/507882#problem/E</span></span><br><span class="line"><span class="comment">// 完全背包</span></span><br><span class="line"><span class="comment">// 快读不能和清缓存一起用！</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">100100</span>;</span><br><span class="line"><span class="type">int</span> t, e, f, n, m; <span class="comment">// n为个数，m为背包大小</span></span><br><span class="line"><span class="type">int</span> cost[maxn], value[maxn], bag[maxn];</span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">int</span> <span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>, w = <span class="number">1</span>;</span><br><span class="line">    <span class="type">char</span> ch = <span class="built_in">getchar</span>();</span><br><span class="line">    <span class="keyword">while</span> (ch &lt; <span class="string">&#x27;0&#x27;</span> || ch &gt; <span class="string">&#x27;9&#x27;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (ch == <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">            w = <span class="number">-1</span>;</span><br><span class="line">        ch = <span class="built_in">getchar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (ch &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; ch &lt;= <span class="string">&#x27;9&#x27;</span>)</span><br><span class="line">        s = s * <span class="number">10</span> + ch - <span class="string">&#x27;0&#x27;</span>, ch = <span class="built_in">getchar</span>();</span><br><span class="line">    <span class="keyword">return</span> s * w;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    t = <span class="built_in">read</span>();</span><br><span class="line">    <span class="keyword">while</span> (t--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(bag, <span class="number">0x3f</span>, <span class="built_in">sizeof</span>(bag));</span><br><span class="line">        e = <span class="built_in">read</span>(), f = <span class="built_in">read</span>(), n = <span class="built_in">read</span>();</span><br><span class="line">        m = f - e;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            value[i] = <span class="built_in">read</span>();</span><br><span class="line">            cost[i] = <span class="built_in">read</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        bag[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> p = value[i];</span><br><span class="line">            <span class="type">int</span> w = cost[i];</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = w; j &lt;= m; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                bag[j] = <span class="built_in">min</span>(p + bag[j - w], bag[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (bag[m] == <span class="number">0x3f3f3f3f</span>)</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;This is impossible.&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;The minimum amount of money in the piggy-bank is %d.\n&quot;</span>, bag[m]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="F-Dividing"><a href="#F-Dividing" class="headerlink" title="F - Dividing"></a>F - Dividing</h3><p><img src="https://img-blog.csdnimg.cn/47edf6090bff4b6cb67807105307c942.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/d01b8e4fbed343a68155f790f1febfca.png" alt="在这里插入图片描述"><br>多重背包的二进制优化</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://vjudge.csgrandeur.cn/contest/507882#problem/F</span></span><br><span class="line"><span class="comment">// 多重背包优化</span></span><br><span class="line"><span class="comment">// 二进制优化</span></span><br><span class="line"><span class="type">int</span> cnt[<span class="number">601000</span>];</span><br><span class="line"><span class="type">int</span> sum;</span><br><span class="line"><span class="type">int</span> value[<span class="number">601000</span>]; <span class="comment">//此题不需要空间</span></span><br><span class="line"><span class="type">int</span> dp[<span class="number">601000</span>];</span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">int</span> <span class="title">read</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">6</span>; i++)</span><br><span class="line">        cin &gt;&gt; cnt[i], sum += cnt[i] * i;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> flag = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">read</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="built_in">sizeof</span>(dp));</span><br><span class="line">        <span class="built_in">memset</span>(value, <span class="number">0</span>, <span class="built_in">sizeof</span>(value));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Collection #%d:\n&quot;</span>, ++flag);</span><br><span class="line">        <span class="keyword">if</span> (sum % <span class="number">2</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Can&#x27;t be divided.\n\n&quot;</span>);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> num = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">6</span>; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> remp = cnt[i];</span><br><span class="line">            <span class="type">int</span> k = <span class="number">1</span>; <span class="comment">//二进制个数</span></span><br><span class="line">            <span class="keyword">while</span> (k &lt;= remp)</span><br><span class="line">            &#123;</span><br><span class="line">                num++;</span><br><span class="line">                value[num] = i * k;</span><br><span class="line">                remp -= k;</span><br><span class="line">                k *= <span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (remp &gt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                num++;</span><br><span class="line">                value[num] = i * remp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        sum /= <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= num; i++)</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = sum; j &gt;= value[i]; j--)</span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - value[i]] + value[i]); <span class="comment">//注意传进max内的两个参数</span></span><br><span class="line">        <span class="keyword">if</span> (dp[sum] == sum)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Can be divided.\n\n&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Can&#x27;t be divided.\n\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="G-石子合并"><a href="#G-石子合并" class="headerlink" title="G - 石子合并"></a>G - 石子合并</h3><p><img src="https://img-blog.csdnimg.cn/eb65a0b6516547d7826e15d3ee4b54e4.png" alt="在这里插入图片描述"><br>一个环形的链！！！<br>处理方法很有意思，直接将原链扩充二倍，在长度2*n的直链取一个长度n的链得到最大贡献值</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 区间dp:https://www.luogu.com.cn/problem/P1880</span></span><br><span class="line"><span class="comment">// 因为是一个环形区间，我们可以通过将数组扩充两倍来实现</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">330</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> a[maxn], sum[maxn], dp_min[maxn][maxn], dp_max[maxn][maxn];</span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">int</span> <span class="title">d</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> sum[j] - sum[i - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="built_in">memset</span>(dp_min, <span class="number">0x3f3f3f3f</span>, <span class="built_in">sizeof</span>(dp_min));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">2</span> * n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i + n] = a[i];</span><br><span class="line">        sum[i] = a[i] + sum[i - <span class="number">1</span>];</span><br><span class="line">        dp_min[i][i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> len = <span class="number">1</span>; len &lt; n; len++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> l = <span class="number">1</span>, r = l + len; (r &lt;= n + n) &amp;&amp; (l &lt;= n + n); l++, r = l + len)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = l; k &lt; r; k++) <span class="comment">//这个地方是小于号！！！加了等于的话就超出边界了！</span></span><br><span class="line">            &#123;</span><br><span class="line">                dp_max[l][r] = <span class="built_in">max</span>(dp_max[l][r], dp_max[l][k] + dp_max[k + <span class="number">1</span>][r] + <span class="built_in">d</span>(l, r));</span><br><span class="line">                dp_min[l][r] = <span class="built_in">min</span>(dp_min[l][r], dp_min[l][k] + dp_min[k + <span class="number">1</span>][r] + <span class="built_in">d</span>(l, r));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> max1 = <span class="number">-0x3f3f3f3f</span>, min1 = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        max1 = <span class="built_in">max</span>(max1, dp_max[i][i + n - <span class="number">1</span>]);</span><br><span class="line">        min1 = <span class="built_in">min</span>(min1, dp_min[i][i + n - <span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; min1 &lt;&lt; endl</span><br><span class="line">         &lt;&lt; max1 &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="H-能量项链"><a href="#H-能量项链" class="headerlink" title="H - 能量项链"></a>H - 能量项链</h3><p><img src="https://img-blog.csdnimg.cn/35fe627ab7c84e918b942a6b0425e413.png" alt="在这里插入图片描述"><br>首尾的处理很有意思<br><img src="https://img-blog.csdnimg.cn/915e026c66f44058b3c79407822ee74c.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 区间dp</span></span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P1063</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">3</span> * <span class="number">110</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> head[maxn], tail[maxn];</span><br><span class="line"><span class="type">int</span> dp[maxn][maxn];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; head[i];</span><br><span class="line">        head[i + n] = head[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">2</span> * n - <span class="number">1</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        tail[i] = head[i + <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    tail[<span class="number">2</span> * n] = head[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> len = <span class="number">2</span>; len &lt;= n; len++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> l = <span class="number">1</span>, r = l + len - <span class="number">1</span>; (l &lt;= <span class="number">2</span> * n) &amp;&amp; (r &lt;= <span class="number">2</span> * n); l++, r = l + len - <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = l; k &lt; r; k++)</span><br><span class="line">                dp[l][r] = <span class="built_in">max</span>(dp[l][r], dp[l][k] + dp[k + <span class="number">1</span>][r] + head[l] * tail[k] * tail[r]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        res = <span class="built_in">max</span>(res, dp[i][i + n <span class="number">-1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="I-没有上司的舞会"><a href="#I-没有上司的舞会" class="headerlink" title="I - 没有上司的舞会"></a>I - 没有上司的舞会</h3><p><img src="https://img-blog.csdnimg.cn/35d5b2a0aabb4416b26f2da99e85f70d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/171aafbea33246aab596b1cbe1c4f0be.png" alt="在这里插入图片描述"><br>树形dp模板题，看注释<br>感觉树形dp的思路还是很清晰的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> happy[<span class="number">6010</span>], dp[<span class="number">6010</span>][<span class="number">2</span>];</span><br><span class="line"><span class="comment">// dp[x][1]:代表x节点参选，此时以x为根的子树贡献最大和</span></span><br><span class="line"><span class="comment">// dp[x][0]:代表x节点不参选，此时以x为根的子树贡献最大和</span></span><br><span class="line"><span class="comment">// 传送门：https://www.luogu.com.cn/problem/P1352</span></span><br><span class="line"><span class="type">bool</span> visit[<span class="number">6010</span>];</span><br><span class="line">vector&lt;<span class="type">int</span>&gt; son[<span class="number">6010</span>];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">search</span><span class="params">(<span class="type">int</span> x)</span> <span class="comment">//搜索</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> fa = x;</span><br><span class="line">    dp[x][<span class="number">1</span>] = happy[x], dp[x][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; son[x].<span class="built_in">size</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> s = son[x][i];</span><br><span class="line">        <span class="built_in">search</span>(s);</span><br><span class="line">        dp[x][<span class="number">1</span>] += dp[s][<span class="number">0</span>];</span><br><span class="line">        dp[x][<span class="number">0</span>] += <span class="built_in">max</span>(dp[s][<span class="number">0</span>], dp[s][<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; happy[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> l, k;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; k;</span><br><span class="line">        visit[l] = <span class="literal">true</span>; <span class="comment">//他有父亲节点</span></span><br><span class="line">        son[k].<span class="built_in">push_back</span>(l);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> root; <span class="comment">//找到根节点</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="keyword">if</span> (visit[i] == <span class="literal">false</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            root = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="built_in">search</span>(root);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">max</span>(dp[root][<span class="number">1</span>], dp[root][<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="J-战略游戏"><a href="#J-战略游戏" class="headerlink" title="J - 战略游戏"></a>J - 战略游戏</h3><p><img src="https://img-blog.csdnimg.cn/9165b884c1a848928565871d0c72c49e.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P2016</span></span><br><span class="line"><span class="comment">// 树形dp</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">200000</span>;</span><br><span class="line"><span class="type">int</span> n, dp[maxn][<span class="number">2</span>];</span><br><span class="line">vector&lt;<span class="type">int</span>&gt; son[maxn];</span><br><span class="line"><span class="type">int</span> cnt;</span><br><span class="line"><span class="type">bool</span> visit[maxn];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">search</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    dp[x][<span class="number">1</span>] = <span class="number">1</span>, dp[x][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> f = x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; son[f].<span class="built_in">size</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> s = son[f][i];</span><br><span class="line">        <span class="built_in">search</span>(s);</span><br><span class="line">        <span class="comment">//若该点无士兵，则子节点必须全部都有士兵占据</span></span><br><span class="line">        dp[f][<span class="number">0</span>] += dp[s][<span class="number">1</span>];</span><br><span class="line">        <span class="comment">// 若该点没有士兵，子节点有无士兵都可，只要最小就行</span></span><br><span class="line">        dp[f][<span class="number">1</span>] += <span class="built_in">min</span>(dp[s][<span class="number">1</span>], dp[s][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> k, tar;</span><br><span class="line">        cin &gt;&gt; tar &gt;&gt; k;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= k; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> remp;</span><br><span class="line">            cin &gt;&gt; remp;</span><br><span class="line">            visit[remp] = <span class="literal">true</span>;</span><br><span class="line">            son[tar].<span class="built_in">push_back</span>(remp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>, root = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        <span class="keyword">if</span> (visit[i] == <span class="literal">false</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            root = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="built_in">search</span>(root);</span><br><span class="line">    cout &lt;&lt; <span class="built_in">min</span>(dp[root][<span class="number">0</span>], dp[root][<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;暑假集训-week3-动态规划&quot;&gt;&lt;a href=&quot;#暑假集训-week3-动态规划&quot; class=&quot;headerlink&quot; title=&quot;暑假集训-week3-动态规划&quot;&gt;&lt;/a&gt;暑假集训-week3-动态规划&lt;/h1&gt;&lt;h3 id=&quot;A-最大子段和&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="集训" scheme="http://example.com/categories/%E9%9B%86%E8%AE%AD/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>暑假集训week2</title>
    <link href="http://example.com/2022/07/31/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%ADweek2/"/>
    <id>http://example.com/2022/07/31/%E6%9A%91%E5%81%87%E9%9B%86%E8%AE%ADweek2/</id>
    <published>2022-07-31T13:56:07.157Z</published>
    <updated>2022-07-31T13:58:41.475Z</updated>
    
    <content type="html"><![CDATA[<h1 id="暑假集训-week2图论"><a href="#暑假集训-week2图论" class="headerlink" title="暑假集训-week2图论"></a>暑假集训-week2图论</h1><h3 id="A-Desert-King"><a href="#A-Desert-King" class="headerlink" title="A - Desert King"></a>A - Desert King<img src="https://img-blog.csdnimg.cn/a08e10d55ad945b683189bb63fbdd573.png" alt="在这里插入图片描述"></h3><p>最优比例生成树+01规划<br>怎么说，，，，，<br>现学的，有点超出能力范围了</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// https://vjudge.net/contest/506494#problem/A</span></span><br><span class="line"><span class="comment">// 最优比例生成树+01分数规划+prim算法+实数二分</span></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">double</span> x;</span><br><span class="line">    <span class="type">double</span> y;</span><br><span class="line">    <span class="type">double</span> h;</span><br><span class="line">&#125; point[<span class="number">10010</span>];</span><br><span class="line"><span class="type">double</span> graph[<span class="number">10010</span>][<span class="number">10010</span>]; <span class="comment">//原图，表示各点之间距离</span></span><br><span class="line"><span class="type">double</span> cost[<span class="number">10010</span>][<span class="number">10010</span>];  <span class="comment">//表示代价，即高度</span></span><br><span class="line"><span class="type">bool</span> visit[<span class="number">10010</span>];</span><br><span class="line"><span class="type">double</span> dis[<span class="number">10010</span>];</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">prim</span><span class="params">(<span class="type">double</span> tar)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">memset</span>(visit, <span class="number">0</span>, <span class="built_in">sizeof</span>(visit));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++) <span class="comment">//外循环必须是n！因为ans要把所有的距离都加上！</span></span><br><span class="line">        dis[i] = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line">    dis[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!visit[j] &amp;&amp; (x == <span class="number">-1</span> || dis[j] &lt; dis[x]))</span><br><span class="line">                x = j;</span><br><span class="line">        &#125;</span><br><span class="line">        visit[x] = <span class="number">1</span>;</span><br><span class="line">        ans += dis[x]; <span class="comment">//总距离更新</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">            <span class="keyword">if</span> (x != j)</span><br><span class="line">                dis[j] = <span class="built_in">min</span>(dis[j], cost[x][j] - tar * graph[x][j]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">while</span> (cin &gt;&gt; n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; point[i].x &gt;&gt; point[i].y &gt;&gt; point[i].h;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt;= n; j++)</span><br><span class="line">            &#123;</span><br><span class="line">                graph[i][j] = graph[j][i] = <span class="built_in">sqrt</span>((point[i].x - point[j].x) * (point[i].x - point[j].x) + (point[i].y - point[j].y) * (point[i].y - point[j].y)); <span class="comment">//距离差</span></span><br><span class="line">                cost[i][j] = cost[j][i] = <span class="built_in">fabs</span>(point[i].h - point[j].h);                                                                                         <span class="comment">//高度差</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="type">double</span> l = <span class="number">0</span>, r = <span class="number">1e5</span>;</span><br><span class="line">        <span class="keyword">while</span> (r - l &gt; <span class="number">1e-5</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">prim</span>(mid) &gt;= <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                l = mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                r = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%.3f\n&quot;</span>, r);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="P-最小生成树"><a href="#P-最小生成树" class="headerlink" title="P - 最小生成树"></a>P - 最小生成树<img src="https://img-blog.csdnimg.cn/fe37615130d84b14ac47a9044fe40bea.png" alt="在这里插入图片描述"></h3><p>克鲁斯卡尔算法</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> s;</span><br><span class="line">    <span class="type">int</span> e;</span><br><span class="line">    <span class="type">int</span> cost;</span><br><span class="line">&#125; path[<span class="number">200020</span>];</span><br><span class="line"><span class="type">int</span> fa[<span class="number">5010</span>];</span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp</span><span class="params">(node a, node b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.cost &lt; b.cost;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (fa[x] != x)</span><br><span class="line">        <span class="keyword">return</span> fa[x] = <span class="built_in">find</span>(fa[x]);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> fa[x];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">connect</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    fa[<span class="built_in">find</span>(x)] = fa[<span class="built_in">find</span>(y)];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        fa[i] = i;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; path[i].s &gt;&gt; path[i].e &gt;&gt; path[i].cost;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(path + <span class="number">1</span>, path + <span class="number">1</span> + m, cmp);</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> s = path[i].s;</span><br><span class="line">        <span class="type">int</span> e = path[i].e;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">find</span>(s) != <span class="built_in">find</span>(e))</span><br><span class="line">        &#123;</span><br><span class="line">            ans += path[i].cost;</span><br><span class="line">            <span class="built_in">connect</span>(s, e);</span><br><span class="line">            cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (cnt == n - <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;orz&quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Q-单源最短路径（标准版）"><a href="#Q-单源最短路径（标准版）" class="headerlink" title="Q - 单源最短路径（标准版）"></a>Q - 单源最短路径（标准版）</h3><p><img src="https://img-blog.csdnimg.cn/de50ba27c4f7410aa11e697aed247862.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/3300a1e15889483490b6b10834f16348.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 迪杰斯特拉(堆优化)</span></span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P4779#submit</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">100010</span>, maxn2 = <span class="number">500010</span>;</span><br><span class="line"><span class="type">int</span> head[maxn], dis[maxn], cnt;</span><br><span class="line"><span class="type">bool</span> visit[maxn];</span><br><span class="line"><span class="type">int</span> n, m, s;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">edge</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> to, dis, next;</span><br><span class="line">&#125; e[maxn2];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> u, <span class="type">int</span> v, <span class="type">int</span> d)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cnt++;</span><br><span class="line">    e[cnt].dis = d;</span><br><span class="line">    e[cnt].to = v;</span><br><span class="line">    e[cnt].next = head[u];</span><br><span class="line">    head[u] = cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> dis;</span><br><span class="line">    <span class="type">int</span> pos;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&lt;(<span class="type">const</span> node &amp;x) <span class="type">const</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> x.dis &lt; dis;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">priority_queue&lt;node&gt; q;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;n, &amp;m, &amp;s);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; ++i)</span><br><span class="line">        dis[i] = <span class="number">0x7fffffff</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">register</span> <span class="type">int</span> u, v, d;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;u, &amp;v, &amp;d);</span><br><span class="line">        <span class="built_in">add</span>(u, v, d);</span><br><span class="line">    &#125;</span><br><span class="line">    dis[s] = <span class="number">0</span>;</span><br><span class="line">    q.<span class="built_in">push</span>((node)&#123;<span class="number">0</span>, s&#125;);</span><br><span class="line">    <span class="keyword">while</span> (!q.<span class="built_in">empty</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        node tmp = q.<span class="built_in">top</span>();</span><br><span class="line">        q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="type">int</span> x = tmp.pos, d = tmp.dis;</span><br><span class="line">        <span class="keyword">if</span> (visit[x])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        visit[x] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = head[x]; i; i = e[i].next)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> y = e[i].to;</span><br><span class="line">            <span class="keyword">if</span> (dis[y] &gt; dis[x] + e[i].dis)</span><br><span class="line">            &#123;</span><br><span class="line">                dis[y] = dis[x] + e[i].dis;</span><br><span class="line">                <span class="keyword">if</span> (!visit[y])</span><br><span class="line">                &#123;</span><br><span class="line">                    q.<span class="built_in">push</span>((node)&#123;dis[y], y&#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, dis[i]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="R-最近公共祖先（LCA）"><a href="#R-最近公共祖先（LCA）" class="headerlink" title="R - 最近公共祖先（LCA）"></a>R - 最近公共祖先（LCA）</h3><p><img src="https://img-blog.csdnimg.cn/762439eaabc74282821a35ae03107eeb.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> maxn 1000010</span></span><br><span class="line"><span class="comment">// LCA问题</span></span><br><span class="line"><span class="comment">// https://www.luogu.com.cn/problem/P3379</span></span><br><span class="line"><span class="type">int</span> cnt, n, m, s;</span><br><span class="line"><span class="type">int</span> to[maxn], Next[maxn], head[maxn];</span><br><span class="line"><span class="type">int</span> d[maxn], f[maxn][<span class="number">20</span>];</span><br><span class="line"><span class="type">int</span> t;</span><br><span class="line">queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> s, <span class="type">int</span> e)</span> <span class="comment">//加边</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    to[++cnt] = e, Next[cnt] = head[s], head[s] = cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bfs</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    q.<span class="built_in">push</span>(s);</span><br><span class="line">    d[s] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (q.<span class="built_in">empty</span>() == <span class="literal">false</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> x = q.<span class="built_in">front</span>();</span><br><span class="line">        q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = head[x]; i; i = Next[i])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> tar = to[i];</span><br><span class="line">            <span class="keyword">if</span> (d[tar])</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            d[tar] = d[x] + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            f[tar][<span class="number">0</span>] = x;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= t; j++)</span><br><span class="line">                f[tar][j] = f[f[tar][j - <span class="number">1</span>]][j - <span class="number">1</span>];</span><br><span class="line">            q.<span class="built_in">push</span>(tar);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">lca</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (d[x] &gt; d[y])</span><br><span class="line">        <span class="built_in">swap</span>(x, y); <span class="comment">//默认x小于y</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = t; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">        <span class="keyword">if</span> (d[f[y][i]] &gt;= d[x])</span><br><span class="line">            y = f[y][i];</span><br><span class="line">    <span class="keyword">if</span> (x == y)</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = t; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">        <span class="keyword">if</span> (f[x][i] != f[y][i])</span><br><span class="line">            x = f[x][i], y = f[y][i];</span><br><span class="line">    <span class="keyword">return</span> f[x][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m &gt;&gt; s;</span><br><span class="line">    t = (<span class="type">int</span>)(<span class="built_in">log</span>(n) / <span class="built_in">log</span>(<span class="number">2</span>)) + <span class="number">1</span>; <span class="comment">//层数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> s, e;</span><br><span class="line">        cin &gt;&gt; s &gt;&gt; e;</span><br><span class="line">        <span class="built_in">add</span>(s, e);</span><br><span class="line">        <span class="built_in">add</span>(e, s);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">bfs</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> a, b;</span><br><span class="line">        cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">        cout &lt;&lt; <span class="built_in">lca</span>(a, b)&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="S-负环"><a href="#S-负环" class="headerlink" title="S - 负环"></a>S - 负环</h3><p><img src="https://img-blog.csdnimg.cn/5bdfe065754e46058d1105a01ac3216d.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 判断负环https://www.luogu.com.cn/problem/P3385</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> maxn 1000000</span></span><br><span class="line"><span class="type">int</span> T, n, m;</span><br><span class="line"><span class="type">int</span> to[maxn], edge[maxn], head[maxn], Next[maxn], cnt;</span><br><span class="line"><span class="type">int</span> dist[maxn];</span><br><span class="line"><span class="type">bool</span> visit[maxn];</span><br><span class="line"><span class="type">int</span> num[maxn]; <span class="comment">//判断负环</span></span><br><span class="line">queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    to[++cnt] = y, edge[cnt] = z, Next[cnt] = head[x], head[x] = cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span> (T--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">        <span class="built_in">memset</span>(visit, <span class="number">0</span>, <span class="built_in">sizeof</span>(visit));</span><br><span class="line">        <span class="built_in">memset</span>(dist, <span class="number">0x3f</span>, <span class="built_in">sizeof</span>(dist));</span><br><span class="line">        <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="built_in">sizeof</span>(num));</span><br><span class="line">        <span class="built_in">memset</span>(head, <span class="number">0</span>, <span class="built_in">sizeof</span>(head));</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> u, v, w;</span><br><span class="line">            cin &gt;&gt; u &gt;&gt; v &gt;&gt; w;</span><br><span class="line">            <span class="keyword">if</span> (w &gt;= <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">add</span>(u, v, w);</span><br><span class="line">                <span class="built_in">add</span>(v, u, w);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (w &lt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">add</span>(u, v, w);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        dist[<span class="number">1</span>] = <span class="number">0</span>, visit[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        q.<span class="built_in">push</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">while</span> (q.<span class="built_in">empty</span>() == <span class="literal">false</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x = q.<span class="built_in">front</span>();</span><br><span class="line">            q.<span class="built_in">pop</span>();</span><br><span class="line">            visit[x] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = head[x]; i; i = Next[i])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> y = to[i], z = edge[i];</span><br><span class="line">                <span class="keyword">if</span> (dist[y] &gt; dist[x] + z)</span><br><span class="line">                &#123;</span><br><span class="line">                    dist[y] = dist[x] + z;</span><br><span class="line">                    num[y] = num[x] + <span class="number">1</span>; <span class="comment">//这一行和下面的num判断去掉就是SPFA了</span></span><br><span class="line">                    <span class="keyword">if</span> (num[y] &gt;= n)</span><br><span class="line">                    &#123;</span><br><span class="line">                        flag = <span class="literal">true</span>;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (!visit[y])</span><br><span class="line">                        q.<span class="built_in">push</span>(y), visit[y] = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (flag)</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;YES&quot;</span> &lt;&lt; endl;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;NO&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="T-二分图最大匹配"><a href="#T-二分图最大匹配" class="headerlink" title="T - 二分图最大匹配"></a>T - 二分图最大匹配</h3><p><img src="https://img-blog.csdnimg.cn/5a3f835af6b746099485b02f3e4ef13d.png" alt="在这里插入图片描述"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="comment">// 传送门;https://www.luogu.com.cn/problem/P3386</span></span><br><span class="line"><span class="comment">// 匈牙利算法</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> maxn 10000100</span></span><br><span class="line"><span class="type">int</span> n, m, e, ans;</span><br><span class="line"><span class="type">bool</span> f[<span class="number">1000</span>][<span class="number">1000</span>];</span><br><span class="line"><span class="type">int</span> to[maxn], Next[maxn], head[maxn], cnt;</span><br><span class="line"><span class="type">int</span> match[maxn];</span><br><span class="line"><span class="type">bool</span> visit[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    to[++cnt] = y, Next[cnt] = head[x], head[x] = cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = head[x]; i; i = Next[i])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> y = to[i];</span><br><span class="line">        <span class="keyword">if</span> (visit[y] == <span class="literal">false</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            visit[y] = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (!match[y] || <span class="built_in">dfs</span>(match[y]))</span><br><span class="line">            &#123;</span><br><span class="line">                match[y] = x;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">    cin.<span class="built_in">tie</span>(<span class="number">0</span>), cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m &gt;&gt; e;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= e; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> u, v;</span><br><span class="line">        cin &gt;&gt; u &gt;&gt; v;</span><br><span class="line">        <span class="keyword">if</span> (u &lt;= n &amp;&amp; v &lt;= m)</span><br><span class="line">            <span class="built_in">add</span>(u, v);</span><br><span class="line">    &#125;</span><br><span class="line">    ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(visit, <span class="number">0</span>, <span class="built_in">sizeof</span>(visit));</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">dfs</span>(i))</span><br><span class="line">            ans++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; ans &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;暑假集训-week2图论&quot;&gt;&lt;a href=&quot;#暑假集训-week2图论&quot; class=&quot;headerlink&quot; title=&quot;暑假集训-week2图论&quot;&gt;&lt;/a&gt;暑假集训-week2图论&lt;/h1&gt;&lt;h3 id=&quot;A-Desert-King&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="集训" scheme="http://example.com/categories/%E9%9B%86%E8%AE%AD/"/>
    
    
    <category term="学习" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
